{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b82119-b336-4f19-af09-56827ed617af",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ad break ê°ì§€ ë° ë§¥ë½ì  ê´‘ê³  íƒ€ê²ŸíŒ…\n",
    "\n",
    "ë§¥ë½ì  ê´‘ê³ ëŠ” ê´‘ê³ ê°€ ì‚¬ìš©ìê°€ ì†Œë¹„í•˜ëŠ” ì›¹í˜ì´ì§€ë‚˜ ë¯¸ë””ì–´ì˜ ë§¥ë½ê³¼ ë§¤ì¹­ë˜ëŠ” íƒ€ê²Ÿ ê´‘ê³ ì˜ í•œ í˜•íƒœì…ë‹ˆë‹¤. ì´ í”„ë¡œì„¸ìŠ¤ì—ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” ì°¸ì—¬ìê°€ ìˆìŠµë‹ˆë‹¤: í¼ë¸”ë¦¬ì…”(ì›¹ì‚¬ì´íŠ¸ ë˜ëŠ” ì½˜í…ì¸  ì†Œìœ ì), ê´‘ê³ ì£¼, ê·¸ë¦¬ê³  ì†Œë¹„ìì…ë‹ˆë‹¤. í¼ë¸”ë¦¬ì…”ëŠ” í”Œë«í¼ê³¼ ì½˜í…ì¸ ë¥¼ ì œê³µí•˜ê³ , ê´‘ê³ ì£¼ëŠ” ë§¥ë½ì— ë§ëŠ” ê´‘ê³ ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì†Œë¹„ìëŠ” ì½˜í…ì¸ ì™€ ìƒí˜¸ì‘ìš©í•˜ê³ , ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ê´‘ê³ ê°€ í‘œì‹œë˜ì–´ ë” ê°œì¸í™”ë˜ê³  ê´€ë ¨ì„± ìˆëŠ” ê´‘ê³  ê²½í—˜ì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "ë§¥ë½ì  ê´‘ê³ ì˜ ì–´ë ¤ìš´ ì˜ì—­ì€ video on demand(VOD) í”Œë«í¼ì—ì„œ ìŠ¤íŠ¸ë¦¬ë°í•˜ê¸° ìœ„í•œ ë¯¸ë””ì–´ ì½˜í…ì¸ ì— ê´‘ê³ ë¥¼ ì‚½ì…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ í”„ë¡œì„¸ìŠ¤ëŠ” ì „í†µì ìœ¼ë¡œ ì¸ê°„ ì „ë¬¸ê°€ê°€ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ê³ , ë‚´ëŸ¬í‹°ë¸Œì˜ breaksë¥¼ ì‹ë³„í•˜ê³  ê´€ë ¨ í‚¤ì›Œë“œë‚˜ ì¹´í…Œê³ ë¦¬ë¥¼ í• ë‹¹í•˜ëŠ” ìˆ˜ë™ íƒœê¹…ì— ì˜ì¡´í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ì ‘ê·¼ ë°©ì‹ì€ ì‹œê°„ì´ ë§ì´ ê±¸ë¦¬ê³ , ì£¼ê´€ì ì´ë©°, ì½˜í…ì¸ ì˜ ì „ì²´ ë§¥ë½ì´ë‚˜ ë‰˜ì•™ìŠ¤ë¥¼ í¬ì°©í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „í†µì ì¸ AI/ML ì†”ë£¨ì…˜ì€ ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ìë™í™”í•  ìˆ˜ ìˆì§€ë§Œ, ì¢…ì¢… ê´‘ë²”ìœ„í•œ í›ˆë ¨ ë°ì´í„°ê°€ í•„ìš”í•˜ê³  ë¹„ìš©ì´ ë§ì´ ë“¤ë©° ê¸°ëŠ¥ì´ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "![Ad decisions](./static/images/02-ad-breaks.jpg)\n",
    "\n",
    "ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ì§€ì›í•˜ëŠ” Generative AIëŠ” ì´ ê³¼ì œì— ëŒ€í•œ ìœ ë§í•œ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ì˜ ë°©ëŒ€í•œ ì§€ì‹ê³¼ ë§¥ë½ì  ì´í•´ë¥¼ í™œìš©í•¨ìœ¼ë¡œì¨, í¼ë¸”ë¦¬ì…”ëŠ” ìë™ìœ¼ë¡œ ë¯¸ë””ì–´ ìì‚°ì— ëŒ€í•œ ë§¥ë½ì  ì¸ì‚¬ì´íŠ¸ì™€ ë¶„ë¥˜ë²•ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°„ì†Œí™”í•˜ê³  ì •í™•í•˜ê³  í¬ê´„ì ì¸ ë§¥ë½ì  ì´í•´ë¥¼ ì œê³µí•˜ì—¬ íš¨ê³¼ì ì¸ ê´‘ê³  íƒ€ê²ŸíŒ…ê³¼ ë¯¸ë””ì–´ ì•„ì¹´ì´ë¸Œì˜ ìˆ˜ìµí™”ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì›Œí¬ìˆì˜ ì´ ë¶€ë¶„ì„ ë§ˆì¹˜ë©´ ë¹„ë””ì˜¤ì— ëŒ€í•œ ë‹¤ìŒê³¼ ê°™ì€ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤:\n",
    "* ë¹„ë””ì˜¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê³ í’ˆì§ˆ ê´‘ê³  ë°°ì¹˜ ê¸°íšŒ ë˜ëŠ” _breaks_ ëª©ë¡\n",
    "* Ad Decision Serversë¥¼ ì‚¬ìš©í•œ ìë™ ë°°ì¹˜ë¥¼ ìœ„í•´ ê´‘ê³ ì£¼ê°€ ì½˜í…ì¸ ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë° ì‚¬ìš©í•˜ëŠ” IAB Content Taxonomyë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ë¥¼ í¬í•¨í•˜ì—¬ ê° break ì „í›„ì˜ ë¹„ë””ì˜¤ì— ëŒ€í•œ ë§¥ë½ì  ì •ë³´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca97816-35b5-40c4-b194-ba88c68b0222",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dadf3d-559a-4a5a-9e9d-509f9139a0b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ë ¤ë©´ ë…¸íŠ¸ë¶ í™˜ê²½ì„ ì„¤ì •í•˜ê³  ì˜¤ë””ì˜¤, ì‹œê°ì , ì˜ë¯¸ë¡ ì  ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ë¥¼ ì„¸ê·¸ë¨¼íŠ¸í™”í•œ ì´ì „ì˜ ëª¨ë“  ê¸°ì´ˆ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í–ˆì–´ì•¼ í•©ë‹ˆë‹¤:\n",
    "1. [00-prerequisites.ipynb](00-prerequisites.ipyn)\n",
    "2. [01A-visual-segments-frames-shots-scenes.ipynb](01A-visual-segments-frames-shots-scenes.ipynb) \n",
    "3. [01B-audio-segments.ipynb](01B-audio-segments.ipynb) nts-frames-shots-scenes.ipynb) \n",
    "3. [01B-audio-segments.ipynb](01B-audio-segments.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f3368-b3ea-406d-bc46-328e3e322d8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d4c51-6580-4386-a398-df098834657b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json_repair\n",
    "import copy\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "#from lib.chapters import Chapters\n",
    "from lib import frame_utils\n",
    "from lib import util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636a0da-4572-4c3b-8b80-4ee2ed465181",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### ì´ì „ ë…¸íŠ¸ë¶ì—ì„œ ì €ì¥ëœ ê°’ ê²€ìƒ‰í•˜ê¸°\n",
    "ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ë ¤ë©´ íŒ¨í‚¤ì§€ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ê³  SageMaker í™˜ê²½ì—ì„œ ì¼ë¶€ ì •ë³´ë¥¼ ìˆ˜ì§‘í•œ ì´ì „ ë…¸íŠ¸ë¶ 00_prerequisites.ipynbë¥¼ ì‹¤í–‰í–ˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96940dd-7733-4a7d-8782-8c7779f15432",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3677d-8b4c-422b-afa0-f633d2b86d51",
   "metadata": {},
   "source": [
    "# ì•„í‚¤í…ì²˜\n",
    "\n",
    "ì´ ì‹¤ìŠµ ì›Œí¬í”Œë¡œìš°ëŠ” SageMakerì˜ AWS ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. scenes, conversation topics ë° ê´‘ê³  ì½˜í…ì¸  ë¶„ë¥˜ë²•ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë§¥ë½ì  ê´‘ê³  breaksì™€ chapter ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì¶œë ¥ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "![Contextual Ads workflow with outputs](./static/images/02-contextual-ads-workflow-w-outputs-drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb30cd-62be-40e5-8370-db452a029a06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ë‚´ëŸ¬í‹°ë¸Œì—ì„œ chaptersë¥¼ ì‹ë³„í•˜ê¸° ìœ„í•´ scenesì™€ topicsë¥¼ ì •ë ¬í•˜ì—¬ ê´‘ê³  ë°°ì¹˜ ê¸°íšŒ ì°¾ê¸°\n",
    "\n",
    "[Video segmentation notebook](video-understanding-with-generative-ai-on-aws-main/01-video-time-segmentation.ipynb)ì—ì„œ, ìš°ë¦¬ëŠ” ë¹„ë””ì˜¤ì˜ ì‹œê°ì  ë° ì˜¤ë””ì˜¤ ë‹¨ì„œë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ë“¤ì„ í•¨ê»˜ ê°€ì ¸ì™€ì„œ transcription topicsê°€ scenesì™€ ì •ë ¬ë˜ë„ë¡ í•˜ëŠ” í•œ ë‹¨ê³„ë¥¼ ë” ìˆ˜í–‰í•  ê²ƒì…ë‹ˆë‹¤. ì§„í–‰ ì¤‘ì¸ ëŒ€í™”ë‚˜ scene ì¤‘ì— ê´‘ê³ ë¥¼ ì‚½ì…í•˜ëŠ” ê²ƒì€ ì›í•˜ì§€ ì•ŠëŠ” ë§ˆì§€ë§‰ ì¼ì…ë‹ˆë‹¤. ì •ë ¬ì„ ìƒì„±í•˜ê¸° ìœ„í•´, ì‹œì‘ ë° ì¢…ë£Œ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ì£¼ì œë¥¼ ìš”ì•½í•˜ëŠ” í…ìŠ¤íŠ¸ ì„¤ëª…ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” ê° ëŒ€í™” ì£¼ì œë¥¼ ë°˜ë³µí•  ê²ƒì…ë‹ˆë‹¤. ê° ì£¼ì œì— ëŒ€í•´, ì½”ë“œëŠ” ì£¼ì œì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ë²”ìœ„ì™€ ê²¹ì¹˜ê±°ë‚˜ ê·¸ ì•ˆì— ì†í•˜ëŠ” ê´€ë ¨ ë¹„ë””ì˜¤ scenesë¥¼ ì‹ë³„í•©ë‹ˆë‹¤. ì´ í”„ë¡œì„¸ìŠ¤ì˜ ì¶œë ¥ì€ chapters ëª©ë¡ì´ë©°, ê° chapterëŠ” í•´ë‹¹ ì˜¤ë””ì˜¤ ëŒ€í™”ì™€ ì •ë ¬ë˜ëŠ” ë¹„ë””ì˜¤ scenesë¥¼ ë‚˜íƒ€ë‚´ëŠ” scene ID ëª©ë¡ì„ í¬í•¨í•©ë‹ˆë‹¤. ì •ë ¬ í”„ë¡œì„¸ìŠ¤ í›„ì—, ìš°ë¦¬ëŠ” ì‹œê°ì  ë° ì˜¤ë””ì˜¤ ë‹¨ì„œë¥¼ ìµœì¢… chaptersë¡œ ê²°í•©í–ˆìŠµë‹ˆë‹¤. chapters ì‚¬ì´ì˜ breaksëŠ” ë¹„ë””ì˜¤ ì½˜í…ì¸ ì˜ ë§¥ë½ì  ë³€í™” ì‚¬ì´ì—ì„œ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ê´‘ê³  ì‚½ì…ì— ì´ìƒì ì¸ ìœ„ì¹˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ì´ëŸ¬í•œ breaksë¥¼ ìš´ì˜ìì—ê²Œ ì œì•ˆìœ¼ë¡œ ì œì‹œí•˜ê³  ìµœì¢… ê´‘ê³  ë°°ì¹˜ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ human-in-the-loop ë‹¨ê³„ë¥¼ ê°–ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437b9ed-1bb4-4522-8e47-249388ac7719",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chapters:\n",
    "    def __init__(self, topics, scenes, frames):\n",
    "        self.video_asset_dir = frames.video_asset_dir()\n",
    "        self.chapters = self.align_scenes_in_chapters(topics, scenes, frames)\n",
    "        \n",
    "    def align_scenes_in_chapters(self, topics, scenes, frames):\n",
    "        \"\"\"\n",
    "        Aligns video scenes with conversation topics to create chronological chapters.\n",
    "    \n",
    "        Args:\n",
    "            topics: List of conversation topics with start_ms, end_ms, and reason\n",
    "            scenes: List of scene metadata with start_ms and end_ms\n",
    "            frames: List of video frame metadata\n",
    "    \n",
    "        Returns:\n",
    "            List of chapters, each containing aligned scenes and associated text\n",
    "    \n",
    "        Note:\n",
    "            - Handles scenes without conversations\n",
    "            - Merges overlapping topics\n",
    "            - Preserves chronological order\n",
    "            - Creates Chapter objects for each segment\n",
    "    \"\"\"\n",
    "        scenes = copy.deepcopy(scenes)\n",
    "    \n",
    "        chapters = []\n",
    "        for topic in topics:\n",
    "            \n",
    "            topic_start_ms = topic['start_ms']\n",
    "            topic_end_ms = topic['end_ms']\n",
    "            text = topic['reason']\n",
    "\n",
    "            # find all the frames that align with the conversation topic\n",
    "            stack = []\n",
    "            while len(scenes) > 0:\n",
    "                scene = scenes[0]\n",
    "                frame_start = scene['start_ms']\n",
    "                frame_end = scene['end_ms']\n",
    "\n",
    "                \n",
    "                if frame_start > topic_end_ms:\n",
    "                    # topic overlaps scenes that belong to previous topic - merge the text\n",
    "                    if not stack:\n",
    "                        num_chapters = len(chapters)\n",
    "                        if num_chapters > 0:\n",
    "                            chapters[num_chapters-1]['text'] = chapters[num_chapters-1]['text'] + ' ' + text\n",
    "                        \n",
    "                    break\n",
    "    \n",
    "                # scenes before any conversation starts\n",
    "                if frame_end < topic_start_ms:\n",
    "                    chapter = Chapter(len(chapters), [scene], frames).__dict__\n",
    "                    chapters.append(chapter)\n",
    "                    scenes.pop(0)\n",
    "                    continue\n",
    "    \n",
    "                stack.append(scene)\n",
    "                scenes.pop(0)\n",
    "    \n",
    "            if stack:\n",
    "                chapter = Chapter(len(chapters), stack, frames, text).__dict__\n",
    "                chapters.append(chapter)\n",
    "    \n",
    "        ## There could be more scenes without converations, append them\n",
    "        for scene in scenes:\n",
    "            chapter = Chapter(len(chapters), [scene], frames).__dict__\n",
    "            chapters.append(chapter)\n",
    "    \n",
    "        return chapters\n",
    "\n",
    "class Chapter:\n",
    "    def __init__(self, chapter_id, scenes, frames, text = ''):\n",
    "        self.scene_ids = [scene['id'] for scene in scenes]\n",
    "        self.start_frame_id = scenes[0]['start_frame_id']\n",
    "        self.end_frame_id = scenes[-1]['end_frame_id']\n",
    "        self.start_ms = scenes[0]['start_ms']\n",
    "        self.end_ms = scenes[-1]['end_ms']\n",
    "        self.id = chapter_id\n",
    "        self.text = text\n",
    "        #folder = os.path.join(frames.video_asset_dir(), 'chapters')\n",
    "        #os.makedirs(folder, exist_ok=True) \n",
    "        self.composite_images = frames.create_composite_images(frames.frames[self.start_frame_id:self.end_frame_id+1], 'chapters', prefix=\"chapter_\")\n",
    "        \n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339c1bf-6ade-4a05-868d-069424bccdf0",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['chapters'] = Chapters(video['topics'], video['scenes'].scenes, video['frames'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f341c0-1e68-487a-b1fd-ad6323bc7214",
   "metadata": {},
   "source": [
    "ê²°ê³¼ ê²€í† í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30eb92-f37f-46f5-98b7-1d238056785a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(JSON(video['chapters'].chapters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b4ffc-bc67-4a1f-8749-362815e1154a",
   "metadata": {},
   "source": [
    "#### chapters ì‹œê°í™”\n",
    "\n",
    "ì´ì œ ê° chapterì˜ í”„ë ˆì„ê³¼ í…ìŠ¤íŠ¸ë¥¼ ì‹œê°í™”í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì´ê²ƒë“¤ì€ ê´‘ê³  breaksì— ëŒ€í•œ ë§¥ë½ì  ì •ë³´ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ì˜ ì…ë ¥ì´ ë  ê²ƒì…ë‹ˆë‹¤. ì¼ë¶€ chaptersì—ëŠ” ê´€ë ¨ëœ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "ğŸ’¡ chaptersë¥¼ ë³´ë ¤ë©´ ì¶œë ¥ ìƒìì˜ ìŠ¤í¬ë¡¤ ë°”ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì¼ë¶€ chaptersëŠ” ë‹¨ì¼ composite imageì— ë§ì¶œ ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ ë” ë§ì€ í”„ë ˆì„ì„ í¬í•¨í•˜ë¯€ë¡œ, ê° chapterì— ëŒ€í•´ ì—¬ëŸ¬ composite imagesê°€ í‘œì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036728d-ea60-47a3-9167-d2f5949f9b96",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize the chapters\n",
    "\n",
    "STOP=10\n",
    "for counter, b in enumerate(video[\"chapters\"].chapters):\n",
    "    print(f'\\nChapter {counter}: frames {b[\"start_frame_id\"] } to {b[\"end_frame_id\"] }, scenes { b[\"scene_ids\"][0] } to { b[\"scene_ids\"][-1] }, time { b[\"start_ms\"]} to { b[\"end_ms\"] } =======\\n')\n",
    "    if len(b[\"text\"]) > 0: \n",
    "        print(f'\\nChapter Text: { b[\"text\"] }')\n",
    "    else:\n",
    "        print(f'\\nChapter Text (conversation topic): None')\n",
    "\n",
    "    video['frames'].display_frames(start=b['start_frame_id'], end=b['end_frame_id']+1)\n",
    "\n",
    "    # ALTERNATIVE: view the composite images that will be used in prompts\n",
    "    #for image_file in b['composite_images']:\n",
    "    #    display(DisplayImage(filename=image_file['file'], height=100))\n",
    "    #if counter == STOP:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e484ed-cf3c-41a5-9bab-771c56fa47f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# chapter ìˆ˜ì¤€ì˜ ë§¥ë½ì  ì •ë³´ ìƒì„±\n",
    "\n",
    "ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ì‹œê°ì ìœ¼ë¡œ ê·¸ë¦¬ê³  ì˜¤ë””ì˜¤ë¡œ ì •ë ¬ëœ ë°ì´í„°ë¥¼ Claude 3 Sonnetì— ë³´ë‚´ ê° ì£¼ì œì— ëŒ€í•œ ë§¥ë½ì  ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” Claude 3.5 ì œí’ˆêµ° ëª¨ë¸ì˜ ë©€í‹°ëª¨ë‹¬ ê¸°ëŠ¥ì„ í™œìš©í•˜ëŠ” ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. ìš°ë¦¬ì˜ í…ŒìŠ¤íŠ¸ì—ì„œ, ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ì ì ˆí•œ ì§€ì¹¨ì´ ì œê³µë  ë•Œ í° ì´ë¯¸ì§€ì—ì„œ ì„¸ë¶€ ì‚¬í•­ì„ í¬ì°©í•˜ê³  ì´ë¯¸ì§€ ì‹œí€€ìŠ¤ë¥¼ ë”°ë¥¼ ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Claude3.5 Sonnetì˜ ì…ë ¥ì„ ì¤€ë¹„í•˜ê¸° ìœ„í•´, ë¨¼ì € ê° ì£¼ì œì™€ ê´€ë ¨ëœ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì¡°í•©í•˜ê³  composite image ê·¸ë¦¬ë“œë¥¼ ë§Œë“­ë‹ˆë‹¤. ì‹¤í—˜ì„ í†µí•´, ìš°ë¦¬ëŠ” 7í–‰ 4ì—´ì˜ ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ ë¹„ìœ¨ì´ ìµœì ì„ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ê° ê°œë³„ í”„ë ˆì„ íƒ€ì¼ì—ì„œ ì¶©ë¶„í•œ ì„¸ë¶€ ì‚¬í•­ì„ ìœ ì§€í•˜ë©´ì„œë„ Claudeì˜ 5MB ì´ë¯¸ì§€ íŒŒì¼ í¬ê¸° ì œí•œ ì•„ë˜ì— ë§ëŠ” 1568 x 1540 í”½ì…€ ì´ë¯¸ì§€ë¥¼ êµ¬ì„±í•  ê²ƒì…ë‹ˆë‹¤. ë˜í•œ í•„ìš”í•œ ê²½ìš° ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì¡°í•©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´í›„, composite images, transcription, IAB Content taxonomy ì •ì˜, GARM taxonomy ì •ì˜ê°€ Claude3 Haiku ëª¨ë¸ì— ëŒ€í•œ ë‹¨ì¼ ì¿¼ë¦¬ë¡œ ì„¤ëª…, ê°ì •, IAB taxonomy, GARM taxonomy ë° ê¸°íƒ€ ê´€ë ¨ ì •ë³´ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ì— ì…ë ¥ë©ë‹ˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼, ì´ ì ‘ê·¼ ë°©ì‹ì„ ë§¤ë²ˆ ëª¨ë¸ì„ í›ˆë ¨í•  í•„ìš” ì—†ì´ ëª¨ë“  ë¶„ë¥˜ë²•ì´ë‚˜ ì‚¬ìš©ì ì§€ì • ë ˆì´ë¸”ë§ ì‚¬ìš© ì‚¬ë¡€ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ì´ ì ‘ê·¼ ë°©ì‹ì˜ ì§„ì •í•œ í˜ì´ ìˆëŠ” ê³³ì…ë‹ˆë‹¤. í•„ìš”í•œ ê²½ìš° ìµœì¢… ì¶œë ¥ì„ ì¸ê°„ ê²€í† ìì—ê²Œ ì œì‹œí•˜ì—¬ ìµœì¢… í™•ì¸ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒì€ íŠ¹ì • ì£¼ì œì— ëŒ€í•œ composite image ê·¸ë¦¬ë“œì™€ í•´ë‹¹ ë§¥ë½ì  ì¶œë ¥ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
    "\n",
    "![Contextualized chapters](./static/images/02-chapter-contextualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e01a83-55cf-4ab7-944c-a6c9d3448414",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## IAB Content Taxonomy ì •ì˜ ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "IAB(Interactive Advertising Bureau) TaxonomyëŠ” ë””ì§€í„¸ ê´‘ê³  ì½˜í…ì¸ ì™€ ëŒ€ìƒì„ ìœ„í•œ í‘œì¤€í™”ëœ ë¶„ë¥˜ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë””ì§€í„¸ ì½˜í…ì¸ ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ê³„ì¸µì  êµ¬ì¡°ë¥¼ ì œê³µí•˜ì—¬ ê´‘ê³ ì£¼ì™€ í¼ë¸”ë¦¬ì…”ê°€ ë””ì§€í„¸ ê´‘ê³ ë¥¼ êµ¬ì„±, íƒ€ê²ŸíŒ… ë° ì¸¡ì •í•˜ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "Anthropic Claudeì—ê²Œ ì´ ë¶„ë¥˜ë²•ì„ ì‚¬ìš©í•˜ì—¬ chaptersë¥¼ ë¶„ë¥˜í•˜ë„ë¡ ì§€ì‹œí•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ chapters ì‚¬ì´ì— ë§ì„ ìˆ˜ ìˆëŠ” ê´‘ê³  ì¢…ë¥˜ë¥¼ ì‹ë³„í•˜ëŠ” ë° ë„ì›€ì„ ì¤„ ê²ƒì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdff3d-466b-4d0e-8f3e-6df0188f29e5",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iab_file = 'iab_content_taxonomy_v3.json'\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/iab/{iab_file}\"\n",
    "\n",
    "!curl {url} -o {iab_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84fd39-1e6a-4c2f-861f-ae0a3f87d43e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_iab_taxonomy(file):\n",
    "    \"\"\"\n",
    "    Loads IAB taxonomy definitions from a JSON file.\n",
    "    Args:\n",
    "        file: Path to the IAB taxonomy JSON file\n",
    "    Returns:\n",
    "        Dictionary containing IAB taxonomy definitions\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        iab_taxonomies = json.load(f)\n",
    "    return iab_taxonomies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bd101-affd-4630-a192-bee997980abb",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iab_taxonomy = load_iab_taxonomy(iab_file)\n",
    "display(JSON(iab_taxonomy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdb888-fb4b-41ea-b85f-59be51f89e93",
   "metadata": {},
   "source": [
    "## ê° chapter ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ë§¥ë½ì  ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "\n",
    "ì´ ì˜ˆì‹œëŠ” Foundation ëª¨ë¸ê³¼ì˜ ë‹¤ì¤‘ í„´ ëŒ€í™”ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ìœ„í•´ Amazon Bedrockê³¼ í•¨ê»˜ [Anthropic Claude Messages API (aka Conversations API)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë¨¼ì €, í”„ë¡¬í”„íŠ¸ì˜ ë¶€ë¶„ë“¤ì— ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9cdc1-1e9b-486b-b284-eb8deefd405a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constructors for parts of prompt messages\n",
    "\n",
    "def make_iab_taxonomoies(iab_list):\n",
    "    iab = [item['name'] for item in iab_list]\n",
    "    iab.append('None')\n",
    "\n",
    "    return iab\n",
    "\n",
    "def make_image_message(composite_images):\n",
    "    \"\"\"\n",
    "    Converts a list of image files into a formatted message with base64-encoded images.\n",
    "    Args:\n",
    "        composite_images: List of dicts containing image file paths\n",
    "    Returns:\n",
    "        Dict with 'role' and 'content' containing:\n",
    "        - Text description of number of images\n",
    "        - List of base64-encoded images with metadata\n",
    "    \"\"\"\n",
    "    # adding the composite image sequences\n",
    "    image_contents = [{\n",
    "        'type': 'text',\n",
    "        'text': 'Here are {0} images containing frame sequence that describes a scene.'.format(len(composite_images))\n",
    "    }]\n",
    "\n",
    "    open_images = []\n",
    "    for image in composite_images:\n",
    "        with open(image['file'], \"rb\") as image_file:\n",
    "            image_data = image_file.read()\n",
    "            open_images.append(image_file)\n",
    "        image_pil = Image.open(BytesIO(image_data))\n",
    "        bas64_image = frame_utils.image_to_base64(image_pil)\n",
    "        image_contents.append({\n",
    "            'type': 'image',\n",
    "            'source': {\n",
    "                'type': 'base64',\n",
    "                'media_type': 'image/jpeg',\n",
    "                'data': bas64_image\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # close the images\n",
    "    for image in open_images:\n",
    "        image.close()\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': image_contents\n",
    "    }\n",
    "\n",
    "def make_output_example():\n",
    "    \"\"\"\n",
    "    Creates a template message for AI model output formatting.\n",
    "    Returns:\n",
    "        Dict with 'role' and 'content' keys containing example JSON structure for:\n",
    "        - Scene description\n",
    "        - Sentiment analysis\n",
    "        - IAB and GARM taxonomies\n",
    "        - Brand/logo detection\n",
    "        - Relevant tags\n",
    "    Note:\n",
    "        Used as part of the prompt to ensure consistent response formatting\n",
    "    \"\"\"\n",
    "    example = {\n",
    "        'description': {\n",
    "            'text': 'The scene describes...',\n",
    "            'score': 98\n",
    "        },\n",
    "        'sentiment': {\n",
    "            'text': 'Positive',\n",
    "            'score': 90\n",
    "        },\n",
    "        'iab_taxonomy': {\n",
    "            'text': 'Station Wagon',\n",
    "            'score': 80\n",
    "        },\n",
    "        'garm_taxonomy': {\n",
    "            'text': 'Online piracy',\n",
    "            'score': 90\n",
    "        },\n",
    "        'brands_and_logos': [\n",
    "            {\n",
    "                'text': 'Amazon',\n",
    "                'score': 95\n",
    "            },\n",
    "            {\n",
    "                'text': 'Nike',\n",
    "                'score': 85\n",
    "            }\n",
    "        ],\n",
    "        'relevant_tags': [\n",
    "            {\n",
    "                'text': 'auto racing',\n",
    "                'score': 95\n",
    "            }\n",
    "        ]            \n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'Return JSON format. An example of the output:\\n{0}\\n'.format(json.dumps(example))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8187c-bc2e-458b-8abe-94822775f64a",
   "metadata": {},
   "source": [
    "ë‹¤ìŒ ì½”ë“œ ë¸”ë¡ì€ í—¬í¼ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•˜ê³  ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ Amazon Bedrockì„ í˜¸ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b817cf-2db8-4d4e-bd0d-96ee558d5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_ID = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "# MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "MODEL_PRICING = (0.003, 0.015)\n",
    "MODEL_VER = 'bedrock-2023-05-31'\n",
    "\n",
    "def get_chapter_description(images, text, iab_definitions):\n",
    "    \"\"\"\n",
    "    Generates chapter descriptions using image analysis, text, and IAB classifications.\n",
    "    Args:\n",
    "        images: List of image analysis results (max 19 images)\n",
    "        text: Transcribed conversation/text (optional)\n",
    "        iab_definitions: IAB taxonomy definitions with tier1 classifications\n",
    "    Returns:\n",
    "        Dict containing chapter description, IAB classifications, and sentiment analysis\n",
    "    Note:\n",
    "        - Uses Claude model for analysis\n",
    "        - Requires make_iab_taxonomies(), make_output_example(), make_image_message()\n",
    "        - Implements retry logic for failed inference calls\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    system = '''You are a media operation engineer. Your job is to review a clip from a video \n",
    "    content presented in a sequence of consecutive images. Each image\n",
    "    contains a sequence of frames presented in a 4x7 grid reading from left to\n",
    "    right and then from top to bottom. Interpret the frames as the time \n",
    "    progression of a video clip.  Don't refer to specific frames, instead, think\n",
    "    about what is happening over time in the scene.  You may also optionally be given the\n",
    "    conversation of the scene you can use to understand the context of\n",
    "    the scene. \n",
    "\n",
    "    You are asked to provide the following information: a detailed \n",
    "    description to describe the scene using the visual and audio, identify the most relevant IAB taxonomy, \n",
    "    GARM, sentiment, and brands and logos that \n",
    "    may appear in the scene, and five most relevant tags from the scene.\n",
    "    \n",
    "    It is important to return the results in JSON format and also includes a\n",
    "    confidence score from 0 to 100. Skip any explanation. Answer in detail in Korean.\n",
    "    '''\n",
    "\n",
    "    other_information = []\n",
    "    other_information.append(\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f'''\n",
    "                    Here is a list of IAB Taxonomies in <iab> tag. Only answer \n",
    "                    the IAB taxonomy from this list:\n",
    "                    <iab>\n",
    "                    { json.dumps(make_iab_taxonomoies(iab_definitions['tier1'])) }\n",
    "                    </iab>\n",
    "                    '''\n",
    "        })\n",
    "    \n",
    "    other_information.append(\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f'''\n",
    "                    Here is a list of GARM Taxonomies in <garm> tag. Only answer\n",
    "                    the GARM taxonomy from this list:\n",
    "                    <garm>\n",
    "                    [\n",
    "                        'Adult & Explicit Sexual Content',\n",
    "                        'Arms & Ammunition',\n",
    "                        'Crime & Harmful acts to individuals and Society, Human Right Violations',\n",
    "                        'Death, Injury or Military Conflict',\n",
    "                        'Online piracy',\n",
    "                        'Hate speech & acts of aggression',\n",
    "                        'Obscenity and Profanity, including language, gestures, and explicitly gory, graphic or repulsive content intended to shock and disgust',\n",
    "                        'Illegal Drugs, Tobacco, ecigarettes, Vaping, or Alcohol',\n",
    "                        'Spam or Harmful Content',\n",
    "                        'Terrorism',\n",
    "                        'Debated Sensitive Social Issue',\n",
    "                        'None',\n",
    "                    ]\n",
    "                    </garm>\n",
    "                    '''\n",
    "        })\n",
    "\n",
    "    other_information.append(\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f'''\n",
    "                Here is a list of Sentiments in <sentiment> tag. Only answer the\n",
    "                sentiment from this list:\n",
    "\n",
    "                <sentiment>\n",
    "                ['Positive', 'Neutral', 'Negative', 'None']\n",
    "                </sentiment>\n",
    "                '''\n",
    "        })\n",
    "\n",
    "    output_format_message = make_output_example()\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    # adding sequences of composite images to the prompt.  Limit is 20.\n",
    "    message_images = make_image_message(images[:19])\n",
    "    messages.append(message_images)\n",
    "\n",
    "    # adding the conversation to the prompt\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the images. Do you have the conversation of the scene?'\n",
    "    })\n",
    "\n",
    "    message_conversation = {\n",
    "        'role': 'user',\n",
    "        'content': 'No conversation.'\n",
    "    }\n",
    "    if text:\n",
    "        message_conversation['content'] = f'''\n",
    "            Here is the conversation of the scene in <conversation> tag.\n",
    "            <conversation>\n",
    "            { text }\n",
    "            </conversation>\n",
    "            '''\n",
    "\n",
    "    messages.append(message_conversation)\n",
    "\n",
    "    # other information\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. Do you have other information to provdie?'\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': other_information\n",
    "    })\n",
    "\n",
    "    # output format\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. What output format?'\n",
    "    })\n",
    "    messages.append(output_format_message)\n",
    "\n",
    "    # prefill '{'\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "    \n",
    "    model_params = {\n",
    "        'anthropic_version': MODEL_VER,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system,\n",
    "        'messages': messages\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = inference(model_params)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(colored(f\"ERR: inference: {str(e)}\\n RETRY...\", 'red'))\n",
    "        response = inference(model_params)\n",
    "\n",
    "    return response\n",
    "\n",
    "def display_prompt(model_params):\n",
    "    \"\"\"\n",
    "    Displays the model parameters including system prompt and messages for debugging or logging purposes.\n",
    "    Args:\n",
    "        model_params (dict): Dictionary containing model parameters with:\n",
    "            - system (str): The system prompt text\n",
    "            - messages (list): List of message objects to be displayed       \n",
    "    Returns:\n",
    "        None: This function prints to console and doesn't return any value\n",
    "    \"\"\"\n",
    "    print (f'MODEL_ID: {MODEL_ID}\\n')\n",
    "    print (f'System Prompt:\\n\\n{model_params[\"system\"]}')\n",
    "    print (f'Messages:\\n\\n')\n",
    "    for message in model_params['messages']:\n",
    "        print (json.dumps(message))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    return\n",
    "\n",
    "def inference(model_params):\n",
    "    \"\"\"\n",
    "    Invokes an Amazon Bedrock model for inference using the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        model_params (dict): Parameters for the model inference, including:\n",
    "            - Any model-specific parameters required for the inference call\n",
    "            - Must be JSON-serializable\n",
    "\n",
    "    Returns:\n",
    "        dict: The processed response containing:\n",
    "            - content (list): List of response contents where each item contains:\n",
    "                - text (str): Raw text response from the model\n",
    "                - json (dict): Parsed JSON response (if successful)\n",
    "            - model_params (dict): Original input parameters\n",
    "            - Additional response metadata from the model\n",
    "    \"\"\"\n",
    "    model_id = MODEL_ID\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    bedrock_runtime_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps(model_params),\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # patch the json string output with '{' and parse it\n",
    "    response_content = response_body['content'][0]['text']\n",
    "    if response_content[0] != '{':\n",
    "        response_content = '{' + response_content\n",
    "\n",
    "    try:\n",
    "        response_content = json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        print(colored(\"Malformed JSON response. Try to repair it...\", 'red'))\n",
    "        try:\n",
    "            response_content = json_repair.loads(response_content, strict=False)\n",
    "        except Exception as e:\n",
    "            print(colored(\"Failed to repair the JSON response...\", 'red'))\n",
    "            print(colored(response_content, 'red'))\n",
    "            raise e\n",
    "\n",
    "    response_body['content'][0]['json'] = response_content\n",
    "    response_body['model_params'] = model_params\n",
    "\n",
    "    return response_body\n",
    "\n",
    "\n",
    "\n",
    "def display_contextual_cost(usage):\n",
    "    \"\"\"\n",
    "    Calculate and display the estimated cost of using the model based on input and output tokens.\n",
    "    Args:\n",
    "        usage (dict): A dictionary containing token usage information with keys:\n",
    "            - input_tokens (int): Number of input tokens used\n",
    "            - output_tokens (int): Number of output tokens generated\n",
    "    Returns:\n",
    "        dict: A dictionary containing cost calculation details:\n",
    "            - input_per_1k (float): Cost per 1000 input tokens\n",
    "            - output_per_1k (float): Cost per 1000 output tokens\n",
    "            - input_tokens (int): Number of input tokens used\n",
    "            - output_tokens (int): Number of output tokens generated\n",
    "            - estimated_cost (float): Total estimated cost in USD\n",
    "    \"\"\"\n",
    "    # us-east-1 pricing\n",
    "    input_per_1k, output_per_1k = MODEL_PRICING\n",
    "\n",
    "    input_tokens = usage['input_tokens']\n",
    "    output_tokens = usage['output_tokens']\n",
    "\n",
    "    contextual_cost = (\n",
    "        input_per_1k * input_tokens +\n",
    "        output_per_1k * output_tokens\n",
    "    ) / 1000\n",
    "\n",
    "    print('\\n')\n",
    "    print('========================================================================')\n",
    "    print('Estimated cost:', colored(f\"${round(contextual_cost, 4)}\", 'green'), f\"in us-east-1 region with {colored(input_tokens, 'green')} input tokens and {colored(output_tokens, 'green')} output tokens.\")\n",
    "    print('========================================================================')\n",
    "\n",
    "    return {\n",
    "        'input_per_1k': input_per_1k,\n",
    "        'output_per_1k': output_per_1k,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'estimated_cost': contextual_cost,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca1d65-48b7-4738-b600-352d67191756",
   "metadata": {},
   "source": [
    "## ëª¨ë“  chapter ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•´ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99effde5-caba-4d24-9199-7346042c34c9",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_usage = {\n",
    "    'input_tokens': 0,\n",
    "    'output_tokens': 0,\n",
    "}\n",
    "\n",
    "iab_definitions = load_iab_taxonomy(iab_file)\n",
    "\n",
    "for chapter in video['chapters'].chapters:\n",
    "\n",
    "    composite_images = chapter['composite_images']\n",
    "    num_images = len(composite_images)\n",
    "\n",
    "    chapter_id = chapter['id']\n",
    "    text = chapter['text'] \n",
    "\n",
    "    contextual_response = get_chapter_description(composite_images, chapter['text'], iab_definitions)\n",
    "    time.sleep(5)\n",
    "    usage = contextual_response['usage']\n",
    "    contextual = contextual_response['content'][0]['json']\n",
    "\n",
    "    # save the contextual to the chapter\n",
    "    chapter['contextual'] = {\n",
    "        'usage': usage,\n",
    "        **contextual\n",
    "    }\n",
    "\n",
    "    total_usage['input_tokens'] += usage['input_tokens']\n",
    "    total_usage['output_tokens'] += usage['output_tokens']\n",
    "\n",
    "    print(f\"==== Chapter #{chapter['id']:02d}: Contextual information ======\")\n",
    "    video['frames'].display_frames(start=chapter['start_frame_id'], end=chapter['end_frame_id']+1)\n",
    "    for key in ['description', 'sentiment', 'iab_taxonomy', 'garm_taxonomy']:\n",
    "        print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "\n",
    "    for key in ['brands_and_logos', 'relevant_tags']:\n",
    "        items = ', '.join([item['text'] for item in contextual[key]])\n",
    "        if len(items) == 0:\n",
    "            items = 'None'\n",
    "        print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "    print(f\"================================================\\n\\n\")\n",
    "\n",
    "output_file = os.path.join(video[\"output_dir\"], 'scenes_in_chapters.json')\n",
    "util.save_to_file(output_file, video['chapters'].chapters)\n",
    "\n",
    "contextual_cost = display_contextual_cost(total_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03269195-fcfd-4a9b-9d5d-a7de1d7cfe7c",
   "metadata": {},
   "source": [
    "## Ad breaks\n",
    "\n",
    "ì´ ì‹œì ì—ì„œ, ìš°ë¦¬ëŠ” scenes ì‚¬ì´ì— ëª…í™•í•œ ì‹œê°ì  breaksê°€ ìˆëŠ” ë¹„ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìƒì„±í–ˆê³ , scenesë¥¼ ì˜¤ë””ì˜¤ì˜ speechì—ì„œ ì£¼ì œ ì‚¬ì´ì— ëª…í™•í•œ breaksê°€ ìˆëŠ” chaptersë¡œ ê·¸ë£¹í™”í–ˆìŠµë‹ˆë‹¤. chapters ì‚¬ì´ì˜ breaksëŠ” ëª¨ë‘ ê´‘ê³  ë°°ì¹˜ ê¸°íšŒ í›„ë³´ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” breaksì— ì–´ë–¤ ê´‘ê³ ë¥¼ ë°°ì¹˜í• ì§€ ë” ë‚˜ì€ ê²°ì •ì„ ë‚´ë¦¬ê¸° ìœ„í•´ breaksì— ì¸ì ‘í•œ chapter ì„¸ê·¸ë¨¼íŠ¸ì˜ IAB taxonomyë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ğŸ¤” chapter ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë³´ë©´ì„œ, ìì‹ ì„ ë¸Œëœë“œë¥¼ ê´‘ê³ í•˜ê³  ì‹¶ì€ íšŒì‚¬ë¼ê³  ìƒìƒí•´ë³´ì„¸ìš”. ë¸Œëœë“œ ì•ˆì „ì„± ì¸¡ë©´ì—ì„œ ë‹¤ë¥¸ breaksë³´ë‹¤ ì„ í˜¸í•˜ëŠ” breaksê°€ ìˆë‚˜ìš”?\n",
    "\n",
    "ğŸ¤” ì´ì œ ìì‹ ì„ ì‹œì²­ìë¼ê³  ìƒìƒí•´ë³´ì„¸ìš”. ì´ ì œëª©ì„ ì„ íƒí–ˆë‹¤ë©´ ì–´ë–¤ ì œí’ˆì´ í¥ë¯¸ë¡œìš¸ê¹Œìš”?\n",
    "\n",
    "ì‹¤ì œë¡œëŠ” ê´‘ê³  breaksëŠ” ì†Œë¹„ì, í¼ë¸”ë¦¬ì…”, ê´‘ê³ ì£¼ì˜ ìš”êµ¬ ì‚¬í•­ì„ ê³ ë ¤í•˜ëŠ” ê°€ì¹˜ í•¨ìˆ˜ì— ì˜í•´ ìˆœìœ„ê°€ ë§¤ê²¨ì§ˆ ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ec109-e2a7-44a4-b1b9-2636fd518046",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## ê´‘ê³  breaks ì‹œê°í™”\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” ê´‘ê³  ê²½í—˜ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•´ breaks ì¤‘ í•˜ë‚˜ì— í…ŒìŠ¤íŠ¸ ê´‘ê³ ë¥¼ ì‚½ì…í•  ê²ƒì…ë‹ˆë‹¤. BREAK_CHAPTER_ID ê°’ì„ ë³€ê²½í•˜ì—¬ ë‹¤ë¥¸ chapter breaksë¥¼ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddc979-bf0f-4aca-a0a8-72147b95aeb4",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import moviepy\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "BREAK_CHAPTER_ID = 8\n",
    "\n",
    "ad_demo_file= f\"ad_break_{ BREAK_CHAPTER_ID }_demo.mp4\"\n",
    "adbreak_start = video['chapters'].chapters[BREAK_CHAPTER_ID]['start_ms']/1000\n",
    "\n",
    "clip1 = VideoFileClip(video[\"path\"], target_resolution=(360, 640)).subclip(adbreak_start-10, adbreak_start)\n",
    "clip2 = VideoFileClip(\"static/images/CountdownClock_0.mp4\", target_resolution=(360, 640))\n",
    "clip3 = VideoFileClip(video[\"path\"], target_resolution=(360, 640)).subclip(adbreak_start, adbreak_start+10)\n",
    "final_clip = concatenate_videoclips([clip1,clip2,clip3], method=\"compose\")\n",
    "final_clip.write_videofile(ad_demo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181640e-de93-4f58-8d87-d82bc49ce33a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(url=ad_demo_file, width=640, height=360)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b4a880-fca8-4fef-93ab-3f39c8db23c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ë‹¤ìŒì€ ë¬´ì—‡ì¸ê°€ìš”?\n",
    "\n",
    "ë‹¤ë¥¸ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì‹œë„í•´ë³´ê±°ë‚˜, ì™„ë£Œí–ˆë‹¤ë©´ [Additional Resources](09-resources.ipynb) ì‹¤ìŠµìœ¼ë¡œ ê³„ì† ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21bf8a-83f4-4120-94c4-17faa02ef16c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
