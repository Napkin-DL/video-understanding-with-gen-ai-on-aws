{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b82119-b336-4f19-af09-56827ed617af",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ad break 감지 및 맥락적 광고 타겟팅\n",
    "\n",
    "맥락적 광고는 광고가 사용자가 소비하는 웹페이지나 미디어의 맥락과 매칭되는 타겟 광고의 한 형태입니다. 이 프로세스에는 세 가지 주요 참여자가 있습니다: 퍼블리셔(웹사이트 또는 콘텐츠 소유자), 광고주, 그리고 소비자입니다. 퍼블리셔는 플랫폼과 콘텐츠를 제공하고, 광고주는 맥락에 맞는 광고를 만듭니다. 소비자는 콘텐츠와 상호작용하고, 맥락을 기반으로 관련 광고가 표시되어 더 개인화되고 관련성 있는 광고 경험을 만듭니다.\n",
    "\n",
    "맥락적 광고의 어려운 영역은 video on demand(VOD) 플랫폼에서 스트리밍하기 위한 미디어 콘텐츠에 광고를 삽입하는 것입니다. 이 프로세스는 전통적으로 인간 전문가가 콘텐츠를 분석하고, 내러티브의 breaks를 식별하고 관련 키워드나 카테고리를 할당하는 수동 태깅에 의존했습니다. 하지만 이 접근 방식은 시간이 많이 걸리고, 주관적이며, 콘텐츠의 전체 맥락이나 뉘앙스를 포착하지 못할 수 있습니다. 전통적인 AI/ML 솔루션은 이 프로세스를 자동화할 수 있지만, 종종 광범위한 훈련 데이터가 필요하고 비용이 많이 들며 기능이 제한적일 수 있습니다.\n",
    "\n",
    "![Ad decisions](./static/images/02-ad-breaks.jpg)\n",
    "\n",
    "대규모 언어 모델이 지원하는 Generative AI는 이 과제에 대한 유망한 솔루션을 제공합니다. 이러한 모델의 방대한 지식과 맥락적 이해를 활용함으로써, 퍼블리셔는 자동으로 미디어 자산에 대한 맥락적 인사이트와 분류법을 생성할 수 있습니다. 이 접근 방식은 프로세스를 간소화하고 정확하고 포괄적인 맥락적 이해를 제공하여 효과적인 광고 타겟팅과 미디어 아카이브의 수익화를 가능하게 합니다.\n",
    "\n",
    "이 워크숍의 이 부분을 마치면 비디오에 대한 다음과 같은 메타데이터를 생성하게 됩니다:\n",
    "* 비디오에서 사용 가능한 고품질 광고 배치 기회 또는 _breaks_ 목록\n",
    "* Ad Decision Servers를 사용한 자동 배치를 위해 광고주가 콘텐츠를 분류하는 데 사용하는 IAB Content Taxonomy를 사용한 분류를 포함하여 각 break 전후의 비디오에 대한 맥락적 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca97816-35b5-40c4-b194-ba88c68b0222",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dadf3d-559a-4a5a-9e9d-509f9139a0b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "이 노트북을 실행하려면 노트북 환경을 설정하고 오디오, 시각적, 의미론적 정보를 사용하여 비디오를 세그먼트화한 이전의 모든 기초 노트북을 실행했어야 합니다:\n",
    "1. [00-prerequisites.ipynb](00-prerequisites.ipyn)\n",
    "2. [01A-visual-segments-frames-shots-scenes.ipynb](01A-visual-segments-frames-shots-scenes.ipynb) \n",
    "3. [01B-audio-segments.ipynb](01B-audio-segments.ipynb) nts-frames-shots-scenes.ipynb) \n",
    "3. [01B-audio-segments.ipynb](01B-audio-segments.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f3368-b3ea-406d-bc46-328e3e322d8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d4c51-6580-4386-a398-df098834657b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json_repair\n",
    "import copy\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "#from lib.chapters import Chapters\n",
    "from lib import frame_utils\n",
    "from lib import util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636a0da-4572-4c3b-8b80-4ee2ed465181",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 이전 노트북에서 저장된 값 검색하기\n",
    "이 노트북을 실행하려면 패키지 종속성을 설치하고 SageMaker 환경에서 일부 정보를 수집한 이전 노트북 00_prerequisites.ipynb를 실행했어야 합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96940dd-7733-4a7d-8782-8c7779f15432",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3677d-8b4c-422b-afa0-f633d2b86d51",
   "metadata": {},
   "source": [
    "# 아키텍처\n",
    "\n",
    "이 실습 워크플로우는 SageMaker의 AWS 서비스를 사용합니다. scenes, conversation topics 및 광고 콘텐츠 분류법을 입력으로 받아 맥락적 광고 breaks와 chapter 세그먼트를 출력으로 생성합니다.\n",
    "\n",
    "![Contextual Ads workflow with outputs](./static/images/02-contextual-ads-workflow-w-outputs-drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb30cd-62be-40e5-8370-db452a029a06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 내러티브에서 chapters를 식별하기 위해 scenes와 topics를 정렬하여 광고 배치 기회 찾기\n",
    "\n",
    "[Video segmentation notebook](video-understanding-with-generative-ai-on-aws-main/01-video-time-segmentation.ipynb)에서, 우리는 비디오의 시각적 및 오디오 단서를 별도로 처리했습니다. 이제 이들을 함께 가져와서 transcription topics가 scenes와 정렬되도록 하는 한 단계를 더 수행할 것입니다. 진행 중인 대화나 scene 중에 광고를 삽입하는 것은 원하지 않는 마지막 일입니다. 정렬을 생성하기 위해, 시작 및 종료 타임스탬프와 주제를 요약하는 텍스트 설명으로 표현되는 각 대화 주제를 반복할 것입니다. 각 주제에 대해, 코드는 주제의 타임스탬프 범위와 겹치거나 그 안에 속하는 관련 비디오 scenes를 식별합니다. 이 프로세스의 출력은 chapters 목록이며, 각 chapter는 해당 오디오 대화와 정렬되는 비디오 scenes를 나타내는 scene ID 목록을 포함합니다. 정렬 프로세스 후에, 우리는 시각적 및 오디오 단서를 최종 chapters로 결합했습니다. chapters 사이의 breaks는 비디오 콘텐츠의 맥락적 변화 사이에서 발생하기 때문에 광고 삽입에 이상적인 위치입니다.\n",
    "\n",
    "실제 응용에서는 이러한 breaks를 운영자에게 제안으로 제시하고 최종 광고 배치를 확인하기 위한 human-in-the-loop 단계를 갖는 것을 권장합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437b9ed-1bb4-4522-8e47-249388ac7719",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Chapters:\n",
    "    def __init__(self, topics, scenes, frames):\n",
    "        self.video_asset_dir = frames.video_asset_dir()\n",
    "        self.chapters = self.align_scenes_in_chapters(topics, scenes, frames)\n",
    "        \n",
    "    def align_scenes_in_chapters(self, topics, scenes, frames):\n",
    "        \"\"\"\n",
    "        Aligns video scenes with conversation topics to create chronological chapters.\n",
    "    \n",
    "        Args:\n",
    "            topics: List of conversation topics with start_ms, end_ms, and reason\n",
    "            scenes: List of scene metadata with start_ms and end_ms\n",
    "            frames: List of video frame metadata\n",
    "    \n",
    "        Returns:\n",
    "            List of chapters, each containing aligned scenes and associated text\n",
    "    \n",
    "        Note:\n",
    "            - Handles scenes without conversations\n",
    "            - Merges overlapping topics\n",
    "            - Preserves chronological order\n",
    "            - Creates Chapter objects for each segment\n",
    "    \"\"\"\n",
    "        scenes = copy.deepcopy(scenes)\n",
    "    \n",
    "        chapters = []\n",
    "        for topic in topics:\n",
    "            \n",
    "            topic_start_ms = topic['start_ms']\n",
    "            topic_end_ms = topic['end_ms']\n",
    "            text = topic['reason']\n",
    "\n",
    "            # find all the frames that align with the conversation topic\n",
    "            stack = []\n",
    "            while len(scenes) > 0:\n",
    "                scene = scenes[0]\n",
    "                frame_start = scene['start_ms']\n",
    "                frame_end = scene['end_ms']\n",
    "\n",
    "                \n",
    "                if frame_start > topic_end_ms:\n",
    "                    # topic overlaps scenes that belong to previous topic - merge the text\n",
    "                    if not stack:\n",
    "                        num_chapters = len(chapters)\n",
    "                        if num_chapters > 0:\n",
    "                            chapters[num_chapters-1]['text'] = chapters[num_chapters-1]['text'] + ' ' + text\n",
    "                        \n",
    "                    break\n",
    "    \n",
    "                # scenes before any conversation starts\n",
    "                if frame_end < topic_start_ms:\n",
    "                    chapter = Chapter(len(chapters), [scene], frames).__dict__\n",
    "                    chapters.append(chapter)\n",
    "                    scenes.pop(0)\n",
    "                    continue\n",
    "    \n",
    "                stack.append(scene)\n",
    "                scenes.pop(0)\n",
    "    \n",
    "            if stack:\n",
    "                chapter = Chapter(len(chapters), stack, frames, text).__dict__\n",
    "                chapters.append(chapter)\n",
    "    \n",
    "        ## There could be more scenes without converations, append them\n",
    "        for scene in scenes:\n",
    "            chapter = Chapter(len(chapters), [scene], frames).__dict__\n",
    "            chapters.append(chapter)\n",
    "    \n",
    "        return chapters\n",
    "\n",
    "class Chapter:\n",
    "    def __init__(self, chapter_id, scenes, frames, text = ''):\n",
    "        self.scene_ids = [scene['id'] for scene in scenes]\n",
    "        self.start_frame_id = scenes[0]['start_frame_id']\n",
    "        self.end_frame_id = scenes[-1]['end_frame_id']\n",
    "        self.start_ms = scenes[0]['start_ms']\n",
    "        self.end_ms = scenes[-1]['end_ms']\n",
    "        self.id = chapter_id\n",
    "        self.text = text\n",
    "        #folder = os.path.join(frames.video_asset_dir(), 'chapters')\n",
    "        #os.makedirs(folder, exist_ok=True) \n",
    "        self.composite_images = frames.create_composite_images(frames.frames[self.start_frame_id:self.end_frame_id+1], 'chapters', prefix=\"chapter_\")\n",
    "        \n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339c1bf-6ade-4a05-868d-069424bccdf0",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['chapters'] = Chapters(video['topics'], video['scenes'].scenes, video['frames'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f341c0-1e68-487a-b1fd-ad6323bc7214",
   "metadata": {},
   "source": [
    "결과 검토합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30eb92-f37f-46f5-98b7-1d238056785a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(JSON(video['chapters'].chapters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b4ffc-bc67-4a1f-8749-362815e1154a",
   "metadata": {},
   "source": [
    "#### chapters 시각화\n",
    "\n",
    "이제 각 chapter의 프레임과 텍스트를 시각화해 보겠습니다. 이것들은 광고 breaks에 대한 맥락적 정보를 생성하기 위한 프롬프트의 입력이 될 것입니다. 일부 chapters에는 관련된 텍스트가 없을 수 있습니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "💡 chapters를 보려면 출력 상자의 스크롤 바를 사용하세요. 일부 chapters는 단일 composite image에 맞출 수 있는 것보다 더 많은 프레임을 포함하므로, 각 chapter에 대해 여러 composite images가 표시될 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036728d-ea60-47a3-9167-d2f5949f9b96",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize the chapters\n",
    "\n",
    "STOP=10\n",
    "for counter, b in enumerate(video[\"chapters\"].chapters):\n",
    "    print(f'\\nChapter {counter}: frames {b[\"start_frame_id\"] } to {b[\"end_frame_id\"] }, scenes { b[\"scene_ids\"][0] } to { b[\"scene_ids\"][-1] }, time { b[\"start_ms\"]} to { b[\"end_ms\"] } =======\\n')\n",
    "    if len(b[\"text\"]) > 0: \n",
    "        print(f'\\nChapter Text: { b[\"text\"] }')\n",
    "    else:\n",
    "        print(f'\\nChapter Text (conversation topic): None')\n",
    "\n",
    "    video['frames'].display_frames(start=b['start_frame_id'], end=b['end_frame_id']+1)\n",
    "\n",
    "    # ALTERNATIVE: view the composite images that will be used in prompts\n",
    "    #for image_file in b['composite_images']:\n",
    "    #    display(DisplayImage(filename=image_file['file'], height=100))\n",
    "    #if counter == STOP:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e484ed-cf3c-41a5-9bab-771c56fa47f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# chapter 수준의 맥락적 정보 생성\n",
    "\n",
    "마지막 단계는 시각적으로 그리고 오디오로 정렬된 데이터를 Claude 3 Sonnet에 보내 각 주제에 대한 맥락적 정보를 생성하는 것입니다. 이는 Claude 3.5 제품군 모델의 멀티모달 기능을 활용하는 접근 방식입니다. 우리의 테스트에서, 이러한 모델들은 적절한 지침이 제공될 때 큰 이미지에서 세부 사항을 포착하고 이미지 시퀀스를 따를 수 있는 능력을 보여주었습니다.\n",
    "\n",
    "Claude3.5 Sonnet의 입력을 준비하기 위해, 먼저 각 주제와 관련된 비디오 프레임을 조합하고 composite image 그리드를 만듭니다. 실험을 통해, 우리는 7행 4열의 이미지 그리드 비율이 최적임을 발견했습니다. 이는 각 개별 프레임 타일에서 충분한 세부 사항을 유지하면서도 Claude의 5MB 이미지 파일 크기 제한 아래에 맞는 1568 x 1540 픽셀 이미지를 구성할 것입니다. 또한 필요한 경우 여러 이미지를 조합할 수도 있습니다.\n",
    "\n",
    "이후, composite images, transcription, IAB Content taxonomy 정의, GARM taxonomy 정의가 Claude3 Haiku 모델에 대한 단일 쿼리로 설명, 감정, IAB taxonomy, GARM taxonomy 및 기타 관련 정보를 생성하기 위한 프롬프트에 입력됩니다. 뿐만 아니라, 이 접근 방식을 매번 모델을 훈련할 필요 없이 모든 분류법이나 사용자 지정 레이블링 사용 사례에 적용할 수 있습니다. 이것이 이 접근 방식의 진정한 힘이 있는 곳입니다. 필요한 경우 최종 출력을 인간 검토자에게 제시하여 최종 확인을 받을 수 있습니다. 다음은 특정 주제에 대한 composite image 그리드와 해당 맥락적 출력의 예시입니다.\n",
    "\n",
    "![Contextualized chapters](./static/images/02-chapter-contextualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e01a83-55cf-4ab7-944c-a6c9d3448414",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## IAB Content Taxonomy 정의 다운로드\n",
    "\n",
    "IAB(Interactive Advertising Bureau) Taxonomy는 디지털 광고 콘텐츠와 대상을 위한 표준화된 분류 시스템입니다. 디지털 콘텐츠를 분류하기 위한 계층적 구조를 제공하여 광고주와 퍼블리셔가 디지털 광고를 구성, 타겟팅 및 측정하기 쉽게 만듭니다.\n",
    "\n",
    "Anthropic Claude에게 이 분류법을 사용하여 chapters를 분류하도록 지시하여 서로 다른 chapters 사이에 맞을 수 있는 광고 종류를 식별하는 데 도움을 줄 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdff3d-466b-4d0e-8f3e-6df0188f29e5",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iab_file = 'iab_content_taxonomy_v3.json'\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/iab/{iab_file}\"\n",
    "\n",
    "!curl {url} -o {iab_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84fd39-1e6a-4c2f-861f-ae0a3f87d43e",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_iab_taxonomy(file):\n",
    "    \"\"\"\n",
    "    Loads IAB taxonomy definitions from a JSON file.\n",
    "    Args:\n",
    "        file: Path to the IAB taxonomy JSON file\n",
    "    Returns:\n",
    "        Dictionary containing IAB taxonomy definitions\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        iab_taxonomies = json.load(f)\n",
    "    return iab_taxonomies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514bd101-affd-4630-a192-bee997980abb",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iab_taxonomy = load_iab_taxonomy(iab_file)\n",
    "display(JSON(iab_taxonomy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdb888-fb4b-41ea-b85f-59be51f89e93",
   "metadata": {},
   "source": [
    "## 각 chapter 세그먼트에 대한 맥락적 메타데이터를 생성하기 위한 프롬프트 구성\n",
    "\n",
    "이 예시는 Foundation 모델과의 다중 턴 대화를 시뮬레이션하는 이 프롬프트를 위해 Amazon Bedrock과 함께 [Anthropic Claude Messages API (aka Conversations API)](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html)를 사용합니다.\n",
    "\n",
    "먼저, 프롬프트의 부분들에 대한 텍스트를 생성하기 위한 헬퍼 함수를 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9cdc1-1e9b-486b-b284-eb8deefd405a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constructors for parts of prompt messages\n",
    "\n",
    "def make_iab_taxonomoies(iab_list):\n",
    "    iab = [item['name'] for item in iab_list]\n",
    "    iab.append('None')\n",
    "\n",
    "    return iab\n",
    "\n",
    "def make_image_message(composite_images):\n",
    "    \"\"\"\n",
    "    Converts a list of image files into a formatted message with base64-encoded images.\n",
    "    Args:\n",
    "        composite_images: List of dicts containing image file paths\n",
    "    Returns:\n",
    "        Dict with 'role' and 'content' containing:\n",
    "        - Text description of number of images\n",
    "        - List of base64-encoded images with metadata\n",
    "    \"\"\"\n",
    "    # adding the composite image sequences\n",
    "    image_contents = [{\n",
    "        'type': 'text',\n",
    "        'text': 'Here are {0} images containing frame sequence that describes a scene.'.format(len(composite_images))\n",
    "    }]\n",
    "\n",
    "    open_images = []\n",
    "    for image in composite_images:\n",
    "        with open(image['file'], \"rb\") as image_file:\n",
    "            image_data = image_file.read()\n",
    "            open_images.append(image_file)\n",
    "        image_pil = Image.open(BytesIO(image_data))\n",
    "        bas64_image = frame_utils.image_to_base64(image_pil)\n",
    "        image_contents.append({\n",
    "            'type': 'image',\n",
    "            'source': {\n",
    "                'type': 'base64',\n",
    "                'media_type': 'image/jpeg',\n",
    "                'data': bas64_image\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # close the images\n",
    "    for image in open_images:\n",
    "        image.close()\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': image_contents\n",
    "    }\n",
    "\n",
    "def make_output_example():\n",
    "    \"\"\"\n",
    "    Creates a template message for AI model output formatting.\n",
    "    Returns:\n",
    "        Dict with 'role' and 'content' keys containing example JSON structure for:\n",
    "        - Scene description\n",
    "        - Sentiment analysis\n",
    "        - IAB and GARM taxonomies\n",
    "        - Brand/logo detection\n",
    "        - Relevant tags\n",
    "    Note:\n",
    "        Used as part of the prompt to ensure consistent response formatting\n",
    "    \"\"\"\n",
    "    example = {\n",
    "        'description': {\n",
    "            'text': 'The scene describes...',\n",
    "            'score': 98\n",
    "        },\n",
    "        'sentiment': {\n",
    "            'text': 'Positive',\n",
    "            'score': 90\n",
    "        },\n",
    "        'iab_taxonomy': {\n",
    "            'text': 'Station Wagon',\n",
    "            'score': 80\n",
    "        },\n",
    "        'garm_taxonomy': {\n",
    "            'text': 'Online piracy',\n",
    "            'score': 90\n",
    "        },\n",
    "        'brands_and_logos': [\n",
    "            {\n",
    "                'text': 'Amazon',\n",
    "                'score': 95\n",
    "            },\n",
    "            {\n",
    "                'text': 'Nike',\n",
    "                'score': 85\n",
    "            }\n",
    "        ],\n",
    "        'relevant_tags': [\n",
    "            {\n",
    "                'text': 'auto racing',\n",
    "                'score': 95\n",
    "            }\n",
    "        ]            \n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'Return JSON format. An example of the output:\\n{0}\\n'.format(json.dumps(example))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8187c-bc2e-458b-8abe-94822775f64a",
   "metadata": {},
   "source": [
    "다음 코드 블록은 헬퍼 함수를 사용하여 프롬프트를 구성하고 추론을 수행하기 위해 Amazon Bedrock을 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b817cf-2db8-4d4e-bd0d-96ee558d5b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_ID = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "# MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "MODEL_PRICING = (0.003, 0.015)\n",
    "MODEL_VER = 'bedrock-2023-05-31'\n",
    "\n",
    "def get_chapter_description(images, text, iab_definitions):\n",
    "    \"\"\"\n",
    "    Generates chapter descriptions using image analysis, text, and IAB classifications.\n",
    "    Args:\n",
    "        images: List of image analysis results (max 19 images)\n",
    "        text: Transcribed conversation/text (optional)\n",
    "        iab_definitions: IAB taxonomy definitions with tier1 classifications\n",
    "    Returns:\n",
    "        Dict containing chapter description, IAB classifications, and sentiment analysis\n",
    "    Note:\n",
    "        - Uses Claude model for analysis\n",
    "        - Requires make_iab_taxonomies(), make_output_example(), make_image_message()\n",
    "        - Implements retry logic for failed inference calls\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    system = '''You are a media operation engineer. Your job is to review a clip from a video \n",
    "    content presented in a sequence of consecutive images. Each image\n",
    "    contains a sequence of frames presented in a 4x7 grid reading from left to\n",
    "    right and then from top to bottom. Interpret the frames as the time \n",
    "    progression of a video clip.  Don't refer to specific frames, instead, think\n",
    "    about what is happening over time in the scene.  You may also optionally be given the\n",
    "    conversation of the scene you can use to understand the context of\n",
    "    the scene. \n",
    "\n",
    "    You are asked to provide the following information: a detailed \n",
    "    description to describe the scene using the visual and audio, identify the most relevant IAB taxonomy, \n",
    "    GARM, sentiment, and brands and logos that \n",
    "    may appear in the scene, and five most relevant tags from the scene.\n",
    "    \n",
    "    It is important to return the results in JSON format and also includes a\n",
    "    confidence score from 0 to 100. Skip any explanation. Answer in detail in Korean.\n",
    "    '''\n",
    "\n",
    "    other_information = []\n",
    "    other_information.append(\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f'''\n",
    "                    Here is a list of IAB Taxonomies in <iab> tag. Only answer \n",
    "                    the IAB taxonomy from this list:\n",
    "                    <iab>\n",
    "                    { json.dumps(make_iab_taxonomoies(iab_definitions['tier1'])) }\n",
    "                    </iab>\n",
    "                    '''\n",
    "        })\n",
    "    \n",
    "    other_information.append(\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f'''\n",
    "                    Here is a list of GARM Taxonomies in <garm> tag. Only answer\n",
    "                    the GARM taxonomy from this list:\n",
    "                    <garm>\n",
    "                    [\n",
    "                        'Adult & Explicit Sexual Content',\n",
    "                        'Arms & Ammunition',\n",
    "                        'Crime & Harmful acts to individuals and Society, Human Right Violations',\n",
    "                        'Death, Injury or Military Conflict',\n",
    "                        'Online piracy',\n",
    "                        'Hate speech & acts of aggression',\n",
    "                        'Obscenity and Profanity, including language, gestures, and explicitly gory, graphic or repulsive content intended to shock and disgust',\n",
    "                        'Illegal Drugs, Tobacco, ecigarettes, Vaping, or Alcohol',\n",
    "                        'Spam or Harmful Content',\n",
    "                        'Terrorism',\n",
    "                        'Debated Sensitive Social Issue',\n",
    "                        'None',\n",
    "                    ]\n",
    "                    </garm>\n",
    "                    '''\n",
    "        })\n",
    "\n",
    "    other_information.append(\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f'''\n",
    "                Here is a list of Sentiments in <sentiment> tag. Only answer the\n",
    "                sentiment from this list:\n",
    "\n",
    "                <sentiment>\n",
    "                ['Positive', 'Neutral', 'Negative', 'None']\n",
    "                </sentiment>\n",
    "                '''\n",
    "        })\n",
    "\n",
    "    output_format_message = make_output_example()\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    # adding sequences of composite images to the prompt.  Limit is 20.\n",
    "    message_images = make_image_message(images[:19])\n",
    "    messages.append(message_images)\n",
    "\n",
    "    # adding the conversation to the prompt\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the images. Do you have the conversation of the scene?'\n",
    "    })\n",
    "\n",
    "    message_conversation = {\n",
    "        'role': 'user',\n",
    "        'content': 'No conversation.'\n",
    "    }\n",
    "    if text:\n",
    "        message_conversation['content'] = f'''\n",
    "            Here is the conversation of the scene in <conversation> tag.\n",
    "            <conversation>\n",
    "            { text }\n",
    "            </conversation>\n",
    "            '''\n",
    "\n",
    "    messages.append(message_conversation)\n",
    "\n",
    "    # other information\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. Do you have other information to provdie?'\n",
    "    })\n",
    "\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': other_information\n",
    "    })\n",
    "\n",
    "    # output format\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. What output format?'\n",
    "    })\n",
    "    messages.append(output_format_message)\n",
    "\n",
    "    # prefill '{'\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "    \n",
    "    model_params = {\n",
    "        'anthropic_version': MODEL_VER,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system,\n",
    "        'messages': messages\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = inference(model_params)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(colored(f\"ERR: inference: {str(e)}\\n RETRY...\", 'red'))\n",
    "        response = inference(model_params)\n",
    "\n",
    "    return response\n",
    "\n",
    "def display_prompt(model_params):\n",
    "    \"\"\"\n",
    "    Displays the model parameters including system prompt and messages for debugging or logging purposes.\n",
    "    Args:\n",
    "        model_params (dict): Dictionary containing model parameters with:\n",
    "            - system (str): The system prompt text\n",
    "            - messages (list): List of message objects to be displayed       \n",
    "    Returns:\n",
    "        None: This function prints to console and doesn't return any value\n",
    "    \"\"\"\n",
    "    print (f'MODEL_ID: {MODEL_ID}\\n')\n",
    "    print (f'System Prompt:\\n\\n{model_params[\"system\"]}')\n",
    "    print (f'Messages:\\n\\n')\n",
    "    for message in model_params['messages']:\n",
    "        print (json.dumps(message))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    return\n",
    "\n",
    "def inference(model_params):\n",
    "    \"\"\"\n",
    "    Invokes an Amazon Bedrock model for inference using the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        model_params (dict): Parameters for the model inference, including:\n",
    "            - Any model-specific parameters required for the inference call\n",
    "            - Must be JSON-serializable\n",
    "\n",
    "    Returns:\n",
    "        dict: The processed response containing:\n",
    "            - content (list): List of response contents where each item contains:\n",
    "                - text (str): Raw text response from the model\n",
    "                - json (dict): Parsed JSON response (if successful)\n",
    "            - model_params (dict): Original input parameters\n",
    "            - Additional response metadata from the model\n",
    "    \"\"\"\n",
    "    model_id = MODEL_ID\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    bedrock_runtime_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps(model_params),\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # patch the json string output with '{' and parse it\n",
    "    response_content = response_body['content'][0]['text']\n",
    "    if response_content[0] != '{':\n",
    "        response_content = '{' + response_content\n",
    "\n",
    "    try:\n",
    "        response_content = json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        print(colored(\"Malformed JSON response. Try to repair it...\", 'red'))\n",
    "        try:\n",
    "            response_content = json_repair.loads(response_content, strict=False)\n",
    "        except Exception as e:\n",
    "            print(colored(\"Failed to repair the JSON response...\", 'red'))\n",
    "            print(colored(response_content, 'red'))\n",
    "            raise e\n",
    "\n",
    "    response_body['content'][0]['json'] = response_content\n",
    "    response_body['model_params'] = model_params\n",
    "\n",
    "    return response_body\n",
    "\n",
    "\n",
    "\n",
    "def display_contextual_cost(usage):\n",
    "    \"\"\"\n",
    "    Calculate and display the estimated cost of using the model based on input and output tokens.\n",
    "    Args:\n",
    "        usage (dict): A dictionary containing token usage information with keys:\n",
    "            - input_tokens (int): Number of input tokens used\n",
    "            - output_tokens (int): Number of output tokens generated\n",
    "    Returns:\n",
    "        dict: A dictionary containing cost calculation details:\n",
    "            - input_per_1k (float): Cost per 1000 input tokens\n",
    "            - output_per_1k (float): Cost per 1000 output tokens\n",
    "            - input_tokens (int): Number of input tokens used\n",
    "            - output_tokens (int): Number of output tokens generated\n",
    "            - estimated_cost (float): Total estimated cost in USD\n",
    "    \"\"\"\n",
    "    # us-east-1 pricing\n",
    "    input_per_1k, output_per_1k = MODEL_PRICING\n",
    "\n",
    "    input_tokens = usage['input_tokens']\n",
    "    output_tokens = usage['output_tokens']\n",
    "\n",
    "    contextual_cost = (\n",
    "        input_per_1k * input_tokens +\n",
    "        output_per_1k * output_tokens\n",
    "    ) / 1000\n",
    "\n",
    "    print('\\n')\n",
    "    print('========================================================================')\n",
    "    print('Estimated cost:', colored(f\"${round(contextual_cost, 4)}\", 'green'), f\"in us-east-1 region with {colored(input_tokens, 'green')} input tokens and {colored(output_tokens, 'green')} output tokens.\")\n",
    "    print('========================================================================')\n",
    "\n",
    "    return {\n",
    "        'input_per_1k': input_per_1k,\n",
    "        'output_per_1k': output_per_1k,\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'estimated_cost': contextual_cost,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca1d65-48b7-4738-b600-352d67191756",
   "metadata": {},
   "source": [
    "## 모든 chapter 세그먼트에 대해 프롬프트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99effde5-caba-4d24-9199-7346042c34c9",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_usage = {\n",
    "    'input_tokens': 0,\n",
    "    'output_tokens': 0,\n",
    "}\n",
    "\n",
    "iab_definitions = load_iab_taxonomy(iab_file)\n",
    "\n",
    "for chapter in video['chapters'].chapters:\n",
    "\n",
    "    composite_images = chapter['composite_images']\n",
    "    num_images = len(composite_images)\n",
    "\n",
    "    chapter_id = chapter['id']\n",
    "    text = chapter['text'] \n",
    "\n",
    "    contextual_response = get_chapter_description(composite_images, chapter['text'], iab_definitions)\n",
    "    time.sleep(5)\n",
    "    usage = contextual_response['usage']\n",
    "    contextual = contextual_response['content'][0]['json']\n",
    "\n",
    "    # save the contextual to the chapter\n",
    "    chapter['contextual'] = {\n",
    "        'usage': usage,\n",
    "        **contextual\n",
    "    }\n",
    "\n",
    "    total_usage['input_tokens'] += usage['input_tokens']\n",
    "    total_usage['output_tokens'] += usage['output_tokens']\n",
    "\n",
    "    print(f\"==== Chapter #{chapter['id']:02d}: Contextual information ======\")\n",
    "    video['frames'].display_frames(start=chapter['start_frame_id'], end=chapter['end_frame_id']+1)\n",
    "    for key in ['description', 'sentiment', 'iab_taxonomy', 'garm_taxonomy']:\n",
    "        print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "\n",
    "    for key in ['brands_and_logos', 'relevant_tags']:\n",
    "        items = ', '.join([item['text'] for item in contextual[key]])\n",
    "        if len(items) == 0:\n",
    "            items = 'None'\n",
    "        print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "    print(f\"================================================\\n\\n\")\n",
    "\n",
    "output_file = os.path.join(video[\"output_dir\"], 'scenes_in_chapters.json')\n",
    "util.save_to_file(output_file, video['chapters'].chapters)\n",
    "\n",
    "contextual_cost = display_contextual_cost(total_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03269195-fcfd-4a9b-9d5d-a7de1d7cfe7c",
   "metadata": {},
   "source": [
    "## Ad breaks\n",
    "\n",
    "이 시점에서, 우리는 scenes 사이에 명확한 시각적 breaks가 있는 비디오 세그먼트를 생성했고, scenes를 오디오의 speech에서 주제 사이에 명확한 breaks가 있는 chapters로 그룹화했습니다. chapters 사이의 breaks는 모두 광고 배치 기회 후보입니다. 우리는 breaks에 어떤 광고를 배치할지 더 나은 결정을 내리기 위해 breaks에 인접한 chapter 세그먼트의 IAB taxonomy를 사용할 수 있습니다.\n",
    "\n",
    "🤔 chapter 세그먼트를 보면서, 자신을 브랜드를 광고하고 싶은 회사라고 상상해보세요. 브랜드 안전성 측면에서 다른 breaks보다 선호하는 breaks가 있나요?\n",
    "\n",
    "🤔 이제 자신을 시청자라고 상상해보세요. 이 제목을 선택했다면 어떤 제품이 흥미로울까요?\n",
    "\n",
    "실제로는 광고 breaks는 소비자, 퍼블리셔, 광고주의 요구 사항을 고려하는 가치 함수에 의해 순위가 매겨질 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ec109-e2a7-44a4-b1b9-2636fd518046",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 광고 breaks 시각화\n",
    "\n",
    "이 섹션에서는 광고 경험을 시각화하기 위해 breaks 중 하나에 테스트 광고를 삽입할 것입니다. BREAK_CHAPTER_ID 값을 변경하여 다른 chapter breaks를 시도해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddc979-bf0f-4aca-a0a8-72147b95aeb4",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import moviepy\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "BREAK_CHAPTER_ID = 8\n",
    "\n",
    "ad_demo_file= f\"ad_break_{ BREAK_CHAPTER_ID }_demo.mp4\"\n",
    "adbreak_start = video['chapters'].chapters[BREAK_CHAPTER_ID]['start_ms']/1000\n",
    "\n",
    "clip1 = VideoFileClip(video[\"path\"], target_resolution=(360, 640)).subclip(adbreak_start-10, adbreak_start)\n",
    "clip2 = VideoFileClip(\"static/images/CountdownClock_0.mp4\", target_resolution=(360, 640))\n",
    "clip3 = VideoFileClip(video[\"path\"], target_resolution=(360, 640)).subclip(adbreak_start, adbreak_start+10)\n",
    "final_clip = concatenate_videoclips([clip1,clip2,clip3], method=\"compose\")\n",
    "final_clip.write_videofile(ad_demo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181640e-de93-4f58-8d87-d82bc49ce33a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Video(url=ad_demo_file, width=640, height=360)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b4a880-fca8-4fef-93ab-3f39c8db23c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 다음은 무엇인가요?\n",
    "\n",
    "다른 사용 사례를 시도해보거나, 완료했다면 [Additional Resources](09-resources.ipynb) 실습으로 계속 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21bf8a-83f4-4120-94c4-17faa02ef16c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
