{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8458ac4-23ed-461e-bf44-55432f709d1a",
   "metadata": {},
   "source": [
    "# 의미론적 비디오 검색\n",
    "\n",
    "의미론적 비디오 검색은 키워드나 메타데이터에만 의존하지 않고 의미와 맥락을 기반으로 비디오 콘텐츠를 찾고 검색할 수 있게 하는 고급 기술입니다. 콘텐츠 제작자는 풍부하고 다차원적인 정보가 있는 비디오를 제작합니다. 의미론적 검색 알고리즘은 컴퓨터 비전, 자연어 처리, 그리고 이제는 generative AI와 같은 AI 기술을 사용하여 이 콘텐츠를 분석하고 이해합니다. 이를 통해 사용자는 시스템과 상호작용하여 비디오 내의 특정 개념, 객체 또는 동작을 검색할 수 있습니다.\n",
    "\n",
    "![Semantic Video Search](./static/images/04-semantic-video-search.png)\n",
    "\n",
    "의미론적 비디오 검색은 콘텐츠 발견, 사용자 참여 및 전반적인 시청 경험을 극적으로 개선하기 때문에 미디어 및 엔터테인먼트 산업에서 매우 중요합니다. 콘텐츠 과잉의 시대에서 사용자들은 관련 비디오를 찾는 더 효율적인 방법을 요구합니다. 전통적인 검색 방법은 종종 부족하여 사용자의 좌절과 충분히 활용되지 않는 콘텐츠 라이브러리로 이어집니다. 의미론적 검색을 통해 미디어 회사는 비디오 아카이브의 잠재력을 최대한 활용하고, 추천 시스템을 개선하며, 더 개인화된 시청 경험을 만들 수 있습니다.\n",
    "\n",
    "그러나 효과적인 의미론적 비디오 검색을 구현하는 것에는 상당한 과제가 있습니다. 비디오 데이터의 방대한 양과 복잡성으로 인해 콘텐츠를 정확하게 분석하고 인덱싱하기가 어렵습니다. 비디오 품질, 언어 및 문화적 맥락의 변화는 잘못된 해석으로 이어질 수 있습니다. Generative AI는 의미론적 비디오 검색 기능을 향상시키는 유망한 솔루션을 제공합니다. 대규모 언어 모델과 멀티모달 AI를 활용함으로써, generative AI는 비디오 콘텐츠의 더 미묘하고 맥락을 인식하는 분석을 제공할 수 있습니다. scenes의 상세한 설명을 생성하고, 복잡한 동작과 감정을 식별하며, 심지어 미묘한 문화적 참조도 이해하여 사용자 의도와 비디오 콘텐츠 간의 격차를 해소할 수 있습니다.\n",
    "\n",
    "이 실습에서는 이전 실습에서 생성된 시각적 및 오디오 메타데이터를 사용하여 멀티모달(MM) 검색 데이터베이스를 구축하기 위한 멀티모달 벡터 데이터베이스를 만들 것입니다. 실습이 끝나면 자연어나 이미지를 사용하여 이 데이터베이스를 쿼리하고 비디오에서 관련 shots를 빠르게 찾을 수 있게 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743be170-1b38-4ba1-b33b-f19841acd135",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292f5c5-22ed-4d89-906c-a9424ccdf01f",
   "metadata": {},
   "source": [
    "이 노트북을 실행하려면 노트북 환경을 설정하고 오디오, 시각적, 의미론적 정보를 사용하여 비디오를 세그먼트화한 이전의 모든 기초 노트북을 실행했어야 합니다:\n",
    "1. [00-prerequisites.ipynb](00-prerequisites.ipyn)\n",
    "2. [01A-visual-segments-frames-shots-scenes.ipynb](01A-visual-segments-frames-shots-scenes.ipynb) \n",
    "3. [01B-audio-segments.ipynb](01B-audio-segments.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870de375-fcf0-46f4-9154-1b54fb7b7737",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c286d5-2bf1-4aff-99ca-74b9d929c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from IPython.display import display, JSON, HTML\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import base64\n",
    "from termcolor import colored\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324a9e1-fbc5-47d3-81f1-24528f9dfba7",
   "metadata": {},
   "source": [
    "### Retrieve saved values from previous notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13068f49-0a48-4d1a-ac1c-c504dc0a58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f749e7-42c4-4c40-8079-6e180e32c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = video['path']\n",
    "rek_client = boto3.client(\"rekognition\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "region = sagemaker_resources['region']\n",
    "oss_host = session['AOSSCollectionEndpoint']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8ae5c-a196-4906-8c85-66ca6002ec64",
   "metadata": {},
   "source": [
    "# 아키텍처\n",
    "\n",
    "이 실습 워크플로우는 SageMaker에서 실행되는 AWS 서비스를 사용합니다. lab01에서 생성된 shot 수준 세그먼테이션 정보(shot 그룹, 오디오 transcription, shot 수준 composite images)를 다양한 AWS AI/GenAI 서비스에 보내 의미론적 비디오 검색 솔루션을 지원하는 임베딩과 메타데이터를 생성합니다. 다음 단계를 완료하게 됩니다:\n",
    "\n",
    "1. 먼저 shot 수준 프레임을 Amazon Rekognition에 보내 유명인 감지를 수행합니다.\n",
    "2. 그런 다음 유명인 정보와 shot 수준 composite image를 Bedrock의 Anthropic Claude 3 Sonnet 모델에 보내 shot 수준 캡션을 생성합니다. 그 후 캡션은 역시 Bedrock의 Amazon Titan Text Embedding 모델을 사용하여 텍스트 임베딩으로 변환됩니다.\n",
    "3. shot 수준 composite image는 또한 이미지 검색 임베딩을 생성하기 위해 Amazon Titan Multi-model(MM) Embedding 모델로 보내집니다.\n",
    "\n",
    "벡터 데이터베이스로 OpenSearch Serverless에 임베딩과 메타데이터를 인덱싱할 것입니다. 데이터 수집이 완료되면 텍스트와 이미지를 모두 사용하여 해당 데이터베이스를 검색하여 비디오에서 가장 일치하는 shots를 찾을 수 있습니다.\n",
    "\n",
    "![Flow diagram](./static/images/04-lab-flow-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98928c85-38f7-49f1-b548-0d15d01f19ae",
   "metadata": {},
   "source": [
    "## 비디오에서 shots의 부분집합을 무작위로 샘플링\n",
    "\n",
    "더 나은 중단 없는 실습 경험을 위해, 원본 비디오에서 시간순으로 10개의 shots를 무작위로 샘플링할 것입니다. 이 접근 방식은 워크숍 환경의 제한된 용량 때문에 필요합니다. 이렇게 하면 모든 참가자가 제한을 받지 않고 실습을 완료할 수 있도록 하면서도 연습의 무결성을 유지할 수 있습니다. 워크숍 샌드박스 환경에 있지 않다면 shots 수를 늘려도 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8d2ea-8858-4c24-87a6-c9c108826186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shot_mapping = {\n",
    "    \"Netflix_Open_Content_Meridian.mp4\": [1, 8, 12, 14, 21, 36, 46, 53, 61, 66],\n",
    "}\n",
    "\n",
    "# select the shot mapping\n",
    "assert video['path'] in shot_mapping, f\"****[{video['path']}]*** is not a supported video.\"\n",
    "\n",
    "# Assert that the key is in the dictionary\n",
    "shot_ids = shot_mapping[video['path']]\n",
    "\n",
    "sampled_shots = []\n",
    "\n",
    "for shot in video['shots'].shots:\n",
    "\n",
    "    if shot['id'] in shot_ids:\n",
    "        sampled_shots.append(shot)\n",
    "        print(colored(f\"Sampled shot id: {shot['id']} ===================\\n\", \"green\"))\n",
    "        display(Image.open(shot['composite_images'][0]['file']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351d55a-0263-4500-9d0a-f9314fbcb4c2",
   "metadata": {},
   "source": [
    "## Amazon Rekognition을 사용한 유명인 감지\n",
    "[Amazon Rekognition](https://aws.amazon.com/rekognition/)은 배우, 스포츠인, 온라인 콘텐츠 제작자와 같은 국제적으로 널리 알려진 유명인을 인식하는 데 사용될 수 있습니다. 유명인 인식 API가 제공하는 메타데이터는 콘텐츠에 태그를 지정하고 쉽게 검색할 수 있도록 하는 데 필요한 반복적인 수동 작업을 크게 줄여줍니다. 다음 섹션에서는 이전 단계에서 추출한 shots에서 유명인을 감지하는 데 도움이 되도록 이 기능을 활용할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400dce05-ad3a-4751-9f58-4d3ac885bcc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect_celebrities(shot):\n",
    "    start_frame_id = shot['start_frame_id']\n",
    "    end_frame_id = shot['end_frame_id']\n",
    "    video_asset_dir = shot['video_asset_dir']\n",
    "\n",
    "    frames = range(start_frame_id, end_frame_id + 1)\n",
    "\n",
    "    celebrities = set()\n",
    "\n",
    "    for frame_id in frames:\n",
    "        try:\n",
    "            #image_path = f\"{video_asset_dir}/frames/frames{frame_id+1:07d}.jpg\"\n",
    "            image_path = f\"{video_asset_dir}/frames/frames{frame_id+1:07d}.jpg\"\n",
    "            with open(image_path, 'rb') as image_file:\n",
    "                image_bytes = image_file.read()      \n",
    "\n",
    "            # Call Rekognition to detect celebrities\n",
    "            response = rek_client.recognize_celebrities(\n",
    "                Image={'Bytes': image_bytes}\n",
    "            )\n",
    "\n",
    "            min_confidence = 95.0 # change this value if the accuracy is low.\n",
    "\n",
    "            for celebrity in response.get('CelebrityFaces', []):\n",
    "                if celebrity.get('MatchConfidence', 0.0) >= min_confidence:\n",
    "                    celebrities.add(celebrity['Name'])\n",
    "\n",
    "        except ClientError as e:\n",
    "            pass\n",
    "\n",
    "    public_figures = ', '.join(celebrities)\n",
    "\n",
    "    shot[\"public_figure\"] = public_figures\n",
    "    \n",
    "    return {\n",
    "            \"shot_id\": shot['id'],\n",
    "            \"public_figure\": public_figures\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5207574-8777-4170-94fc-3792dc59b130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(colored(\"===== [Celebrities detected in each shot] ======\\n\", 'green'))\n",
    "for shot in sampled_shots:\n",
    "    print(detect_celebrities(shot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3257c5c-eb42-4950-8470-21c47a68b42e",
   "metadata": {},
   "source": [
    "## 오디오 Transcription 처리\n",
    "\n",
    "자막을 타임스탬프가 있는 문장으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcbf6e-a367-495c-8c40-858e61c81257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript(s):\n",
    "    subtitle_blocks = re.findall(\n",
    "        r\"(\\d+\\n(\\d{2}:\\d{2}:\\d{2}.\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}.\\d{3})\\n(.*?)(?=\\n\\d+\\n|\\Z))\",\n",
    "        s,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    sentences = [block[3].replace(\"\\n\", \" \").strip() for block in subtitle_blocks]\n",
    "    startTimes = [block[1] for block in subtitle_blocks]\n",
    "    endTimes = [block[2] for block in subtitle_blocks]\n",
    "\n",
    "    startTimes_ms = [time_to_ms(time) for time in startTimes]\n",
    "    endTimes_ms = [time_to_ms(time) for time in endTimes]\n",
    "\n",
    "    filtered_sentences = []\n",
    "    filtered_startTimes_ms = []\n",
    "    filtered_endTimes_ms = []\n",
    "\n",
    "    startTime_ms = -1\n",
    "    endTime_ms = -1\n",
    "    sentence = \"\"\n",
    "    for i in range(len(sentences)):\n",
    "        if startTime_ms == -1:\n",
    "            startTime_ms = startTimes_ms[i]\n",
    "        sentence += \" \" + sentences[i]\n",
    "        if (\n",
    "            sentences[i].endswith(\".\")\n",
    "            or sentences[i].endswith(\"?\")\n",
    "            or sentences[i].endswith(\"!\")\n",
    "            or i == len(sentences) - 1\n",
    "        ):\n",
    "            endTime_ms = endTimes_ms[i]\n",
    "            filtered_sentences.append(sentence.strip())\n",
    "            filtered_startTimes_ms.append(startTime_ms)\n",
    "            filtered_endTimes_ms.append(endTime_ms)\n",
    "            startTime_ms = -1\n",
    "            endTime_ms = -1\n",
    "            sentence = \"\"\n",
    "\n",
    "    processed_transcript = []\n",
    "    for i in range(len(filtered_sentences)):\n",
    "        processed_transcript.append(\n",
    "            {\n",
    "                \"sentence_startTime\": filtered_startTimes_ms[i],\n",
    "                \"sentence_endTime\": filtered_endTimes_ms[i],\n",
    "                \"sentence\": filtered_sentences[i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return processed_transcript\n",
    "\n",
    "def time_to_ms(time_str):\n",
    "    h, m, s, ms = re.split(r\"[:|.]\", time_str)\n",
    "    return int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339e88a-4e83-4e1d-a3ad-486350f7de05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(video['transcript'].vtt_file, encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "processed_transcript = process_transcript(transcript)\n",
    "\n",
    "print(colored(\"===== [Complete Sentences W/ Timestamps] ======\\n\", \"green\"))\n",
    "processed_transcript[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b0a6d-24f8-41f9-ae5a-de529fe02981",
   "metadata": {},
   "source": [
    "## 문장을 shots에 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080549d-e492-45a4-801c-5a311e00eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shot_transcript(shot_startTime, shot_endTime, transcript):\n",
    "    relevant_transcript = \"\"\n",
    "    for item in transcript:\n",
    "        if item[\"sentence_startTime\"] >= shot_endTime:\n",
    "            break\n",
    "        if item[\"sentence_endTime\"] <= shot_startTime:\n",
    "            continue\n",
    "        delta_start = max(item[\"sentence_startTime\"], shot_startTime)\n",
    "        delta_end = min(item[\"sentence_endTime\"], shot_endTime)\n",
    "        if delta_end - delta_start >= 500:\n",
    "            relevant_transcript += item[\"sentence\"] + \"; \"\n",
    "    return relevant_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34349e7c-1135-4956-8819-74d60dcb5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"===== [Complete sentence align to every shot] ======\\n\", \"green\"))\n",
    "\n",
    "for shot in sampled_shots:\n",
    "    relevant_transcript = add_shot_transcript(shot['start_ms'], shot['end_ms'], processed_transcript)\n",
    "    shot['transcript'] = relevant_transcript\n",
    "    print({\n",
    "        'shot_id': shot['id'],\n",
    "        'transcript': relevant_transcript\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23263f99-db96-45e3-9cf8-f826303a9c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66d60506-0258-4a62-9c44-8b88e79d1d18",
   "metadata": {},
   "source": [
    "## Shot 설명 생성\n",
    "shot에 속하는 프레임 이미지에서 주요 요소를 추출하기 위해 LLM을 활용합니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "⏳ 이 단계는 composite 프레임 이미지에서 주요 요소를 추출하기 위해 Anthropic Claude Sonnet 3.5 모델을 활용하며, 실행하는 데 2분 이상 걸릴 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e8b88-2b67-4ce8-ac69-6b9dfe2f30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shot_description(model_id, composite_images, celebrities):\n",
    "\n",
    "   #  system_prompts = [{\"text\": \"\"\"\n",
    "   #  You are an expert video content analyst specializing in generating rich, contextual metadata for semantic search systems. \n",
    "   #  Your task is to analyze video shots presented in a sequence of frame images and provide a detailed but concise description \n",
    "   #  of a video shot based on the given frame images. Focus on creating a cohesive narrative of the entire shot rather than \n",
    "   #  describing each frame individually.\n",
    "   #  \"\"\"}]\n",
    "    \n",
    "   #  prompt = \"\"\"\n",
    "   #  <celebrities>\n",
    "   #  {{CELEBRITIES}}\n",
    "   #  </celebrities>\n",
    "    \n",
    "   # Context:\n",
    "   #  - Each image contains a sequence of consecutive video frames, read from left to right and top to bottom.\n",
    "   #  - Your goal is to generate metadata that makes the video content easily discoverable through various search queries.\n",
    "   #  - ALL identified <celebrities> MUST be integrated into descriptions.\n",
    "\n",
    "   #  STRICT VALIDATION REQUIREMENTS:\n",
    "   #  1. STOP AND CHECK BEFORE OUTPUTTING:\n",
    "   #     - Are there any names in the \"celebrities\" field? \n",
    "   #     - If YES, verify these names appear in the description text\n",
    "   #     - If NO match found, rewrite description to include celebrity names\n",
    "       \n",
    "   #  2. REQUIRED FORMAT FOR DESCRIPTIONS WITH CELEBRITIES:\n",
    "   #     - MUST start with celebrity names and their actions\n",
    "   #     - Example format: \"[Celebrity Name] appears/is shown/portrays...\"\n",
    "   #     - NEVER output generic terms (\"a man\", \"someone\") when celebrity identity is known\n",
    "    \n",
    "   #  3. AUTOMATIC ERROR CHECKING:\n",
    "   #     If (celebrities.length > 0):\n",
    "   #        If (description does not contain ALL celebrity names):\n",
    "   #           MUST rewrite description\n",
    "    \n",
    "   #  Description Template When Celebrities Present:\n",
    "   #  \"[Celebrity Name 1] [action/appearance], [clothing/setting details]. [Additional context]. [Celebrity Name 2 if present] [their action/appearance]...\"\n",
    "\n",
    "   #  REQUIRED PRE-SUBMISSION CHECKS:\n",
    "   #  □ Are all celebrity names from <celebrities> present in description?\n",
    "   #  □ Does description start with a celebrity name (not generic terms)?\n",
    "   #  □ Are all celebrities actively described (not passively mentioned)?\n",
    "   #  □ Have you avoided generic terms like \"a man\" or \"someone\"?\n",
    "    \n",
    "   #  INCORRECT (Reject):\n",
    "   #  \"A man in a white shirt and tie is shown...\"\n",
    "   #  (When celebrities field contains \"Kevin Kilner\")\n",
    "    \n",
    "   #  CORRECT (Accept):\n",
    "   #  \"Kevin Kilner appears in a white shirt and tie...\"\n",
    "    \n",
    "   #  Input Description:\n",
    "   #  <input_description>\n",
    "   #  - Sequence of images representing video frames\n",
    "   #  - List of known celebrities (if applicable)\n",
    "   #  </input_description>\n",
    "\n",
    "   #  Step-by-step Instructions:\n",
    "   #  <instructions>\n",
    "   #  1. Analyze the visual content:\n",
    "   #     a. First priority: Identify any celebrities or notable individuals\n",
    "   #     b. Check for dark/empty frames:\n",
    "   #        - If frames are black or empty, use specialized template\n",
    "   #        - Set appropriate technical descriptors\n",
    "   #        - Mark confidence scores as 100 for verified empty content\n",
    "   #        - Use \"None\" or \"Undefined\" for inapplicable categories\n",
    "   #     c. If celebrities identified, prepare description using required template\n",
    "   #     d. Identify key objects, actions, and settings in the scene\n",
    "   #     e. Detect any text or graphics visible in the frames\n",
    "   #     f. Recognize brands, logos, or products\n",
    "    \n",
    "   #  2. Determine temporal aspects:\n",
    "   #     a. Identify any scene transitions or significant changes in the sequence\n",
    "   #     b. Note any recurring elements across multiple frames\n",
    "    \n",
    "   #  3. Synthesize a detailed description:\n",
    "   #     a. REQUIRED: If celebrities present, use template format\n",
    "   #     b. MUST start with celebrity identification and actions\n",
    "   #     c. Integrate setting, atmosphere, and context\n",
    "   #     d. Include all identified celebrities in natural narrative flow\n",
    "   #     e. Run pre-submission checks before finalizing\n",
    "    \n",
    "   #  4. Final Validation:\n",
    "   #     a. Run through pre-submission checklist\n",
    "   #     b. Verify celebrity integration in description (if applicable)\n",
    "   #     c. Confirm no generic terms used for identified people\n",
    "   #     d. For dark frames, verify all technical descriptors are accurate\n",
    "    \n",
    "   #  5. Special Cases Handling:\n",
    "   #      a. For dark/empty frames:\n",
    "   #         - Use technical description template\n",
    "   #         - Set appropriate null values\n",
    "   #         - Mark relevant technical indicators\n",
    "   #         - Note possible transition purpose\n",
    "   #      b. For partially visible content:\n",
    "   #         - Note visibility issues\n",
    "   #         - Describe what can be confidently identified\n",
    "   #         - Adjust confidence scores accordingly\n",
    "    \n",
    "   #  6. Final Output Preparation:\n",
    "   #      a. Skip the preamble; go straight into the description\n",
    "   #      b. Check for proper formatting and syntax\n",
    "   #  \"\"\".replace(\"{{CELEBRITIES}}\", celebrities)\n",
    "\n",
    "    system_prompts = [{\"text\": \"\"\"\n",
    "    당신은 의미론적 검색 시스템을 위한 풍부한 맥락적 메타데이터를 생성하는 전문 비디오 콘텐츠 분석가입니다. \n",
    "    당신의 임무는 프레임 이미지 시퀀스로 제시된 비디오 shots를 분석하고 주어진 프레임 이미지를 기반으로 \n",
    "    비디오 shot에 대한 상세하지만 간결한 설명을 제공하는 것입니다. 각 프레임을 개별적으로 \n",
    "    설명하기보다는 전체 shot의 일관된 내러티브를 만드는 데 집중하세요.\n",
    "    \"\"\"}]\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    <celebrities>\n",
    "    {{CELEBRITIES}}\n",
    "    </celebrities>\n",
    "    \n",
    "   맥락:\n",
    "    - 각 이미지는 왼쪽에서 오른쪽, 위에서 아래로 읽는 연속적인 비디오 프레임의 시퀀스를 포함합니다.\n",
    "    - 당신의 목표는 다양한 검색 쿼리를 통해 비디오 콘텐츠를 쉽게 발견할 수 있게 하는 메타데이터를 생성하는 것입니다.\n",
    "    - 식별된 모든 <celebrities>는 반드시 설명에 통합되어야 합니다.\n",
    "\n",
    "    엄격한 검증 요구사항:\n",
    "    1. 출력 전 중지 및 확인:\n",
    "       - \"celebrities\" 필드에 이름이 있습니까? \n",
    "       - 있다면, 이 이름들이 설명 텍스트에 나타나는지 확인\n",
    "       - 일치하는 것이 없다면, 유명인 이름을 포함하도록 설명 다시 작성\n",
    "       \n",
    "    2. 유명인이 있는 설명에 대한 필수 형식:\n",
    "       - 반드시 유명인 이름과 그들의 행동으로 시작해야 함\n",
    "       - 예시 형식: \"[Celebrity Name] 등장/보여짐/묘사...\"\n",
    "       - 유명인 신원을 알 때는 절대 일반적인 용어(\"한 남자\", \"누군가\")를 출력하지 않음\n",
    "    \n",
    "    3. 자동 오류 검사:\n",
    "       If (celebrities.length > 0):\n",
    "          If (설명이 모든 유명인 이름을 포함하지 않음):\n",
    "             반드시 설명 다시 작성\n",
    "    \n",
    "    유명인이 있을 때의 설명 템플릿:\n",
    "    \"[Celebrity Name 1] [행동/등장], [의상/배경 세부사항]. [추가 맥락]. [Celebrity Name 2가 있다면] [그들의 행동/등장]...\"\n",
    "\n",
    "    필수 제출 전 확인사항:\n",
    "    □ <celebrities>의 모든 유명인 이름이 설명에 있습니까?\n",
    "    □ 설명이 유명인 이름으로 시작합니까(일반적인 용어가 아님)?\n",
    "    □ 모든 유명인이 능동적으로 설명되었습니까(수동적으로 언급된 것이 아님)?\n",
    "    □ \"한 남자\" 또는 \"누군가\"와 같은 일반적인 용어를 피했습니까?\n",
    "    \n",
    "    잘못됨 (거부):\n",
    "    \"흰 셔츠와 넥타이를 입은 한 남자가 보입니다...\"\n",
    "    (celebrities 필드에 \"Kevin Kilner\"가 있을 때)\n",
    "    \n",
    "    올바름 (수락):\n",
    "    \"Kevin Kilner가 흰 셔츠와 넥타이를 입고 등장합니다...\"\n",
    "    \n",
    "    입력 설명:\n",
    "    <input_description>\n",
    "    - 비디오 프레임을 나타내는 이미지 시퀀스\n",
    "    - 알려진 유명인 목록 (해당되는 경우)\n",
    "    </input_description>\n",
    "\n",
    "    단계별 지침:\n",
    "    <instructions>\n",
    "    1. 시각적 콘텐츠 분석:\n",
    "       a. 첫 번째 우선순위: 유명인 또는 주목할 만한 개인 식별\n",
    "       b. 어둡거나 빈 프레임 확인:\n",
    "          - 프레임이 검거나 비어 있다면, 특수 템플릿 사용\n",
    "          - 적절한 기술적 설명자 설정\n",
    "          - 확인된 빈 콘텐츠에 대해 신뢰도 점수를 100으로 설정\n",
    "          - 해당되지 않는 카테고리에 대해 \"None\" 또는 \"Undefined\" 사용\n",
    "       c. 유명인이 식별되면, 필수 템플릿을 사용하여 설명 준비\n",
    "       d. 장면의 주요 객체, 행동 및 배경 식별\n",
    "       e. 프레임에서 보이는 텍스트나 그래픽 감지\n",
    "       f. 브랜드, 로고 또는 제품 인식\n",
    "    \n",
    "    2. 시간적 측면 결정:\n",
    "       a. 장면 전환이나 시퀀스의 중요한 변화 식별\n",
    "       b. 여러 프레임에 걸쳐 반복되는 요소 기록\n",
    "    \n",
    "    3. 상세한 설명 종합:\n",
    "       a. 필수: 유명인이 있다면, 템플릿 형식 사용\n",
    "       b. 반드시 유명인 식별과 행동으로 시작\n",
    "       c. 배경, 분위기 및 맥락 통합\n",
    "       d. 자연스러운 내러티브 흐름에 모든 식별된 유명인 포함\n",
    "       e. 최종화하기 전에 제출 전 확인 실행\n",
    "    \n",
    "    4. 최종 검증:\n",
    "       a. 제출 전 체크리스트 실행\n",
    "       b. 설명에서 유명인 통합 확인 (해당되는 경우)\n",
    "       c. 식별된 사람들에 대해 일반적인 용어를 사용하지 않았는지 확인\n",
    "       d. 어두운 프레임의 경우, 모든 기술적 설명자가 정확한지 확인\n",
    "    \n",
    "    5. 특수 사례 처리:\n",
    "        a. 어둡거나 빈 프레임의 경우:\n",
    "           - 기술적 설명 템플릿 사용\n",
    "           - 적절한 null 값 설정\n",
    "           - 관련 기술적 지표 표시\n",
    "           - 가능한 전환 목적 기록\n",
    "        b. 부분적으로 보이는 콘텐츠의 경우:\n",
    "           - 가시성 문제 기록\n",
    "           - 확실히 식별할 수 있는 것 설명\n",
    "           - 그에 따라 신뢰도 점수 조정\n",
    "    \n",
    "    6. 최종 출력 준비:\n",
    "        a. 서문 건너뛰고 바로 설명으로 들어가기\n",
    "        b. 적절한 형식과 구문 확인\n",
    "    \"\"\".replace(\"{{CELEBRITIES}}\", celebrities)\n",
    "\n",
    "\n",
    "    message = {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[]\n",
    "    }\n",
    "                         \n",
    "    for composite in composite_images:\n",
    "\n",
    "        with open(composite['file'], \"rb\") as image_file:\n",
    "            image_string = image_file.read()\n",
    "\n",
    "        message[\"content\"].append({\n",
    "            \"image\":{\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\":{\n",
    "                    \"bytes\": image_string\n",
    "                }\n",
    "            }\n",
    "                \n",
    "        })\n",
    "\n",
    "    message[\"content\"].append({\n",
    "        \"text\": prompt\n",
    "    })\n",
    "    \n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": .1}\n",
    "    \n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": 200}\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[message],\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    \n",
    "    return output_message[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca072a9b-c287-4fb2-8a42-be897a4a977a",
   "metadata": {},
   "source": [
    "shot 설명을 생성하기 위해 코드를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08d87b-2533-4823-a3b8-b4a2edd229ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# model_id = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "# model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "for shot in sampled_shots:\n",
    "    description = get_shot_description(\n",
    "        model_id = model_id, \n",
    "        composite_images = shot['composite_images'], \n",
    "        celebrities = shot['public_figure']\n",
    "    )\n",
    "    shot['shot_description'] = description\n",
    "\n",
    "print(colored(\"===== [Example caption] ======\\n\", \"green\"))\n",
    "\n",
    "example = random.choice(sampled_shots)\n",
    "example['shot_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d186e9-89d1-48b4-9b6b-91dc468a4b92",
   "metadata": {},
   "source": [
    "\n",
    "## Shots에 대한 임베딩 생성\n",
    "\n",
    "이 함수는 제공된 model_id를 기반으로 텍스트 또는 멀티모달 임베딩을 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177f3fe-2ae5-4bf7-8b28-17014b6e52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model_id, input_data):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    if 'text' in model_id:\n",
    "        body = json.dumps({\n",
    "            \"inputText\": input_data,\n",
    "            \"dimensions\": 1024,\n",
    "            \"normalize\": True\n",
    "        })\n",
    "    elif 'image' in model_id:\n",
    "        # Read image from file and encode it as base64 string.\n",
    "        with open(input_data, \"rb\") as image_file:\n",
    "            input_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "        \n",
    "        body = json.dumps({\n",
    "            \"inputImage\": input_image,\n",
    "            \"embeddingConfig\": {\n",
    "                \"outputEmbeddingLength\": 1024\n",
    "            }\n",
    "        })\n",
    "    else:\n",
    "        raise ValueError(\"Invalid embedding_type. Choose 'text' or 'image'.\")\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type,\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53d953-ea15-4d0f-9636-2e222e6303dd",
   "metadata": {},
   "source": [
    "## OpenSearch Serverless 벡터 인덱스 구축\n",
    "\n",
    "OpenSearch Serverless (OSS)는 AWS(Amazon Web Services)에서 제공하는 완전 관리형, 온디맨드 검색 및 분석 서비스입니다. 인프라 관리 없이 OpenSearch 클러스터를 배포, 운영 및 확장할 수 있습니다.\n",
    "\n",
    "OpenSearch의 인덱스는 유사한 특성을 공유하는 문서의 모음입니다. 이 경우, 우리는 벡터 임베딩을 효율적으로 저장하고 검색하도록 설계된 벡터 인덱스에 초점을 맞추고 있습니다.\n",
    "\n",
    "### 인덱스 구성은 다음과 같습니다\n",
    "\n",
    "인덱스는 다음 속성들을 포함합니다:\n",
    "- `video_path`: 비디오 파일 경로 (텍스트 필드)\n",
    "- `shot_id`: 각 shot의 고유 식별자 (텍스트 필드)\n",
    "- `shot_startTime`: shot의 시작 시간 (텍스트 필드)\n",
    "- `shot_endTime`: shot의 종료 시간 (텍스트 필드)\n",
    "- `shot_description`: shot의 설명 (텍스트 필드)\n",
    "- `shot_celebrities`: shot에서 식별된 유명인 (텍스트 필드)\n",
    "- `shot_transcript`: shot의 오디오 Transcript (텍스트 필드)\n",
    "\n",
    "이것들은 각 검색 쿼리에 대한 shots를 검색하고 결과를 필터링하는 데 사용할 수 있는 메타데이터 필드입니다.\n",
    "\n",
    "- `shot_image_vector`: shot 이미지의 벡터 표현\n",
    "- `shot_desc_vector`: shot 설명의 벡터 표현\n",
    "- `transcript_vector`: transcript의 벡터 표현\n",
    "\n",
    "`shot_image_vector`, `transcript_vector` 및 `shot_desc_vector`는 `knn_vector` 필드로 구성됩니다. 이 두 필드를 사용하여 텍스트 쿼리나 입력 이미지에 해당하는 가장 일치하는 카메라 shot을 찾기 위해 벡터 유사도 검색을 수행할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d4371-bd0f-4cf3-bb8c-fc8009ef4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish client connection OSS\n",
    "def get_opensearch_client(host, region):\n",
    "    host = host.split(\"://\")[1] if \"://\" in host else host\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    auth = AWSV4SignerAuth(credentials, region, \"aoss\")\n",
    "\n",
    "    oss_client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        pool_maxsize=20,\n",
    "    )\n",
    "\n",
    "    return oss_client\n",
    "\n",
    "\n",
    "# Create OpenSearch Severless Index\n",
    "def create_opensearch_index(oss_client, index_name, len_embedding=1024):\n",
    "\n",
    "    exist = oss_client.indices.exists(index_name)\n",
    "    if not exist:\n",
    "        print(\"Creating index\")\n",
    "        index_body = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"video_path\": {\"type\": \"text\"},\n",
    "                    \"shot_id\": {\"type\": \"text\"},\n",
    "                    \"shot_startTime\": {\"type\": \"text\"},\n",
    "                    \"shot_endTime\": {\"type\": \"text\"},\n",
    "                    \"shot_description\": {\"type\": \"text\"},\n",
    "                    \"shot_celebrities\": {\"type\": \"text\"},\n",
    "                    \"shot_transcript\": {\"type\": \"text\"},\n",
    "                    \"shot_image_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                    \"shot_desc_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                    \"transcript_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"number_of_shards\": 2,\n",
    "                    \"knn.algo_param\": {\"ef_search\": 512},\n",
    "                    \"knn\": True,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        response = oss_client.indices.create(index_name, body=index_body)\n",
    "\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbfcdb-00f2-4d15-adcb-5f63a3d7a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"video_search_index\"\n",
    "\n",
    "oss_client = get_opensearch_client(oss_host, region)\n",
    "create_opensearch_index(oss_client, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687d533-7027-48c1-9999-dc82b4b3db94",
   "metadata": {},
   "source": [
    "OpenSearch Serverless (OSS)에 데이터를 수집하기 위한 수집 페이로드를 생성하기 위해 각 shot을 반복합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119aa56-0a5f-4caa-a534-995f5fc8c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shot in sampled_shots:\n",
    "\n",
    "    # generate text embedding from description\n",
    "    shot_desc_vector = get_embedding(\n",
    "        model_id='amazon.titan-embed-text-v2:0',\n",
    "        input_data=shot['shot_description']\n",
    "    )\n",
    "\n",
    "    # generate mm embedding from composite frames\n",
    "    shot_image_vector = get_embedding(\n",
    "        model_id='amazon.titan-embed-image-v1',\n",
    "        input_data=shot['composite_images'][0]['file']\n",
    "    )\n",
    "\n",
    "    index_obj = {\n",
    "                \"video_path\": video_path,\n",
    "                \"shot_id\": shot['id'],\n",
    "                \"shot_startTime\": shot['start_ms'],\n",
    "                \"shot_endTime\": shot['end_ms'],\n",
    "                \"shot_description\": shot['shot_description'],\n",
    "                \"shot_celebrities\": shot['public_figure'],\n",
    "                \"shot_transcript\": shot['transcript'],\n",
    "                \"shot_desc_vector\": shot_desc_vector,\n",
    "                \"shot_image_vector\": shot_image_vector,\n",
    "            }\n",
    "\n",
    "    # generate text embedding from transcript\n",
    "    if shot['transcript']:\n",
    "        \n",
    "        transcript_vector = get_embedding(\n",
    "            model_id='amazon.titan-embed-text-v2:0',\n",
    "            input_data=shot['transcript']\n",
    "        )\n",
    "        \n",
    "        index_obj[\"transcript_vector\"] = transcript_vector\n",
    "        \n",
    "    #build the payload to index in OSS\n",
    "    payload = json.dumps(index_obj)\n",
    "    response = oss_client.index(\n",
    "                    index=index_name,\n",
    "                    body=payload,\n",
    "                    params={\"timeout\": 60},\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3fc17-e5c4-42e7-ba00-eff91e92ee5f",
   "metadata": {},
   "source": [
    "## 비디오 의미론적 검색 수행\n",
    "\n",
    "OpenSearch에 삽입된 데이터가 검색될 준비가 되었는지 확인하기 위해 기다립니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073b6ac-a09c-4a36-bd65-357671105ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for the recent inserted data to be searchable in OpenSearch...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        result = oss_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "        if result['hits']['total']['value'] >= len(sampled_shots):\n",
    "            print(\"\\nData is now available for search!\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdc04d-d906-490f-ad9d-03dd559fe24b",
   "metadata": {},
   "source": [
    "자연어 쿼리를 사용한 검색을 시연합니다. 기본적으로 Question Bank에서 질문이 무작위로 샘플링됩니다. 그런 다음 검색은 OSS 인덱스의 시각적(`shot_desc_vector`) 및 오디오(`transcript_vector`) 데이터를 모두 결합합니다. 이 프로세스는 이러한 유형의 비디오에서 최적화된 결과를 위해 시각적(75%)을 오디오 전사(25%)보다 우선시하도록 콘텐츠 가중치를 적용합니다. 이 결합된 검색 예시는 이러한 매개변수를 다른 사용자 검색 의도에 더 잘 맞도록 조정하여 전반적인 검색 관련성과 사용자 만족도를 향상시킬 수 있는 방법을 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a8fa9-4d03-47f3-917e-9fc9a93fc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_bank = {\n",
    "    \"Netflix_Open_Content_Meridian.mp4\": [\n",
    "            \"Scott driving a car\",\n",
    "            \"Elyse staring through the rear view mirror\",\n",
    "            \"Kevin opening a car door\",\n",
    "            \"Lightning strike from the sky\"\n",
    "        ]    \n",
    "}\n",
    "# check if questions are available for the video\n",
    "assert video['path'] in question_bank, f\"****[{video['path']}]*** is not a supported video.\"\n",
    "\n",
    "# Sample a question from the question bank. You can change to use your own.\n",
    "user_query = random.choice(question_bank[video['path']])\n",
    "\n",
    "print(\"Sampled query: \", colored(user_query, \"green\"))\n",
    "\n",
    "query_embedding = get_embedding('amazon.titan-embed-text-v2:0', user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bee18-b641-4a08-a9fc-a6c731214630",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_query = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"query\": {\"match_all\": {}},\n",
    "                            \"script\": {\n",
    "                                \"lang\": \"knn\",\n",
    "                                \"source\": \"knn_score\",\n",
    "                                \"params\": {\n",
    "                                    \"field\": \"shot_desc_vector\",\n",
    "                                    \"query_value\": query_embedding,\n",
    "                                    \"space_type\": \"cosinesimil\",\n",
    "                                },\n",
    "                            },\n",
    "                            \"boost\": 3.0\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"query\": {\"match_all\": {}},\n",
    "                            \"script\": {\n",
    "                                \"lang\": \"knn\",\n",
    "                                \"source\": \"knn_score\",\n",
    "                                \"params\": {\n",
    "                                    \"field\": \"transcript_vector\",\n",
    "                                    \"query_value\": query_embedding,\n",
    "                                    \"space_type\": \"cosinesimil\",\n",
    "                                },\n",
    "                            },\n",
    "                            \"boost\": 1.0\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"video_path\",\n",
    "            \"shot_id\",\n",
    "            \"shot_startTime\",\n",
    "            \"shot_endTime\",\n",
    "            \"shot_description\",\n",
    "            \"shot_celebrities\",\n",
    "            \"shot_transcript\",\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1323c3c-3dc0-4745-94cf-91db90ab32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = oss_client.search(body=aoss_query, index=index_name)\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "responses = []\n",
    "for hit in hits:\n",
    "    if hit[\"_score\"] >= 0:  # Set score threshold\n",
    "        responses.append(\n",
    "            {\n",
    "                \"video_path\": hit[\"_source\"][\"video_path\"],\n",
    "                \"shot_id\": hit[\"_source\"][\"shot_id\"],\n",
    "                \"shot_startTime\": hit[\"_source\"][\"shot_startTime\"],\n",
    "                \"shot_endTime\": hit[\"_source\"][\"shot_endTime\"],\n",
    "                \"shot_description\": hit[\"_source\"][\"shot_description\"],\n",
    "                \"shot_celebrities\": hit[\"_source\"][\"shot_celebrities\"],\n",
    "                \"shot_transcript\": hit[\"_source\"][\"shot_transcript\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651fbe9f-5d65-4bae-87ac-efc9bf17db36",
   "metadata": {},
   "source": [
    "상위 `x`개의 검색 결과를 표시합니다. 아래의 헬퍼 함수는 shots를 독립적으로 그리고 원본 비디오의 일부로 렌더링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a1096-0014-42c3-887a-6f8aa5089253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_with_original_video(top_hit):\n",
    "    \n",
    "    video_path = top_hit['video_path']\n",
    "    video_start = top_hit['shot_startTime']/1000\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <video alt=\"test\" controls id=\"{top_hit['shot_id']}\" width=\"100\" >\n",
    "      <source src=\"{video_path}\">\n",
    "    </video>\n",
    "    \n",
    "    <script>\n",
    "    video = document.getElementById(\"{top_hit['shot_id']}\")\n",
    "    video.currentTime = {video_start};\n",
    "    </script>\n",
    "    \"\"\"))\n",
    "    \n",
    "    \n",
    "def display_shot_segment_results(response, top_results=2):\n",
    "\n",
    "    css_style = \"\"\"\n",
    "    <style>\n",
    "        .video-container {\n",
    "            display: flex;\n",
    "            justify-content: space-around;\n",
    "            flex-wrap: wrap;\n",
    "        }\n",
    "        .video {\n",
    "            flex: 1;\n",
    "            min-width: 200px;\n",
    "            margin: 10px;\n",
    "        }\n",
    "        video {\n",
    "            width: 100%;\n",
    "            height: auto;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    html_content = \"<div class='video-container'>\\n\"\n",
    "    \n",
    "    for idx in range(top_results):\n",
    "        # convert format of timestamps\n",
    "        video_start = responses[idx]['shot_startTime']/1000\n",
    "        video_end = responses[idx]['shot_endTime']/1000\n",
    "    \n",
    "        converted_start = str(datetime.timedelta(seconds = video_start))\n",
    "        converted_end = str(datetime.timedelta(seconds = video_end))\n",
    "        output_file = f\"shot-{responses[idx]['shot_id']}.mp4\"\n",
    "        _ = subprocess.run(\n",
    "            [\n",
    "                \"/usr/bin/ffmpeg\",\n",
    "                \"-ss\",\n",
    "                converted_start,\n",
    "                \"-to\",\n",
    "                converted_end,\n",
    "                \"-i\",\n",
    "                responses[idx]['video_path'], # path to video\n",
    "                \"-c\",\n",
    "                \"copy\",\n",
    "                output_file,\n",
    "            ],\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        html_content += f\"\"\"\n",
    "            <div class=\"video\">\n",
    "                <h5>Shot Id: {responses[idx]['shot_id']}, Time Range: {video_start} ms - {video_end} ms</p>\n",
    "                <video controls>\n",
    "                    <source src=\"{output_file}\" type=\"video/mp4\">\n",
    "                    Your browser does not support the video tag.\n",
    "                </video>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    # render the shots\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    display(HTML(css_style + html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1c2e0-06ca-4ed3-83c1-9d2f1d898f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"====== [TOP results] =======\", 'green'))\n",
    "display_shot_segment_results(responses, top_results=3)\n",
    "\n",
    "print(colored(\"\\n====== [Display top hit as part Of original video] =======\\n\", 'green'))\n",
    "\n",
    "top_hit = responses[0]\n",
    "\n",
    "video_start = top_hit['shot_startTime']/1000\n",
    "video_end = top_hit['shot_endTime']/1000\n",
    "\n",
    "print(f\"Shot Id: {top_hit['shot_id']}, Time Range: {video_start} ms - {video_end} ms\")\n",
    "render_with_original_video(top_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8579f37-81b6-4b45-b30e-d6b5452a0585",
   "metadata": {},
   "source": [
    "### 멀티모달 비디오 검색\n",
    "\n",
    "콘텐츠 분석 및 비디오 편집 워크플로우에서, 비디오 제작자는 핵심 순간을 완벽하게 포착하는 특정 프레임이나 이미지를 발견할 수 있지만 수 시간의 원본 영상 내에서 그 위치를 찾아야 할 수 있습니다. 이 Generative AI 기술을 사용한 멀티모달 비디오 검색을 통해, 제작자는 프레임이나 이미지를 입력하여 프레임이 발생하는 정확한 타임스탬프를 빠르게 찾을 수 있습니다.\n",
    "\n",
    "다음 검색 예시에서는 사용 가능한 shots에서 프레임을 무작위로 샘플링한 다음, 프레임 이미지를 사용하여 비디오에서 shot을 식별할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5374481-3a36-49d9-9cc5-bcd6967a73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_image(shots):\n",
    "    shot = random.choice(shots) if shots else None\n",
    "    frame_locations = shot['composite_images'][0]['layout']\n",
    "    frame_info = random.choice(frame_locations) if frame_locations else None\n",
    "    return frame_info[0]\n",
    "\n",
    "random_frame = random_sample_image(sampled_shots)\n",
    "image = Image.open(random_frame)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a33089-5044-4543-bbf3-cc98d354bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = get_embedding('amazon.titan-embed-image-v1', random_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278349e-f96d-4a80-943b-32d8d4fe3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_query = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"bool\": {\"should\": []}},\n",
    "                \"script\": {\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"shot_image_vector\",\n",
    "                        \"query_value\": image_embedding,\n",
    "                        \"space_type\": \"cosinesimil\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"video_path\",\n",
    "            \"shot_id\",\n",
    "            \"shot_startTime\",\n",
    "            \"shot_endTime\",\n",
    "            \"shot_description\",\n",
    "            \"shot_celebrities\",\n",
    "            \"shot_transcript\",\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d0858-63a3-449b-9829-a13716487db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = oss_client.search(body=aoss_query, index=index_name)\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "responses = []\n",
    "for hit in hits:\n",
    "    if hit[\"_score\"] >= 0:  # Set score threshold\n",
    "        responses.append(\n",
    "            {\n",
    "                \"video_path\": hit[\"_source\"][\"video_path\"],\n",
    "                \"shot_id\": hit[\"_source\"][\"shot_id\"],\n",
    "                \"shot_startTime\": hit[\"_source\"][\"shot_startTime\"],\n",
    "                \"shot_endTime\": hit[\"_source\"][\"shot_endTime\"],\n",
    "                \"shot_description\": hit[\"_source\"][\"shot_description\"],\n",
    "                \"shot_celebrities\": hit[\"_source\"][\"shot_celebrities\"],\n",
    "                \"shot_transcript\": hit[\"_source\"][\"shot_transcript\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc4dc6-c525-4d50-81b1-48b18a30b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"====== [TOP results] =======\", 'green'))\n",
    "display_shot_segment_results(response, top_results=3)\n",
    "\n",
    "print(colored(\"\\n====== [Display top hit as part Of original video] =======\\n\", 'green'))\n",
    "\n",
    "top_hit = responses[0]\n",
    "\n",
    "video_start = top_hit['shot_startTime']/1000\n",
    "video_end = top_hit['shot_endTime']/1000\n",
    "\n",
    "print(f\"Shot Id: {top_hit['shot_id']}, Time Range: {video_start} ms - {video_end} ms\")\n",
    "render_with_original_video(top_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289023ae-6fe5-45c9-8420-d161dd8bd1a3",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "방금 생성한 벡터 인덱스를 제거하려면 아래 코드의 주석을 해제하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bd7ce-044e-4a54-a7c3-4b3bedbb9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     response = oss_client.indices.delete(index=index_name)\n",
    "#     print(f\"Index '{index_name}' deleted successfully\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting index '{index_name}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3595a36-7271-4076-8131-da13bb99dfb5",
   "metadata": {},
   "source": [
    "# 다음은 무엇인가요?\n",
    "\n",
    "다른 사용 사례를 시도해보거나, 완료했다면 [Additional Resources](09-resources.ipynb) 실습으로 계속 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85544b34-2954-449b-9eb0-3faff010d39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
