{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500968f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# AI 비디오 요약\n",
    "\n",
    "퍼블리셔와 방송사는 Facebook, Instagram, TikTok과 같은 소셜 미디어 플랫폼에서 짧은 형식의 비디오를 활용하여 새로운 시청자를 유치하고 추가 수익 기회를 창출할 수 있습니다.\n",
    "\n",
    "그러나 복잡한 콘텐츠 이해, 일관성 유지, 다양한 비디오 유형, 그리고 대량의 비디오를 다룰 때의 확장성 부족과 같은 과제들로 인해 비디오 요약 생성은 수동적이고 시간이 많이 소요되는 프로세스입니다. artificial intelligence (AI)와 machine learning (ML)을 활용한 자동화를 도입하면 자동 콘텐츠 분석, 실시간 처리, 맥락적 적응, 사용자 정의, 그리고 지속적인 AI/ML 시스템 개선을 통해 이 프로세스를 더 실행 가능하고 확장 가능하게 만들 수 있습니다.\n",
    "\n",
    "### 상위 수준 워크플로우\n",
    "\n",
    "![video summarization diagram](static/images/video-summarization-diagram.png)\n",
    "\n",
    "이 실습에서는 각 단계를 분석하고 [Amazon Transcribe](https://aws.amazon.com/pm/transcribe), [Amazon Bedrock](https://aws.amazon.com/bedrock), [Amazon Polly](https://aws.amazon.com/polly/) 및 [AWS Elemental MediaConvert](https://aws.amazon.com/mediaconvert/)와 같은 AWS 네이티브 서비스를 사용하여 비디오 요약을 어떻게 달성할 수 있는지 자세히 보여드릴 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4657c2",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6a13b-0469-4f5c-b6ca-e7d2c02b9978",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "이 노트북을 실행하려면 노트북 환경을 설정하고 오디오, 시각적, 의미론적 정보를 사용하여 비디오를 세그먼트화한 이전의 모든 기초 노트북을 실행했어야 합니다:\n",
    "1. [00-prerequisites.ipynb](00-prerequisites.ipyn)\n",
    "2. [01A-visual-segments-frames-shots-scenes.ipynb](01A-visual-segments-frames-shots-scenes.ipynb) \n",
    "3. [01B-audio-segments.ipynb](01B-audio-segments.ipynb) \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "⏳ 이 노트북에는 실행하는 데 5분 이상 걸리는 단계가 있습니다. 이 노트북을 실행하기 위해 위의 노트북 메뉴에서 ⏩ 모든 셀 실행 옵션을 선택하는 것을 권장합니다. \"03-video-summarization.ipynb의 커널을 다시 시작하시겠습니까? 모든 변수가 손실됩니다.\"라는 팝업이 표시됩니다. \"다시 시작\" 버튼을 클릭하여 계속 진행하세요. 노트북의 나머지 부분이 실행되는 동안 결과를 읽을 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab95822",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Retrieve saved values from previous notebooks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733d62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe018e1",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202fc75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json_repair\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "import base64\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e300a9a",
   "metadata": {},
   "source": [
    "## transcript에서 비디오 콘텐츠 요약\n",
    "\n",
    "[Amazon Bedrock](https://aws.amazon.com/bedrock/)와 함께 **Large Language Model (LLM)**을 사용하여 비디오의 콘텐츠를 요약합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6113bbd-6571-42c3-a0cc-cf9bca4e3410",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "accept = \"application/json\"\n",
    "content_type = \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24333345",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(video['transcript'].transcript_file, 'r') as file:\n",
    "    transcript_file = json.load(file)\n",
    "transcript = transcript_file['results']['transcripts'][0]['transcript']\n",
    "\n",
    "# model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "# model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "prompt = f\"\"\"Summarize the key points from the following video content in chronological order:\n",
    "\n",
    "{transcript} \n",
    "\n",
    "\\n\\nThe summary should only contain information present in the video content. Do not include any new or unrelated information.\n",
    "\n",
    "Important: Start the summary immediately without any introductory phrases. Begin directly with the first key point.\"\"\"\n",
    "\n",
    "body = json.dumps(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.25,\n",
    "        \"top_p\": 0.9,\n",
    "\n",
    "    }\n",
    ")\n",
    "response = bedrock_client.invoke_model(\n",
    "    body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    ")\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "summarized_text = response_body[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a94157",
   "metadata": {},
   "source": [
    "페이로드에 정의된 다른 매개변수로 엔드포인트를 호출하여 텍스트 요약에 영향을 줄 수 있습니다. 두 가지 중요한 매개변수는 `top_p`와 `temperature`입니다. `top_p`는 누적 확률을 기반으로 모델이 고려하는 토큰의 범위를 제어하는 데 사용되는 반면, `temperature`는 출력의 무작위성 수준을 제어합니다.\n",
    "\n",
    "모든 사용 사례에 대한 `top_p`와 `temperature`의 만능 조합은 없지만, 이전 예시에서는 높은 `top_p`와 낮은 `temperature`를 가진 샘플 값을 보여주었습니다. 이는 주요 정보에 초점을 맞춘 요약을 생성하고 원본 텍스트에서 벗어나는 것을 피하면서도 출력을 흥미롭게 유지하기 위해 약간의 창의적 변형을 도입합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07258089",
   "metadata": {},
   "source": [
    "Let's check the summarized video content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fc218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da241b73",
   "metadata": {},
   "source": [
    "## 음성 내레이션을 위한 메타데이터 생성\n",
    "\n",
    "다음 단계는 요약된 텍스트에서 음성을 생성하기 위해 [Amazon Polly](https://aws.amazon.com/polly/)로 시작합니다. Polly 작업의 출력은 MP3 파일과 [Speech Synthesis Markup Language (SSML)](https://docs.aws.amazon.com/polly/latest/dg/ssml.html)로 마크업된 문서입니다. 이 SSML 파일 내에는 특정 Polly 음성이 발성한 개별 문장의 지속 시간을 설명하는 필수 메타데이터가 캡슐화되어 있습니다. 이 오디오 지속 시간 정보를 통해 비디오 세그먼트의 길이를 정의할 수 있습니다. 이 경우에는 1:1 직접 대응이 사용됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61487d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "polly_client = boto3.client(\"polly\")\n",
    "voice_id = \"Matthew\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485e527-5e9c-489d-939b-98e5d1202f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = polly_client.synthesize_speech(\n",
    "    Engine=\"neural\",\n",
    "    OutputFormat=\"json\",\n",
    "    Text=summarized_text + \" This video is generated by Video Summarization Hub.\", \n",
    "    TextType=\"text\",\n",
    "    SpeechMarkTypes=[\"sentence\"],\n",
    "    VoiceId=voice_id,\n",
    ")\n",
    "\n",
    "stream_data = response['AudioStream'].read()\n",
    "polly_ssml = stream_data.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643bba8",
   "metadata": {},
   "source": [
    "다음은 SSML 형식의 Amazon Polly 합성 음성 출력입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f9d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "polly_ssml = polly_ssml.split(\"\\n\")\n",
    "polly_ssml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a0a09-886e-4bb6-9ad6-a6961f603da7",
   "metadata": {},
   "source": [
    "\n",
    "SSML 파일은 비디오 요약 문장과 Amazon Polly가 각 문장을 발성하는 데 걸리는 시간을 나타내는 음성 지속 시간을 모두 제공합니다. 다음 몇 단계에서 이러한 값을 추출하여 합성된 음성을 비디오 타임라인과 정렬할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7e41e-102a-45b9-bace-8f0e1a61e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_sentences = []\n",
    "speech_durations = []\n",
    "\n",
    "for i in range(len(polly_ssml) - 1):\n",
    "    curr = polly_ssml[i]\n",
    "    next = polly_ssml[i + 1]\n",
    "    if curr.strip() == \"\" or next.strip() == \"\":\n",
    "        continue\n",
    "    curr = json.loads(curr)\n",
    "    next = json.loads(next)\n",
    "    summarized_sentences.append(curr[\"value\"])\n",
    "    speech_durations.append(int(next[\"time\"]) - int(curr[\"time\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aeeb3e",
   "metadata": {},
   "source": [
    "## 가장 관련성 있는 비디오 shots/scenes 선택\n",
    "\n",
    "요약된 콘텐츠의 모든 문장과 일치하는 가장 관련성 있는 비디오 프레임 시퀀스를 선택해야 합니다. 따라서 우리는 두 텍스트가 얼마나 유사한지 결정하는 문장 유사도 작업을 수행하기 위해 텍스트 임베딩을 사용합니다.\n",
    "\n",
    "문장 유사도 모델은 입력 텍스트를 의미론적 정보를 캡처하는 벡터(임베딩)로 변환하고 그들 사이의 근접성이나 유사성을 계산합니다.\n",
    "\n",
    "이 단계에서는 [Amazon Bedrock](https://aws.amazon.com/bedrock/)와 함께 **Text Embedding Model**을 사용하여 원본 자막과 비디오 요약의 모든 문장에 대한 임베딩을 생성합니다.\n",
    "\n",
    "먼저, 원본 자막 파일을 가져와서 시작 시간과 종료 시간이 있는 문장으로 분해하기 위한 처리를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0c51a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(video['transcript'].vtt_file, 'r', encoding='utf-8') as file:\n",
    "    subtitle = file.read()\n",
    "\n",
    "if subtitle.startswith(\"WEBVTT\"):\n",
    "    subtitle = subtitle[len(\"WEBVTT\"):].lstrip()\n",
    "\n",
    "print(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bd7bb-aeaa-4cbf-a26c-97f9c19e3091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def srt_to_array(s):\n",
    "    \"\"\"\n",
    "    Converts the given transcription in SRT/WEBVTT format to list of sentences and their corresponding timecodes.\n",
    "    Args:\n",
    "       s - transcription in SRT/WEBVTT format.\n",
    "    Returns:\n",
    "       A list of dictionaries, where each dictionary represents a sentence and its corresponding start time and end time.\n",
    "    \"\"\"\n",
    "    sentences = [line.strip() for line in re.findall(r\"\\d+\\n.*?\\n(.*?)\\n\", s)]\n",
    "\n",
    "    def get_time(s):\n",
    "        return re.findall(r\"\\d{2}:\\d{2}:\\d{2}.\\d{3}\", s)\n",
    "\n",
    "    def time_to_ms(time_str):\n",
    "        match = re.match(r\"(\\d+):(\\d+):(\\d+)[.,](\\d+)\", time_str)\n",
    "        h, m, s, ms = match.groups()\n",
    "        return int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)\n",
    "\n",
    "    startTimes = get_time(s)[::2]\n",
    "    endTimes = get_time(s)[1::2]\n",
    "    startTimes_ms = [time_to_ms(time) for time in startTimes]\n",
    "    endTimes_ms = [time_to_ms(time) for time in endTimes]\n",
    "\n",
    "    complete_sentences = []\n",
    "    complete_startTimes_ms = []\n",
    "    complete_endTimes_ms = []\n",
    "\n",
    "    startTime_ms = -1\n",
    "    endTime_ms = -1\n",
    "    sentence = \"\"\n",
    "    for i in range(len(sentences)):\n",
    "        if startTime_ms == -1:\n",
    "            startTime_ms = startTimes_ms[i]\n",
    "        sentence += \" \" + sentences[i]\n",
    "        if (\n",
    "            sentences[i].endswith(\".\")\n",
    "            or sentences[i].endswith(\"?\")\n",
    "            or sentences[i].endswith(\"!\")\n",
    "            or i == len(sentences) - 1\n",
    "        ):\n",
    "            endTime_ms = endTimes_ms[i]\n",
    "            complete_sentences.append(sentence)\n",
    "            complete_startTimes_ms.append(startTime_ms)\n",
    "            complete_endTimes_ms.append(endTime_ms)\n",
    "            startTime_ms = -1\n",
    "            endTime_ms = -1\n",
    "            sentence = \"\"\n",
    "    processed_transcript = []\n",
    "    for i in range(len(complete_sentences)):\n",
    "        processed_transcript.append(\n",
    "            {\n",
    "                \"sentence_startTime\": complete_startTimes_ms[i],\n",
    "                \"sentence_endTime\": complete_endTimes_ms[i],\n",
    "                \"sentence\": complete_sentences[i],\n",
    "            }\n",
    "        )\n",
    "    return processed_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95579d-84c3-4e12-a3dc-174610a0e6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_transcript = srt_to_array(subtitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb3f31-538d-401a-ae82-a827b279ddfb",
   "metadata": {},
   "source": [
    "\n",
    "비디오의 원본 transcript에서 일부 문장을 시각화해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ebd7f-e9f7-4b0d-a502-a4530b6d31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentences = [item['sentence'] for item in processed_transcript]\n",
    "original_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d932e",
   "metadata": {},
   "source": [
    "다음으로, 원본 자막과 비디오 요약의 모든 문장에 대한 텍스트 임베딩을 생성합니다. 다음 코드는 Amazon Bedrock API를 사용한 텍스트 임베딩이 어떻게 작동하는지 보여주는 예시입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de19961",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_matching_sentences(original_sentences, summarized_sentences):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between the given original sentences and the summarized sentences.\n",
    "    Args:\n",
    "       original_sentences - sentences extacted from the original video\n",
    "       summarized_sentences - sentences extacted from the video summary\n",
    "    Return:\n",
    "       best_matching_indices - list of indices indicating which original sentence best matches each summarized sentence\n",
    "       similarity_matrix - sentences similarity matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    def np_cosine_similarity(original_embeddings, summarized_embeddings):\n",
    "        \"\"\"\n",
    "        We use `Cosine similarity` to measure similarities between two vectors.\n",
    "        \"\"\"\n",
    "        dot_products = np.dot(summarized_embeddings, original_embeddings.T)\n",
    "        summarized_norms = np.linalg.norm(summarized_embeddings, axis=1)\n",
    "        original_norms = np.linalg.norm(original_embeddings, axis=1)\n",
    "        similarity_matrix = dot_products / summarized_norms[:, None] / original_norms[None, :]\n",
    "        return similarity_matrix\n",
    "        \n",
    "    model_id = \"amazon.titan-embed-image-v1\"\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    original_embeddings = []\n",
    "    for str in original_sentences:\n",
    "        body = json.dumps({\"inputText\": str})\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    "        )\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        original_embeddings.append(response_body.get(\"embedding\"))\n",
    "    original_embeddings = np.array(original_embeddings)\n",
    "\n",
    "    summarized_embeddings = []\n",
    "    for str in summarized_sentences:\n",
    "        body = json.dumps({\"inputText\": str})\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    "        )\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        summarized_embeddings.append(response_body.get(\"embedding\"))\n",
    "    summarized_embeddings = np.array(summarized_embeddings)\n",
    "\n",
    "    similarity_matrix = np_cosine_similarity(original_embeddings, summarized_embeddings)\n",
    "    best_matching_indices = []\n",
    "    len_summarized_sentences = len(summarized_sentences)\n",
    "    len_original_sentences = len(original_sentences)\n",
    "\n",
    "    # Find the best matching sentences.\n",
    "    dp = np.zeros([len_summarized_sentences, len_original_sentences], dtype=float)\n",
    "    for i in range(0, len_summarized_sentences):\n",
    "        for j in range(0, len_original_sentences):\n",
    "            if i == 0:\n",
    "                dp[i][j] = similarity_matrix[i][j]\n",
    "            else:\n",
    "                max_score = -1\n",
    "                for k in range(0, j):\n",
    "                    if similarity_matrix[i][j] > 0 and dp[i - 1][k] > 0:\n",
    "                        max_score = max(\n",
    "                            max_score, similarity_matrix[i][j] + dp[i - 1][k]\n",
    "                        )\n",
    "                dp[i][j] = max_score\n",
    "\n",
    "    j = len_original_sentences\n",
    "\n",
    "    for i in range(len_summarized_sentences - 1, -1, -1):\n",
    "        arr = dp[i][:j]\n",
    "        idx = np.argmax(arr)\n",
    "        best_matching_indices.append(idx)\n",
    "        j = idx\n",
    "    best_matching_indices.reverse()\n",
    "\n",
    "    return best_matching_indices, similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97b08a-4ffa-4a51-a5f4-329210421681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_matching_indices, similarity_matrix = find_matching_sentences(original_sentences, summarized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef055d4",
   "metadata": {},
   "source": [
    "이는 다음과 같은 유사도 행렬 결과를 반환할 것입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed777de-070a-448c-90ee-3aaaff5dbbc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f486a9",
   "metadata": {},
   "source": [
    "이전 결과를 다음과 같이 해석할 수 있습니다: 행렬의 첫 번째 행은 요약된 콘텐츠의 첫 번째 문장에 해당하며 모든 열은 원본 텍스트의 문장들과의 유사도 점수를 보여줍니다. 유사도 값은 일반적으로 -1과 1 사이이며, 1은 벡터가 동일하거나 매우 유사함을 나타내고, 0은 벡터가 직교(상관관계 없음)하며 유사성이 없음을 나타내고, -1은 벡터가 정반대이거나 매우 다름을 나타냅니다.\n",
    "\n",
    "유사도 행렬에서, 우리는 요약된 콘텐츠의 각 문장에 대해 상위 k개의 가장 높은 유사도 점수를 식별하여 원본 텍스트의 가장 유사한 문장들과 정렬합니다. 원본 텍스트의 각 문장에는 원본 자막에 저장된 해당 타임스탬프(즉, startTime, endTime)도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f9d01-2b2c-40bf-b798-0655f50a25aa",
   "metadata": {},
   "source": [
    "각 요약된 문장에 대한 Polly 오디오의 지속 시간과 원본 자막 파일의 타임스탬프를 모두 통합함으로써, 각 요약된 문장에 해당하는 가장 관련성 있는 프레임의 타임스탬프 시퀀스를 선택할 수 있습니다. 요약된 문장에 대해 선택된 각 비디오 세그먼트의 길이는 해당 내레이션 오디오의 길이와 정렬될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2c77b-bb75-42b0-9d37-4e38ee24524b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_timecodes(best_matching_indices, idx, endTimes, duration, timecodes):\n",
    "    \"\"\"\n",
    "    Calculate the best start and end time for each summarized sentence aligned with the timecode from the original sentences\n",
    "    Args:\n",
    "      best_matching_indices - the indices from the original sentence that is most similar with the summarized sentences.\n",
    "      idx - index from the summarized sentences to process\n",
    "      endTimes - the endtime from the original sentences\n",
    "      duration - speech duration for the synthesized sentences from the summarized text\n",
    "      timecodes - timecode used for calculating the best placement for the summarized text within the video.\n",
    "    Return:\n",
    "      \n",
    "    \"\"\"\n",
    "    best_matching_idx = best_matching_indices[idx]\n",
    "    startTime = int(endTimes[best_matching_idx]) - duration\n",
    "    carry = max(0, timecodes[len(timecodes) - 1][1] - startTime)\n",
    "    startTime += carry\n",
    "    endTime = int(endTimes[best_matching_idx]) + carry\n",
    "    return startTime, endTime\n",
    "\n",
    "def ms_to_timecode(ms, drop_frame=False):\n",
    "    \"\"\"\n",
    "    Convert milliseconds to SMPTE timecode\n",
    "    Args:\n",
    "        ms: milliseconds\n",
    "        drop_frame: Boolean, True for drop frame, False for non-drop frame\n",
    "    Return:\n",
    "        string in HH:MM:SS:FF or HH:MM:SS;FF format\n",
    "    \"\"\"\n",
    "    total_frames = int(ms * (29.97 if drop_frame else 30) / 1000)\n",
    "    frames = total_frames % 30\n",
    "    \n",
    "    total_seconds = total_frames // 30\n",
    "    seconds = total_seconds % 60\n",
    "    \n",
    "    total_minutes = total_seconds // 60\n",
    "    minutes = total_minutes % 60\n",
    "    \n",
    "    hours = total_minutes // 60\n",
    "    separator = ';' if drop_frame else ':'    \n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}{separator}{frames:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690d90e-9cfb-412d-9098-43811db8b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_time = float(transcript_file[\"results\"][\"items\"][0][\"start_time\"]) * 1000\n",
    "\n",
    "timecodes = [[0, intro_time]]\n",
    "for i in range(len(summarized_sentences)):\n",
    "    startTime, endTime = get_timecodes(\n",
    "        best_matching_indices,\n",
    "        i,\n",
    "        [item['sentence_endTime'] for item in processed_transcript],\n",
    "        speech_durations[i],\n",
    "        timecodes,\n",
    "    )\n",
    "    timecodes.append([startTime, endTime])\n",
    "creditTime = endTime + 3500\n",
    "timecodes.append([endTime, creditTime])\n",
    "timecodes_text = \"\"\n",
    "for timecode in timecodes:\n",
    "    timecodes_text += (\n",
    "        ms_to_timecode(timecode[0], True)\n",
    "        + \",\"\n",
    "        + ms_to_timecode(timecode[1], True)\n",
    "        + \"\\n\"\n",
    "    )\n",
    "to_json = lambda s: [\n",
    "    {\"StartTimecode\": t1, \"EndTimecode\": t2}\n",
    "    for t1, t2 in (line.split(\",\") for line in s.split(\"\\n\") if line.strip())\n",
    "]\n",
    "timecodes = to_json(timecodes_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030714ef-a2bd-46e8-b895-a5d29785cfac",
   "metadata": {},
   "source": [
    "다음은 AWS Elemental MediaConvert 입력 클리핑에 사용될 생성된 타임코드입니다:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a603b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timecodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad03bf-b48b-4e0e-9f35-bdd629f12468",
   "metadata": {},
   "source": [
    "이제 Amazon Polly를 사용하여 비디오 요약에서 MP3 형식의 오디오 내레이션을 생성할 수 있습니다. SSML 호환성을 위해 요약된 텍스트의 특수 문자를 이스케이프하고 인트로 타이밍을 위한 적절한 breaks로 SSML 마크업을 생성하는 것을 잊지 마세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7972d-f9d5-4bdf-8a18-be1e7cea87d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "escaped_summarized_text = (\n",
    "        summarized_text.replace(\"&\", \"&amp;\")\n",
    "        .replace('\"', \"&quot;\")\n",
    "        .replace(\"'\", \"&apos;\")\n",
    "        .replace(\"<\", \"&lt;\")\n",
    "        .replace(\">\", \"&gt;\")\n",
    "    )\n",
    "ssml = \"<speak>\\n\"\n",
    "break_time = intro_time\n",
    "\n",
    "while break_time > 10000:  # maximum break time in Polly is 10s\n",
    "    ssml += '<break time = \"' + str(break_time) + 'ms\"/>'\n",
    "    break_time -= 10000\n",
    "ssml += '<break time = \"' + str(break_time) + 'ms\"/>'\n",
    "ssml += escaped_summarized_text\n",
    "ssml += \"</speak>\"\n",
    "\n",
    "response = polly_client.synthesize_speech(\n",
    "    Engine=\"neural\",\n",
    "    OutputFormat=\"mp3\",\n",
    "    Text=ssml,\n",
    "    TextType=\"ssml\",\n",
    "    VoiceId=voice_id,\n",
    ")\n",
    "\n",
    "if \"AudioStream\" in response:\n",
    "    with response[\"AudioStream\"] as stream:\n",
    "        audio_narration = stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16ae98-439f-4211-9ff8-550f6bf77d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f72f3-bbad-4d89-ad4f-afa62e06fa83",
   "metadata": {},
   "source": [
    "AWS Elemental MediaConvert로 비디오 트랜스코딩 단계를 위해 오디오 내레이션을 Amazon S3 버킷에 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab1d74-942d-419a-a9e8-e7c9facf3013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_bucket = session[\"bucket\"]\n",
    "audio_narration_filename = os.path.splitext(os.path.basename(video['path']))[0] + \".mp3\"\n",
    "s3_client.put_object(\n",
    "    Body=audio_narration, Bucket=s3_bucket, Key=audio_narration_filename, ContentType=\"audio/mpeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046c3b2",
   "metadata": {},
   "source": [
    "## MediaConvert 어셈블리 워크플로우 생성\n",
    "\n",
    "기본 입력 클리핑을 수행하기 위해 타임코드의 시퀀스를 AWS Elemental MediaConvert 어셈블리 워크플로우를 생성하는 매개변수로 사용합니다.\n",
    "\n",
    "Amazon Polly의 MP3 오디오와 결합하고 선호하는 배경 음악을 통합할 수 있는 가능성과 함께, 최종적으로 최종 비디오 요약 출력을 달성할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e622269-11c1-42ce-9318-7d9c2f7b0c5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "원본 비디오 입력에서 어셈블리 워크플로우를 시작해 보겠습니다. 어셈블리 워크플로우는 별도의 편집 소프트웨어 없이 하나 또는 다른 소스에서 출력 자산을 조립하기 위해 기본 입력 클리핑과 스티칭을 수행하는 MediaConvert 작업입니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "⏳ 다음 셀들은 AWS Elemental MediaConvert 작업을 시작할 것이며, 완료하는 데 몇 분이 걸릴 수 있습니다. 작업이 완료될 때까지 충분한 시간을 허용해 주세요.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99a505-65b2-4fbe-a932-07f5d3bdb631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam_role = session[\"MediaConvertRole\"]\n",
    "input_video_path = video[\"url\"]\n",
    "output_video_path = f\"s3://{s3_bucket}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6014b0-ce58-49bd-8c37-28c8b8918e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "video[\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f8db3-2022-4aae-8baa-a25243105a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "media_convert = boto3.client(\"mediaconvert\")\n",
    "response = media_convert.create_job(\n",
    "    Queue=\"Default\",\n",
    "    UserMetadata={},\n",
    "    Role=iam_role,\n",
    "    Settings={\n",
    "        \"TimecodeConfig\": {\"Source\": \"ZEROBASED\"},\n",
    "        \"OutputGroups\": [\n",
    "            {\n",
    "                \"Name\": \"File Group\",\n",
    "                \"Outputs\": [\n",
    "                    {\n",
    "                        \"ContainerSettings\": {\n",
    "                            \"Container\": \"MP4\",\n",
    "                            \"Mp4Settings\": {},\n",
    "                        },\n",
    "                        \"VideoDescription\": {\n",
    "                            \"CodecSettings\": {\n",
    "                                \"Codec\": \"H_264\",\n",
    "                                \"H264Settings\": {\n",
    "                                    \"MaxBitrate\": 40000000,\n",
    "                                    \"RateControlMode\": \"QVBR\",\n",
    "                                    \"SceneChangeDetect\": \"TRANSITION_DETECTION\",\n",
    "                                },\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"OutputGroupSettings\": {\n",
    "                    \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "                    \"FileGroupSettings\": {\"Destination\": output_video_path},\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"Inputs\": [\n",
    "            {\n",
    "                \"VideoSelector\": {},\n",
    "                \"TimecodeSource\": \"ZEROBASED\",\n",
    "                \"FileInput\": input_video_path,\n",
    "                \"InputClippings\": timecodes,\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    AccelerationSettings={\"Mode\": \"DISABLED\"},\n",
    "    StatusUpdateInterval=\"SECONDS_60\",\n",
    "    Priority=0,\n",
    ")\n",
    "\n",
    "job_complete = False\n",
    "\n",
    "while not job_complete:\n",
    "    job_response = media_convert.get_job(Id=response[\"Job\"][\"Id\"])\n",
    "    \n",
    "    job_status = job_response['Job']['Status']\n",
    "    print(f\"MediaConvert job status: {job_status}\")\n",
    "    \n",
    "    if job_status == 'COMPLETE':\n",
    "        print(\"Job is complete!\")\n",
    "        job_complete = True\n",
    "    elif job_status == 'ERROR':\n",
    "        print(\"Job has failed.\")\n",
    "        job_complete = True\n",
    "    else:\n",
    "        time.sleep(10)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"\\nElapsed time: {round(t1 - t0, 2)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a672bd8-f338-49d2-a4d1-7387aca9ef83",
   "metadata": {},
   "source": [
    "마지막으로, 출력에 오디오 트랙을 생성하고 각 출력 트랙과 단일 오디오 선택기를 연결합니다. 또한 최종 비디오 출력에 자막을 추가할 수도 있습니다. 다음과 같이 비디오 요약에 대한 자막을 생성할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf7378-b0a4-429d-a2f3-b29bb2ec6ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_summary_subtitle = \"\"\n",
    "start = intro_time\n",
    "\n",
    "def split_long_lines(text, max_line_length):\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_length + len(word) + len(current_line) > max_line_length:\n",
    "            lines.append(\" \".join(current_line))\n",
    "            current_line = []\n",
    "            current_length = 0\n",
    "        current_line.append(word)\n",
    "        current_length += len(word) + 1\n",
    "\n",
    "    if current_line:\n",
    "        lines.append(\" \".join(current_line))\n",
    "\n",
    "    return lines\n",
    "\n",
    "def milliseconds_to_subtitleTimeFormat(ms):\n",
    "    return \"{:02d}:{:02d}:{:02d},{:03d}\".format(\n",
    "        int((ms // 3600000) % 24),  # hours\n",
    "        int((ms // 60000) % 60),  # minutes\n",
    "        int((ms // 1000) % 60),  # seconds\n",
    "        int(ms % 1000),  # milliseconds\n",
    "    )\n",
    "\n",
    "for i in range(len(summarized_sentences)):\n",
    "    end = start + speech_durations[i]\n",
    "    video_summary_subtitle += f\"{i+1}\\n\"\n",
    "    video_summary_subtitle += f\"{milliseconds_to_subtitleTimeFormat(start)} --> {milliseconds_to_subtitleTimeFormat(end)}\\n\"\n",
    "    sentence_lines = split_long_lines(summarized_sentences[i], 90)\n",
    "    for line in sentence_lines:\n",
    "        video_summary_subtitle += f\"{line}\\n\"\n",
    "    video_summary_subtitle += \"\\n\"\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cee4cf-cc78-4948-8c82-decdc68994cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_summary_subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52990b82-c585-4aed-87c9-dfd8026146cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subtitle_filename = os.path.splitext(os.path.basename(video['path']))[0] + \".srt\"\n",
    "s3_client.put_object(\n",
    "    Body=video_summary_subtitle, Bucket=s3_bucket, Key=subtitle_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab15bb2-d99c-4c68-8b24-8b3bc32d3fe1",
   "metadata": {},
   "source": [
    "마지막으로, 최종 비디오 출력을 위한 MediaConvert 작업을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc265f-19d2-4e79-88bc-00d0c19877a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_video_path = f\"s3://{s3_bucket}/{video['path']}\"\n",
    "audio_file_path = f\"s3://{s3_bucket}/{audio_narration_filename}\"\n",
    "subtitle_file_path = f\"s3://{s3_bucket}/{subtitle_filename}\"\n",
    "output_video_path = f\"s3://{s3_bucket}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba312ef-46a2-4166-ba5b-61a613ba94f3",
   "metadata": {},
   "source": [
    "다음 단계에서는 [AWS Elemental MediaConvert](https://aws.amazon.com/mediaconvert/) 작업을 사용하여 내레이션된 음성과 자막을 원본 비디오에 적용합니다. 출력은 다운스트림 소비를 위해 S3 버킷에 작성됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ec775-5ad9-4ab8-a95b-5b3090d40b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "response = media_convert.create_job(\n",
    "    Queue=\"Default\",\n",
    "    UserMetadata={},\n",
    "    Role=iam_role,\n",
    "    Settings={\n",
    "        \"TimecodeConfig\": {\"Source\": \"ZEROBASED\"},\n",
    "        \"OutputGroups\": [\n",
    "            {\n",
    "                \"Name\": \"File Group\",\n",
    "                \"Outputs\": [\n",
    "                    {\n",
    "                        \"ContainerSettings\": {\n",
    "                            \"Container\": \"MP4\",\n",
    "                            \"Mp4Settings\": {},\n",
    "                        },\n",
    "                        \"VideoDescription\": {\n",
    "                            \"CodecSettings\": {\n",
    "                                \"Codec\": \"H_264\",\n",
    "                                \"H264Settings\": {\n",
    "                                    \"MaxBitrate\": 40000000,\n",
    "                                    \"RateControlMode\": \"QVBR\",\n",
    "                                    \"SceneChangeDetect\": \"TRANSITION_DETECTION\",\n",
    "                                },\n",
    "                            }\n",
    "                        },\n",
    "                        \"NameModifier\": \"_summary\",\n",
    "                        \"AudioDescriptions\": [\n",
    "                            {\n",
    "                                \"AudioSourceName\": \"Audio Selector Group 1\",\n",
    "                                \"CodecSettings\": {\n",
    "                                    \"Codec\": \"AAC\",\n",
    "                                    \"AacSettings\": {\n",
    "                                        \"Bitrate\": 96000,\n",
    "                                        \"CodingMode\": \"CODING_MODE_2_0\",\n",
    "                                        \"SampleRate\": 48000,\n",
    "                                    },\n",
    "                                },\n",
    "                            }\n",
    "                        ],\n",
    "                        \"CaptionDescriptions\": [\n",
    "                            {\n",
    "                                \"CaptionSelectorName\": \"Captions Selector 1\",\n",
    "                                \"DestinationSettings\": {\n",
    "                                    \"DestinationType\": \"BURN_IN\",\n",
    "                                    \"BurninDestinationSettings\": {\n",
    "                                        \"BackgroundOpacity\": 100,\n",
    "                                        \"FontSize\": 18,\n",
    "                                        \"FontColor\": \"WHITE\",\n",
    "                                        \"ApplyFontColor\": \"ALL_TEXT\",\n",
    "                                        \"BackgroundColor\": \"BLACK\",\n",
    "                                    },\n",
    "                                },\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                \"OutputGroupSettings\": {\n",
    "                    \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "                    \"FileGroupSettings\": {\"Destination\": output_video_path},\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"Inputs\": [\n",
    "            {\n",
    "                \"AudioSelectors\": {\n",
    "                    \"Audio Selector 1\": {\n",
    "                        \"DefaultSelection\": \"NOT_DEFAULT\",\n",
    "                        \"ExternalAudioFileInput\": audio_file_path,\n",
    "                    },\n",
    "                },\n",
    "                \"AudioSelectorGroups\": {\n",
    "                    \"Audio Selector Group 1\": {\n",
    "                        \"AudioSelectorNames\": [\"Audio Selector 1\"]\n",
    "                    }\n",
    "                },\n",
    "                \"VideoSelector\": {},\n",
    "                \"TimecodeSource\": \"ZEROBASED\",\n",
    "                \"CaptionSelectors\": {\n",
    "                    \"Captions Selector 1\": {\n",
    "                        \"SourceSettings\": {\n",
    "                            \"SourceType\": \"SRT\",\n",
    "                            \"FileSourceSettings\": {\"SourceFile\": subtitle_file_path},\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"FileInput\": input_video_path,\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    AccelerationSettings={\"Mode\": \"DISABLED\"},\n",
    "    StatusUpdateInterval=\"SECONDS_60\",\n",
    "    Priority=0,\n",
    ")\n",
    "\n",
    "job_complete = False\n",
    "\n",
    "while not job_complete:\n",
    "    job_response = media_convert.get_job(Id=response[\"Job\"][\"Id\"])\n",
    "    \n",
    "    job_status = job_response['Job']['Status']\n",
    "    print(f\"MediaConvert job status: {job_status}\")\n",
    "    \n",
    "    if job_status == 'COMPLETE':\n",
    "        print(\"Job is complete!\")\n",
    "        job_complete = True\n",
    "    elif job_status == 'ERROR':\n",
    "        print(\"Job has failed.\")\n",
    "        job_complete = True\n",
    "    else:\n",
    "        time.sleep(10)\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"\\nElapsed time: {round(t1 - t0, 2)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8189efa",
   "metadata": {},
   "source": [
    "## Short-form video output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b130c0-dcf5-478b-b244-e9688f8410e1",
   "metadata": {},
   "source": [
    "\n",
    "다음은 우리의 요약 프로세스에서 생성된 최종 비디오 출력입니다. 생성된 내레이션을 시작하기 전에 원본 비디오의 인트로를 보존했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637926aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_summary = os.path.splitext(os.path.basename(video['path']))[0] + \"_summary.mp4\"\n",
    "s3_client.download_file(s3_bucket, video_summary, video_summary)\n",
    "display(Video(url=video_summary, width=640, height=360, html_attributes=\"controls muted autoplay\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480292d0-51a9-4737-b1c3-b7b7b76b9250",
   "metadata": {},
   "source": [
    "## 시각적 및 오디오 이해를 통한 비디오 요약"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8b5e4-949f-4656-98f8-4482a3ae49a8",
   "metadata": {},
   "source": [
    "이전 섹션에서는 다음과 같이 비디오의 transcription만을 기반으로 요약된 비디오를 생성했습니다:\n",
    "1. 원본 비디오 transcript 추출 및 요약\n",
    "2. 요약과 원본 transcript를 의미론적으로 비교하여 가장 잘 매칭되는 비디오 세그먼트 찾기\n",
    "3. 최종 비디오 출력 생성\n",
    "\n",
    "이제 비디오 시각적 이해와 transcript 분석을 모두 결합하여 더 포괄적인 요약 비디오를 만드는 접근 방식을 향상시켜 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a21bd2-0cdc-4363-b36e-d186bd110af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_shots = video[\"shots\"].shots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43cccf-110e-48f2-88ed-dbe775f3c400",
   "metadata": {},
   "source": [
    "비디오에서 감지된 각 shot을 분석해 보겠습니다. 각 shot에 대해, Amazon Bedrock의 Large Language Model을 사용하여 시각적 콘텐츠의 설명을 생성할 것입니다. 또한 이 shot 세그먼트 동안 말해진 내용의 해당 transcript를 매칭할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a04db-3148-404a-8778-be7014458561",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_RETRIES = 50\n",
    "INITIAL_BACKOFF = 5\n",
    "bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "def invoke_model_with_retry(body, modelId, accept, contentType):\n",
    "    retries = 0\n",
    "    backoff = INITIAL_BACKOFF\n",
    "\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            response = bedrock_client.invoke_model(\n",
    "                body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "            )\n",
    "            return response\n",
    "        except ClientError as e:\n",
    "            error_code = e.response['Error']['Code']\n",
    "            print(f\"Error: {error_code}. Retrying in {backoff} seconds...\")\n",
    "            time.sleep(backoff)\n",
    "            retries += 1\n",
    "            backoff += 1\n",
    "    \n",
    "    raise Exception(\"Max retries reached. Unable to invoke model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62146f-ba8d-4fd8-9862-b5f19271acd2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_shot_description(shot):\n",
    "    \"\"\"\n",
    "    Generate a natural language description of a video shot using LLM in Amazon Bedrock\n",
    "    Args:\n",
    "        shot - Dictionary containing shot information including:\n",
    "                - id: unique identifier for the shot\n",
    "                - start_ms: start time of the shot in milliseconds\n",
    "                - end_ms: end time of the shot in milliseconds\n",
    "                - composite_images: visual representation that combine multiple frames from a single shot into one image\n",
    "              \n",
    "    Returns:\n",
    "        response_body - String containing the generated description of the visual content in the shot based on the analyzed frames\n",
    "    \"\"\"\n",
    "\n",
    "    model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "    \n",
    "    \n",
    "    prompt = f\"\"\"Provide a concise description of a video shot based on the given frame images. Focus on creating a cohesive narrative of the entire shot rather than describing each frame individually.\n",
    "        Skip the preamble; go straight into the description.Please translate the output to Korean and write it as complete sentences, without summarizing\"\"\"\n",
    "        \n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 512,\n",
    "    }\n",
    "\n",
    "    with open(f\"{shot['composite_images'][0]['file']}\", \"rb\") as image_file:\n",
    "        file_content = image_file.read()\n",
    "        base64_image_string = base64.b64encode(file_content).decode()\n",
    "        body[\"messages\"][0][\"content\"].append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/png\",\n",
    "                \"data\": base64_image_string,\n",
    "            },\n",
    "        })\n",
    "        \n",
    "    response = invoke_model_with_retry(\n",
    "        body=json.dumps(body), modelId=model_id, accept=accept, contentType=content_type\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    response_body = response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef0b651-b599-4917-b0bd-08742e9df600",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_shot_transcript(shot_startTime, shot_endTime, transcript):\n",
    "    \"\"\"\n",
    "    Extract relevant transcript that corresponds to a specific video shot's time range\n",
    "    Args:\n",
    "        shot_startTime - Start time of the shot in milliseconds\n",
    "        shot_endTime - End time of the shot in milliseconds\n",
    "        transcript - List of dictionaries containing sentence information including:\n",
    "                    - sentence_startTime: start time of the sentence\n",
    "                    - sentence_endTime: end time of the sentence\n",
    "                    - sentence: the transcript text\n",
    "                    \n",
    "    Returns:\n",
    "        relevant_transcript - String containing concatenated sentences that overlap with the shot's time range by at least 1 second\n",
    "    \"\"\"\n",
    "    relevant_transcript = \"\"\n",
    "    for item in transcript:\n",
    "        if item[\"sentence_startTime\"] >= shot_endTime:\n",
    "            break\n",
    "        if item[\"sentence_endTime\"] <= shot_startTime:\n",
    "            continue\n",
    "        delta_start = max(item[\"sentence_startTime\"], shot_startTime)\n",
    "        delta_end = min(item[\"sentence_endTime\"], shot_endTime)\n",
    "        if delta_end - delta_start >= 1000:\n",
    "            relevant_transcript += item[\"sentence\"] + \"; \"\n",
    "    return relevant_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f3252-39bf-4dee-9729-b168f3d0a8fb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Shot 설명 생성\n",
    "\n",
    "모든 shots의 텍스트 설명을 생성합니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "⏳ 호스팅된 워크숍에 대해 설정된 Amazon Bedrock의 매우 낮은 계정 한도로 인해 샘플 비디오의 약 100개 shots에 대한 설명을 생성하는 데 10분이 걸릴 수 있습니다. 속도를 높이기 위해 미리 계산된 shot 설명을 로드할 것입니다. 아래 셀에서 FASTPATH=False를 설정하여 이를 항상 끌 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb535b2f-6549-4729-aa77-a20b45dc09cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "FASTPATH = True\n",
    "\n",
    "if FASTPATH:\n",
    "    video[\"shots\"].load_fastpath_results(\"shots-descriptions.json\")\n",
    "else:\n",
    "    for counter, shot in enumerate(video['shots'].shots, start=1):\n",
    "        shot['shot_description'] = generate_shot_description(shot)\n",
    "        shot['shot_transcript'] = add_shot_transcript(shot['start_ms'], shot['end_ms'], processed_transcript)\n",
    "\n",
    "    # store shot descriptions so they can be loaded when the notebook is re-executed with FASTPATH=True.\n",
    "    video[\"shots\"].store_fastpath_results(\"shots-descriptions.json\")\n",
    "\n",
    "video_shots = video['shots'].shots\n",
    "t1 = time.time()\n",
    "print(f\"  Elapsed time: {round(t1 - t0, 2)}s\")\n",
    "print(f\"  Shots: {len(video_shots)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2de593-a780-4925-ad42-41a9aacb9dd5",
   "metadata": {},
   "source": [
    "### Shot 설명 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3279e1-8b6b-45ed-b63f-65e00b2b618d",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for counter, shot in enumerate(video_shots, start=1):\n",
    "    \n",
    "    print(f'\\nSHOT {counter}/{len(video_shots)}: from {shot[\"start_ms\"] }ms to {shot[\"end_ms\"] }ms =======\\n')\n",
    "    display(DisplayImage(f\"{shot['composite_images'][0]['file']}\"))\n",
    "    print(f'Shot description: {shot[\"shot_description\"]}\\n')\n",
    "    print(f'Shot transcript: {shot[\"shot_transcript\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707fd29-fdf1-4848-b8ec-d83a40d987dc",
   "metadata": {},
   "source": [
    "이제 각 shot에 대한 설명과 transcript가 있으므로, 의미론적 검색 기능을 위해 [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/) 벡터 데이터베이스에 저장해 보겠습니다.\n",
    "\n",
    "먼저, OpenSearch 인덱스를 생성할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a875b6-1d2a-462d-8d87-56e7e8aed73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_resources[\"region\"]\n",
    "aoss_host = session[\"AOSSCollectionEndpoint\"]\n",
    "aoss_index = \"video-summarization-index\"\n",
    "text_embedding_model = \"amazon.titan-embed-text-v2:0\"\n",
    "text_embedding_dimension = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71ec38-0586-4688-8c9c-cd9011639856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opensearch_index(host, region, index, len_embedding):\n",
    "    \"\"\"\n",
    "    Create an OpenSearch Serverless index with vector search capabilities\n",
    "    Args:\n",
    "        host - OpenSearch domain endpoint URL\n",
    "        region - AWS region where the OpenSearch domain is hosted\n",
    "        index - Name of the index to create\n",
    "        len_embedding - Dimension size of the vector embeddings\n",
    "    Returns:\n",
    "        client - Configured OpenSearch client object\n",
    "    \"\"\"\n",
    "    host = host.split(\"://\")[1] if \"://\" in host else host\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    auth = AWSV4SignerAuth(credentials, region, \"aoss\")\n",
    "\n",
    "    client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        pool_maxsize=20,\n",
    "    )\n",
    "\n",
    "    exist = client.indices.exists(index)\n",
    "    if exist:\n",
    "        client.indices.delete(index=index)\n",
    "        print(f\"Clean up previous index if it already exists\")\n",
    "        # Wait for a short time to ensure the deletion is processed\n",
    "        time.sleep(30)\n",
    "        \n",
    "    print(\"Creating index\")\n",
    "    index_body = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"video_name\": {\"type\": \"text\"},\n",
    "                \"shot_id\": {\"type\": \"text\"},\n",
    "                \"shot_startTime\": {\"type\": \"text\"},\n",
    "                \"shot_endTime\": {\"type\": \"text\"},\n",
    "                \"shot_description\": {\"type\": \"text\"},\n",
    "                \"shot_transcript\": {\"type\": \"text\"},\n",
    "                \"shot_desc_vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": len_embedding,\n",
    "                    \"method\": {\n",
    "                        \"engine\": \"nmslib\",\n",
    "                        \"space_type\": \"cosinesimil\",\n",
    "                        \"name\": \"hnsw\",\n",
    "                        \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                    },\n",
    "                },\n",
    "                \"shot_transcript_vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": len_embedding,\n",
    "                    \"method\": {\n",
    "                        \"engine\": \"nmslib\",\n",
    "                        \"space_type\": \"cosinesimil\",\n",
    "                        \"name\": \"hnsw\",\n",
    "                        \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"settings\": {\n",
    "            \"index\": {\n",
    "                \"number_of_shards\": 2,\n",
    "                \"knn.algo_param\": {\"ef_search\": 512},\n",
    "                \"knn\": True,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = client.indices.create(index, body=index_body)\n",
    "\n",
    "    print(\"Completed!\")\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1f202-5d1f-422e-a251-dff8db175b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_client = create_opensearch_index(aoss_host, region, aoss_index, text_embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d5107-6138-4d74-a515-5c63b6625e63",
   "metadata": {},
   "source": [
    "Amazon Bedrock의 텍스트 임베딩 모델을 사용하여, 이 데이터를 OpenSearch 인덱스에 삽입하기 전에 shot 설명과 transcripts에 대한 텍스트 임베딩을 생성합니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "⏳ 다음 셀은 Amazon Bedrock을 사용하여 임베딩을 생성하고 Amazon OpenSearch Serverless 인덱스에 삽입할 것입니다. 이 프로세스는 비디오 shots의 수에 따라 몇 분이 걸릴 수 있습니다. 모든 임베딩이 생성되고 삽입될 때까지 충분한 시간을 허용해 주세요.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bb8141-f760-4b03-866e-be882f84118f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "def get_text_embedding(text_embedding_model, text):\n",
    "    \"\"\"\n",
    "    Generate vector embeddings for text using Amazon Bedrock's embedding model\n",
    "    Args:\n",
    "        text_embedding_model - Model id of the Bedrock embedding model\n",
    "        text - Input text to generate embeddings\n",
    "        \n",
    "    Returns:\n",
    "        embedding - Result text's vector embedding\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        text = \"No transcript\"\n",
    "    body = json.dumps({\"inputText\": text, \"dimensions\": 1024, \"normalize\": True})\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, modelId=text_embedding_model, accept=accept, contentType=content_type\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    return response_body.get(\"embedding\")\n",
    "        \n",
    "print(f\"Insert embeddings to AOSS index ...\")\n",
    "for counter, shot in enumerate(video_shots, start=1):\n",
    "    shot_desc_embedding = get_text_embedding(text_embedding_model, shot[\"shot_description\"])\n",
    "    shot_transcript_embedding = get_text_embedding(text_embedding_model, shot[\"shot_transcript\"])\n",
    "    embedding_request_body = json.dumps(\n",
    "        {\n",
    "            \"video_name\": video[\"path\"],\n",
    "            \"shot_id\": shot[\"id\"],\n",
    "            \"shot_startTime\": shot[\"start_ms\"],\n",
    "            \"shot_endTime\": shot[\"end_ms\"],\n",
    "            \"shot_description\": shot[\"shot_description\"],\n",
    "            \"shot_transcript\": shot[\"shot_transcript\"],\n",
    "            \"shot_desc_vector\": shot_desc_embedding,\n",
    "            \"shot_transcript_vector\": shot_transcript_embedding\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = aoss_client.index(\n",
    "        index=aoss_index,\n",
    "        body=embedding_request_body,\n",
    "        params={\"timeout\": 60},\n",
    "    )\n",
    "\n",
    "print(\"Completed!\")\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"\\nElapsed time: {round(t1 - t0, 2)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f22eeaf-a930-4286-97ee-0c64009d74b2",
   "metadata": {},
   "source": [
    "비디오 요약의 각 문장에 대해, shots의 설명 임베딩과 shots의 transcription 임베딩을 사용하여 벡터 데이터베이스에서 관련 shots를 검색할 것입니다. 검색 프로세스는 시각적 및 오디오 정보의 중요성의 균형을 맞추기 위해 이러한 임베딩에 다른 가중치를 할당합니다: 관련 shots를 찾는 데 있어 시각적 콘텐츠의 중요성을 강조하는 **shot 맥락적 설명에 대해 75%(또는 3.0 부스트)** 그리고 시각적 설명보다 적은 영향력으로 오디오 콘텐츠가 검색 결과에 기여할 수 있도록 하는 **shot transcript에 대해 25%(또는 1.0 부스트)**.\n",
    "\n",
    "선택된 shots의 총 지속 시간도 각 문장의 음성 지속 시간과 일치해야 할 것입니다.\n",
    "\n",
    "하지만 먼저 OpenSearch에 삽입된 데이터가 검색될 준비가 되었는지 확인할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13610bd3-40fc-4772-b5d7-76596a2ef94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for the recent inserted data to be searchable in OpenSearch...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        result = aoss_client.search(index=aoss_index, body={\"query\": {\"match_all\": {}}})\n",
    "        if result['hits']['total']['value'] == len(video_shots):\n",
    "            print(\"\\nData is now available for search!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\".\", end=\"\", flush=True)\n",
    "            time.sleep(10)\n",
    "    except Exception as e:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033ee6b-8107-4d2f-9574-da5f9555e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_text(aoss_index, client, user_query):\n",
    "    \"\"\"\n",
    "    Search for relevant video shots using semantic similarity with user's text query\n",
    "    Args:\n",
    "        aoss_index - Name of the OpenSearch index\n",
    "        client - Configured OpenSearch client object\n",
    "        user_query - Text query from the user\n",
    "        \n",
    "    Returns:\n",
    "        response - List of dictionaries containing matching shots, where each dictionary includes:\n",
    "                  - shot_id: unique identifier for the shot\n",
    "                  - shot_startTime: start time of the shot\n",
    "                  - shot_endTime: end time of the shot\n",
    "                  - shot_description: visual description of the shot\n",
    "                  - shot_transcript: transcript text from the shot\n",
    "                  - score: similarity score of the match\n",
    "    \"\"\"\n",
    "    text_embedding = get_text_embedding(text_embedding_model, user_query)\n",
    "\n",
    "    aoss_query = {\n",
    "        \"size\": 100,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"query\": {\"match_all\": {}},\n",
    "                            \"script\": {\n",
    "                                \"lang\": \"knn\",\n",
    "                                \"source\": \"knn_score\",\n",
    "                                \"params\": {\n",
    "                                    \"field\": \"shot_desc_vector\",\n",
    "                                    \"query_value\": text_embedding,\n",
    "                                    \"space_type\": \"cosinesimil\",\n",
    "                                },\n",
    "                            },\n",
    "                            \"boost\": 3.0\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"query\": {\"match_all\": {}},\n",
    "                            \"script\": {\n",
    "                                \"lang\": \"knn\",\n",
    "                                \"source\": \"knn_score\",\n",
    "                                \"params\": {\n",
    "                                    \"field\": \"shot_transcript_vector\",\n",
    "                                    \"query_value\": text_embedding,\n",
    "                                    \"space_type\": \"cosinesimil\",\n",
    "                                },\n",
    "                            },\n",
    "                            \"boost\": 1.0\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"shot_id\",\n",
    "            \"shot_startTime\",\n",
    "            \"shot_endTime\",\n",
    "            \"shot_description\",\n",
    "            \"shot_transcript\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    response = client.search(body=aoss_query, index=aoss_index)\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    response = []\n",
    "    for hit in hits:\n",
    "        if hit[\"_score\"] >= 0:  # Set score threshold\n",
    "            response.append(\n",
    "                {\n",
    "                    \"shot_id\": hit[\"_source\"][\"shot_id\"],\n",
    "                    \"shot_startTime\": hit[\"_source\"][\"shot_startTime\"],\n",
    "                    \"shot_endTime\": hit[\"_source\"][\"shot_endTime\"],\n",
    "                    \"shot_description\": hit[\"_source\"][\"shot_description\"],\n",
    "                    \"shot_transcript\": hit[\"_source\"][\"shot_transcript\"],\n",
    "                    \"score\": hit[\"_score\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02548fc-4b9e-493e-99ae-f5220aaa5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_shots(timecodes, sentence, duration):\n",
    "    \"\"\"\n",
    "    Find and select video shots that match a summarized sentence, considering timing constraints\n",
    "    Args:\n",
    "        timecodes - List to store selected shot timecodes [[start_time, end_time], ...]\n",
    "        sentence - Text to search for matching video shots\n",
    "        duration - Required duration for the shots\n",
    "    \"\"\"\n",
    "    relevant_shots = search_by_text(aoss_index, aoss_client, sentence)\n",
    "    if duration is None: # intro\n",
    "        timecodes.append([relevant_shots[0][\"shot_startTime\"], relevant_shots[0][\"shot_endTime\"]])\n",
    "        shot_ids.add(relevant_shots[0][\"shot_id\"])\n",
    "        intro_time = relevant_shots[0][\"shot_endTime\"] - relevant_shots[0][\"shot_startTime\"]\n",
    "    else:\n",
    "        i = 0\n",
    "        while i < len(relevant_shots) and duration > 0:\n",
    "            if relevant_shots[i][\"shot_id\"] in shot_ids:\n",
    "                i += 1\n",
    "                continue\n",
    "            shot_duration = relevant_shots[i][\"shot_endTime\"] - relevant_shots[i][\"shot_startTime\"]\n",
    "            # timecodes.append([relevant_shots[i][\"shot_startTime\"], relevant_shots[i][\"shot_startTime\"] + min(shot_duration, duration)])\n",
    "            timecodes.append([relevant_shots[i][\"shot_endTime\"] - min(shot_duration, duration), relevant_shots[i][\"shot_endTime\"]])\n",
    "            shot_ids.add(relevant_shots[i][\"shot_id\"])\n",
    "            duration -= shot_duration\n",
    "            i += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c913d89-0a8f-41d1-b251-151c8039596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_ids = set()\n",
    "timecodes = []\n",
    "\n",
    "find_shots(timecodes, \"Meridian\", None) # Intro\n",
    "for i in range(len(summarized_sentences)):\n",
    "    find_shots(timecodes, summarized_sentences[i], speech_durations[i])\n",
    "\n",
    "# creditTime = timecodes[-1][1] + 1000\n",
    "# timecodes.append([timecodes[-1][1], creditTime])\n",
    "timecodes_text = \"\"\n",
    "for timecode in timecodes:\n",
    "    timecodes_text += (\n",
    "        ms_to_timecode(timecode[0], True)\n",
    "        + \",\"\n",
    "        + ms_to_timecode(timecode[1], True)\n",
    "        + \"\\n\"\n",
    "    )\n",
    "intro_time = timecodes[0][1] - timecodes[0][0]\n",
    "to_json = lambda s: [\n",
    "    {\"StartTimecode\": t1, \"EndTimecode\": t2}\n",
    "    for t1, t2 in (line.split(\",\") for line in s.split(\"\\n\") if line.strip())\n",
    "]\n",
    "timecodes = to_json(timecodes_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c838304-5037-4e20-8407-192e6a3a2033",
   "metadata": {},
   "source": [
    "결과는 우리의 최종 비디오 세그먼트를 정의하는 타임코드 목록입니다.\n",
    "    \n",
    "다음은 AWS Elemental MediaConvert 입력 클리핑에 사용될 생성된 타임코드입니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f8230-d50d-4e5c-9890-ee7dcd0bd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timecodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfc5d0-e0ba-4a55-814f-cbfa7d3d1358",
   "metadata": {},
   "source": [
    "이제 타임코드가 있으므로, 이전 섹션과 동일한 단계를 따를 수 있습니다:\n",
    "1. AWS Elemental MediaConvert를 사용하여 입력 클립 생성\n",
    "2. 최종 짧은 형식의 비디오를 만들기 위해 오디오 내레이션과 자막 삽입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b1312-b378-4aa6-a46a-e3a42853e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "escaped_summarized_text = (\n",
    "        summarized_text.replace(\"&\", \"&amp;\")\n",
    "        .replace('\"', \"&quot;\")\n",
    "        .replace(\"'\", \"&apos;\")\n",
    "        .replace(\"<\", \"&lt;\")\n",
    "        .replace(\">\", \"&gt;\")\n",
    "    )\n",
    "ssml = \"<speak>\\n\"\n",
    "break_time = intro_time\n",
    "\n",
    "while break_time > 10000:  # maximum break time in Polly is 10s\n",
    "    ssml += '<break time = \"' + str(break_time) + 'ms\"/>'\n",
    "    break_time -= 10000\n",
    "ssml += '<break time = \"' + str(break_time) + 'ms\"/>'\n",
    "ssml += escaped_summarized_text\n",
    "ssml += \"</speak>\"\n",
    "\n",
    "response = polly_client.synthesize_speech(\n",
    "    Engine=\"neural\",\n",
    "    OutputFormat=\"mp3\",\n",
    "    Text=ssml,\n",
    "    TextType=\"ssml\",\n",
    "    VoiceId=voice_id,\n",
    ")\n",
    "\n",
    "if \"AudioStream\" in response:\n",
    "    with response[\"AudioStream\"] as stream:\n",
    "        audio_narration = stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e548633-5f0b-40d1-b240-8cc8037d3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.put_object(\n",
    "    Body=audio_narration, Bucket=s3_bucket, Key=audio_narration_filename, ContentType=\"audio/mpeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6faae7b-bc4e-457f-863a-7d84be6cd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_summary_subtitle = \"\"\n",
    "start = intro_time\n",
    "for i in range(len(summarized_sentences)):\n",
    "    end = start + speech_durations[i]\n",
    "    video_summary_subtitle += f\"{i+1}\\n\"\n",
    "    video_summary_subtitle += f\"{milliseconds_to_subtitleTimeFormat(start)} --> {milliseconds_to_subtitleTimeFormat(end)}\\n\"\n",
    "    sentence_lines = split_long_lines(summarized_sentences[i], 90)\n",
    "    for line in sentence_lines:\n",
    "        video_summary_subtitle += f\"{line}\\n\"\n",
    "    video_summary_subtitle += \"\\n\"\n",
    "    start = end\n",
    "s3_client.put_object(\n",
    "    Body=video_summary_subtitle, Bucket=s3_bucket, Key=subtitle_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c40409-fbdf-448a-8d0c-4d76c8612db7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "⏳ 다음 셀들은 비디오 입력 클리핑을 처리하고, 이들을 함께 병합하며 오디오와 자막을 추가하기 위해 여러 AWS Elemental MediaConvert 작업을 실행할 것입니다. 이 프로세스는 비디오 클립의 수와 길이에 따라 몇 분, 잠재적으로 10분 이상이 걸릴 수 있습니다. 각 단계(개별 클립 처리, 병합, 오디오/자막 추가)는 MediaConvert 작업이 완료될 때까지 기다려야 합니다. 셀이 진행되는 동안 기다려 주세요.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad5d2e-0bc4-4c5d-ae4d-493c94f4f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_clip(media_convert, iam_role, input_video_path, output_video_path, timecode, clip_index):\n",
    "    \"\"\"\n",
    "    Create a MediaConvert job to process a single video clip (due to multiple input clipping need be processed in chronological order)\n",
    "    Args:\n",
    "        media_convert - MediaConvert client\n",
    "        iam_role - IAM role ARN for MediaConvert\n",
    "        input_video_path - S3 path for input video\n",
    "        output_video_path - S3 path for output video\n",
    "        timecode - Dictionary containing start and end timecodes for the clip\n",
    "        clip_index - Index number for the clip\n",
    "        \n",
    "    Returns:\n",
    "        - job_id: MediaConvert job Id\n",
    "        - clip_output: S3 path of the output clip\n",
    "    \"\"\"\n",
    "    clip_output = f\"{output_video_path}{video['output_dir']}_{clip_index}\"\n",
    "    \n",
    "    response = media_convert.create_job(\n",
    "        Queue=\"Default\",\n",
    "        UserMetadata={},\n",
    "        Role=iam_role,\n",
    "        Settings={\n",
    "            \"TimecodeConfig\": {\"Source\": \"ZEROBASED\"},\n",
    "            \"OutputGroups\": [\n",
    "                {\n",
    "                    \"Name\": \"File Group\",\n",
    "                    \"Outputs\": [\n",
    "                        {\n",
    "                            \"ContainerSettings\": {\n",
    "                                \"Container\": \"MP4\",\n",
    "                                \"Mp4Settings\": {},\n",
    "                            },\n",
    "                            \"VideoDescription\": {\n",
    "                                \"CodecSettings\": {\n",
    "                                    \"Codec\": \"H_264\",\n",
    "                                    \"H264Settings\": {\n",
    "                                        \"MaxBitrate\": 40000000,\n",
    "                                        \"RateControlMode\": \"QVBR\",\n",
    "                                        \"SceneChangeDetect\": \"TRANSITION_DETECTION\",\n",
    "                                    },\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"OutputGroupSettings\": {\n",
    "                        \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "                        \"FileGroupSettings\": {\"Destination\": clip_output},\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "            \"Inputs\": [\n",
    "                {\n",
    "                    \"VideoSelector\": {},\n",
    "                    \"TimecodeSource\": \"ZEROBASED\",\n",
    "                    \"FileInput\": video[\"url\"],\n",
    "                    \"InputClippings\": [timecode],\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        AccelerationSettings={\"Mode\": \"DISABLED\"},\n",
    "        StatusUpdateInterval=\"SECONDS_60\",\n",
    "        Priority=0,\n",
    "    )\n",
    "    \n",
    "    return response[\"Job\"][\"Id\"], clip_output\n",
    "\n",
    "def wait_for_job(media_convert, job_id):\n",
    "    \"\"\"\n",
    "    Wait for a MediaConvert job to complete\n",
    "    Args:\n",
    "        media_convert - MediaConvert client\n",
    "        job_id - MediaConvert job Id\n",
    "        \n",
    "    Returns:\n",
    "        bool - True if job completed successfully, False if error occurred\n",
    "    \"\"\"\n",
    "    job_complete = False\n",
    "    while not job_complete:\n",
    "        job_response = media_convert.get_job(Id=job_id)\n",
    "        job_status = job_response['Job']['Status']\n",
    "        print(f\"MediaConvert job status: {job_status}\")\n",
    "        \n",
    "        if job_status in ['COMPLETE', 'ERROR']:\n",
    "            return job_status == 'COMPLETE'\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba5225-ab9b-4513-be26-bb0ada9e6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "clip_paths = []\n",
    "    \n",
    "for i, timecode in enumerate(timecodes):\n",
    "    print(f\"Processing clip {i+1}/{len(timecodes)}\")\n",
    "    print(timecode)\n",
    "    \n",
    "    job_id, clip_output = process_single_clip(\n",
    "        media_convert, \n",
    "        iam_role, \n",
    "        input_video_path, \n",
    "        output_video_path, \n",
    "        timecode, \n",
    "        i\n",
    "    )\n",
    "\n",
    "    if wait_for_job(media_convert, job_id):\n",
    "        clip_paths.append(clip_output)\n",
    "    else:\n",
    "        print(f\"Failed to process clip {i+1}\")\n",
    "        continue\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"\\nElapsed time: {round(t1 - t0, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7ccf7-5d67-4324-8a59-65f40c9ca0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clips(media_convert, iam_role, clip_paths, output_video_path):\n",
    "    \"\"\"\n",
    "    Merge multiple video clips into a single video\n",
    "    Args:\n",
    "        media_convert - MediaConvert client\n",
    "        iam_role - IAM role ARN for MediaConvert\n",
    "        clip_paths - List of S3 paths of video clips to merge\n",
    "        output_video_path - S3 path for video output\n",
    "        \n",
    "    Returns:\n",
    "        - job_id: MediaConvert job Id\n",
    "        - merged_output: S3 path of the video output\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    \n",
    "    for clip_path in clip_paths:\n",
    "        inputs.append({\n",
    "            \"VideoSelector\": {},\n",
    "            \"TimecodeSource\": \"ZEROBASED\",\n",
    "            \"FileInput\": clip_path + \".mp4\",\n",
    "        })\n",
    "\n",
    "    merged_output = f\"{output_video_path}{video['output_dir']}\"\n",
    "    \n",
    "    response = media_convert.create_job(\n",
    "        Queue=\"Default\",\n",
    "        UserMetadata={},\n",
    "        Role=iam_role,\n",
    "        Settings={\n",
    "            \"TimecodeConfig\": {\"Source\": \"ZEROBASED\"},\n",
    "            \"OutputGroups\": [\n",
    "                {\n",
    "                    \"Name\": \"File Group\",\n",
    "                    \"Outputs\": [\n",
    "                        {\n",
    "                            \"ContainerSettings\": {\n",
    "                                \"Container\": \"MP4\",\n",
    "                                \"Mp4Settings\": {},\n",
    "                            },\n",
    "                            \"VideoDescription\": {\n",
    "                                \"CodecSettings\": {\n",
    "                                    \"Codec\": \"H_264\",\n",
    "                                    \"H264Settings\": {\n",
    "                                        \"MaxBitrate\": 40000000,\n",
    "                                        \"RateControlMode\": \"QVBR\",\n",
    "                                        \"SceneChangeDetect\": \"TRANSITION_DETECTION\",\n",
    "                                    },\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                    \"OutputGroupSettings\": {\n",
    "                        \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "                        \"FileGroupSettings\": {\"Destination\": merged_output},\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "            \"Inputs\": inputs,\n",
    "        },\n",
    "        AccelerationSettings={\"Mode\": \"DISABLED\"},\n",
    "        StatusUpdateInterval=\"SECONDS_60\",\n",
    "        Priority=0,\n",
    "    )\n",
    "    \n",
    "    return response[\"Job\"][\"Id\"], merged_output\n",
    "\n",
    "def add_audio_subtitles(media_convert, iam_role, input_video_path, audio_file_path, subtitle_file_path, final_output_path):\n",
    "    \"\"\"\n",
    "    Add audio narration and subtitle into the video\n",
    "    Args:\n",
    "        media_convert - MediaConvert client\n",
    "        iam_role - IAM role ARN for MediaConvert\n",
    "        input_video_path - S3 path for input video\n",
    "        audio_file_path - S3 path for audio narration file\n",
    "        subtitle_file_path - S3 path for SRT subtitle file\n",
    "        final_output_path - S3 path for video output\n",
    "        \n",
    "    Returns:\n",
    "        job_id: MediaConvert job Id\n",
    "    \"\"\"\n",
    "    \n",
    "    response = media_convert.create_job(\n",
    "        Queue=\"Default\",\n",
    "        UserMetadata={},\n",
    "        Role=iam_role,\n",
    "        Settings={\n",
    "            \"TimecodeConfig\": {\"Source\": \"ZEROBASED\"},\n",
    "            \"OutputGroups\": [\n",
    "                {\n",
    "                    \"Name\": \"File Group\",\n",
    "                    \"Outputs\": [\n",
    "                        {\n",
    "                            \"ContainerSettings\": {\n",
    "                                \"Container\": \"MP4\",\n",
    "                                \"Mp4Settings\": {},\n",
    "                            },\n",
    "                            \"VideoDescription\": {\n",
    "                                \"CodecSettings\": {\n",
    "                                    \"Codec\": \"H_264\",\n",
    "                                    \"H264Settings\": {\n",
    "                                        \"MaxBitrate\": 40000000,\n",
    "                                        \"RateControlMode\": \"QVBR\",\n",
    "                                        \"SceneChangeDetect\": \"TRANSITION_DETECTION\",\n",
    "                                    },\n",
    "                                }\n",
    "                            },\n",
    "                            \"NameModifier\": \"_summary_v2\",\n",
    "                            \"AudioDescriptions\": [\n",
    "                                {\n",
    "                                    \"AudioSourceName\": \"Audio Selector Group 1\",\n",
    "                                    \"CodecSettings\": {\n",
    "                                        \"Codec\": \"AAC\",\n",
    "                                        \"AacSettings\": {\n",
    "                                            \"Bitrate\": 96000,\n",
    "                                            \"CodingMode\": \"CODING_MODE_2_0\",\n",
    "                                            \"SampleRate\": 48000,\n",
    "                                        },\n",
    "                                    },\n",
    "                                }\n",
    "                            ],\n",
    "                            \"CaptionDescriptions\": [\n",
    "                                {\n",
    "                                    \"CaptionSelectorName\": \"Captions Selector 1\",\n",
    "                                    \"DestinationSettings\": {\n",
    "                                        \"DestinationType\": \"BURN_IN\",\n",
    "                                        \"BurninDestinationSettings\": {\n",
    "                                            \"BackgroundOpacity\": 100,\n",
    "                                            \"FontSize\": 18,\n",
    "                                            \"FontColor\": \"WHITE\",\n",
    "                                            \"ApplyFontColor\": \"ALL_TEXT\",\n",
    "                                            \"BackgroundColor\": \"BLACK\",\n",
    "                                        },\n",
    "                                    },\n",
    "                                }\n",
    "                            ],\n",
    "                        }\n",
    "                    ],\n",
    "                    \"OutputGroupSettings\": {\n",
    "                        \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "                        \"FileGroupSettings\": {\"Destination\": final_output_path},\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "            \"Inputs\": [\n",
    "                {\n",
    "                    \"VideoSelector\": {},\n",
    "                    \"TimecodeSource\": \"ZEROBASED\",\n",
    "                    \"FileInput\": input_video_path,\n",
    "                    \"AudioSelectors\": {\n",
    "                        \"Audio Selector 1\": {\n",
    "                            \"DefaultSelection\": \"NOT_DEFAULT\",\n",
    "                            \"ExternalAudioFileInput\": audio_file_path,\n",
    "                        },\n",
    "                    },\n",
    "                    \"AudioSelectorGroups\": {\n",
    "                        \"Audio Selector Group 1\": {\n",
    "                            \"AudioSelectorNames\": [\"Audio Selector 1\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"CaptionSelectors\": {\n",
    "                        \"Captions Selector 1\": {\n",
    "                            \"SourceSettings\": {\n",
    "                                \"SourceType\": \"SRT\",\n",
    "                                \"FileSourceSettings\": {\"SourceFile\": subtitle_file_path},\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        AccelerationSettings={\"Mode\": \"DISABLED\"},\n",
    "        StatusUpdateInterval=\"SECONDS_60\",\n",
    "        Priority=0,\n",
    "    )\n",
    "    \n",
    "    return response[\"Job\"][\"Id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed1493-a464-4027-b330-b9cfb614feb0",
   "metadata": {},
   "source": [
    "오디오와 자막으로 모든 클립을 병합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1ae31-cff6-4432-944a-12bea0e42a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "merge_job_id, merged_output = merge_clips(\n",
    "    media_convert, \n",
    "    iam_role, \n",
    "    clip_paths, \n",
    "    output_video_path\n",
    ")\n",
    "\n",
    "if wait_for_job(media_convert, merge_job_id):\n",
    "    print(\"Successfully merged video clips\")\n",
    "else:\n",
    "    print(\"Failed to merge video clips\")\n",
    "\n",
    "final_job_id = add_audio_subtitles(\n",
    "    media_convert,\n",
    "    iam_role,\n",
    "    merged_output + \".mp4\",\n",
    "    audio_file_path,\n",
    "    subtitle_file_path,\n",
    "    output_video_path\n",
    ")\n",
    "\n",
    "if wait_for_job(media_convert, final_job_id):\n",
    "    print(\"Successfully created final video with audio and subtitle\")\n",
    "else:\n",
    "    print(\"Failed to add audio and subtitle\")\n",
    "\n",
    "t1 = time.time()\n",
    "print(f\"\\nElapsed time: {round(t1 - t0, 2)}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d6dc22-8908-4e26-ab86-390debed5539",
   "metadata": {},
   "source": [
    "다음은 시각적 및 오디오 이해를 모두 통합한 우리의 최종 비디오 요약입니다. 차이점을 보기 위해 이전 버전(오디오 내레이션만 기반)과 비교해 봅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26fda34-e421-4349-84cd-b1352407d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_summary_v2 = os.path.splitext(os.path.basename(video['path']))[0] + \"_summary_v2.mp4\"\n",
    "s3_client.download_file(s3_bucket, video_summary_v2, video_summary_v2)\n",
    "print(\"Short-form video with video and audio understanding\\n\")\n",
    "print(\"=========================================\\n\")\n",
    "display(Video(url=video_summary_v2, width=640, height=360, html_attributes=\"controls muted autoplay\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9602a9-3ff7-430e-8cb6-938ffe3a1f08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Short-form video with audio understanding only\\n\")\n",
    "print(\"=========================================\\n\")\n",
    "display(Video(url=video_summary, width=640, height=360, html_attributes=\"controls muted autoplay\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69f6e3-d26e-4aac-b09f-cecaee7c9b99",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Clean up\n",
    "이 실습에서 생성한 일부 리소스를 제거하려면 아래 코드의 주석을 해제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2189d57-fe0e-4ce5-85b6-95ee9a0a82f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aoss_client.indices.delete(aoss_index)\n",
    "# s3_client.delete_object(Bucket=s3_bucket, Key=audio_narration_filename)\n",
    "# s3_client.delete_object(Bucket=s3_bucket, Key=video['path'])\n",
    "# s3_client.delete_object(Bucket=s3_bucket, Key=subtitle_filename)\n",
    "# s3_client.delete_object(Bucket=s3_bucket, Key=video_summary)\n",
    "# s3_client.delete_object(Bucket=s3_bucket, Key=video_summary_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d2af71-3a48-49a3-8884-1faae95d9ca6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 다음은 무엇인가요?\n",
    "\n",
    "다른 사용 사례를 시도해보거나, 완료했다면 [Additional Resources](09-resources.ipynb) 실습으로 계속 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4463c363-3ef6-4e8f-9736-6e7db875f6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
