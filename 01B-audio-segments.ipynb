{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6416fcdf-75c5-49fc-a9ec-adb8018df284",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Speech to text를 사용한 오디오 세그먼트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe197858",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "비디오의 오디오 부분은 비디오의 구조와 내러티브에 대한 수많은 단서를 제공합니다. 오디오에는 다음이 포함됩니다:\n",
    "\n",
    "* 단어, 문장부호, 문장 또는 자막과 같은 다른 언어 세그먼트로 더 세분화될 수 있는 Speech\n",
    "* 스포츠 이벤트의 군중 소음, foley effects 또는 배경 음악과 같은 소리\n",
    "* 무음을 포함한 사운드 레벨\n",
    "\n",
    "아래 그림 1은 비디오의 서로 다른 시점의 사운드 레벨에 해당하는 오디오 파형이 있는 Meridian의 클립을 보여줍니다.\n",
    "\n",
    "![Video with audio waveform](./static/images/01-meridian-audio-wide.png)\n",
    "**그림 1. 오디오 파형이 있는 비디오**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2545f85-1a23-4923-8577-f471f0293ce2",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "이 노트북에서는 비디오의 speech에 초점을 맞춰 오디오를 사용하여 비디오를 세그먼트로 분해하는 기술을 계속 탐구할 것입니다. 구체적으로 다음을 수행합니다:\n",
    "\n",
    "* Amazon Transcribe를 사용하여 오디오 대본을 추출하고 단어, 문장, 자막 및 speech 세그먼트를 식별\n",
    "* 서로 다른 대화 주제를 포함하는 비디오 세그먼트를 식별하기 위해 Foundation Model 사용\n",
    "* 비디오의 chapters를 식별하기 위해 대화 주제와 시각적 scenes 결합\n",
    "\n",
    "이 노트북의 출력은 이후 워크숍의 사용 사례 섹션에서 사용될 것입니다. 예를 들어, Ad Break Detection and Contextual Ad Placement, Video Summarization 등에서 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0182508-2fb3-4ad5-a154-02d94b92b493",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 주요 용어 및 정의\n",
    "\n",
    "노트북에서 사용되는 용어의 정의를 확인하고 싶을 때 이 섹션을 참조할 수 있습니다.\n",
    "\n",
    "- **Transcript** - 비디오의 오디오 트랙에서 speech 내용을 나타내는 연속적인 단어와 문장부호의 시퀀스\n",
    "- **Subtitle** - 시청자가 비디오를 시청할 때 표시되도록 의도된 비디오의 speech를 나타내는 텍스트 세그먼트\n",
    "- **Speech Segment** - Amazon Transcribe의 출력인 speech 세그먼트\n",
    "- **Conversation topic (aka topic)** - 유사한 주제에 대한 논의를 포함하는 문장 그룹의 요약\n",
    "- **WebVTT** - 웹의 비디오 콘텐츠에 대한 자막과 같은 시간이 지정된 텍스트 트랙 데이터를 저장하는 데 사용되는 파일 형식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54483bf9-04f4-44c6-92b9-9bf2d7047882",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "### 워크플로우\n",
    "\n",
    "이 실습의 목적은 speech to text를 사용하여 비디오의 오디오 요소를 다루는 실습과 오디오에서 파생된 시간이 지정된 텍스트로 prompt engineering을 연습하는 것입니다. SageMaker Studio에서 AWS 서비스 API를 실행하게 됩니다. 워크플로우는 비디오를 입력으로 받아 transcript, sentence segments, WebVTT 형식 subtitles, conversation topics 출력을 생성합니다.\n",
    "\n",
    "![Audio Workflow](./static/images/01-audio-workflow.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "💡 이 Jupyter 노트북의 왼쪽 탐색 패널에서 목록 아이콘을 클릭하면 노트북의 개요와 현재 위치를 볼 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed3f92-e7fd-4e2f-a8df-8646718e00ee",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488c3b7-2901-497e-8a1e-10d768f31943",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ab94-9b8a-42fe-a0e5-2b2e742d71fb",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import json_repair\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "from lib import util\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022f442-6ef9-4621-8c15-09eab7d82398",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 이전 노트북에서 저장된 값 검색하기\n",
    "\n",
    "이 노트북을 실행하려면 이전 노트북 [01A-visual-segments-frames-shots-scenes.ipynb](01A-visual-segments-frames-shots-scenes.ipynb)을 실행했어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ad005-d406-404e-9d6a-ef4d673980d6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get variables from the previous notebook\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa577b6f-b260-42a1-be85-ed71b9cc91f3",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b31731-37cd-44ac-bedf-901103b280a7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Amazon Transcribe를 사용하여 transcript, 오디오 세그먼트 및 WebVTT 자막 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2244ca-bc6c-4af3-bb69-3dd4f78fbfcd",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Transcript 라이브러리\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>참고</b>: Transcript 라이브러리를 위한 코드는 더 자세히 살펴보고 싶다면 이 프로젝트의 <b>lib</b> 폴더에 있지만, 이 연습의 목적은 transcript의 개념을 이해하는 것입니다.\n",
    "</div>\n",
    "\n",
    "[lib/transcript.py](lib/transcript.py)를 열려면 링크를 클릭합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05138c47-4707-44ac-88ee-8cc98e5a8e7d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Transcript 및 기타 파생 출력 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f75f1b-2ce4-4b9c-a870-b5bfd56ce4ff",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['transcript'] = Transcript(video[\"path\"], session['bucket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0353ce-7f0d-43a1-9681-de6f10c34a99",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Amazon Transcribe 결과 검토\n",
    "Amazon Transcribe의 응답에는 텍스트 전용 transcript와 각 단어 및 문장부호의 신뢰도 점수와 타임스탬프가 포함된 items 컬렉션이 포함된 transcript가 있는 results 딕셔너리가 포함되어 있습니다. 응답에는 또한 WebVTT 또는 SRT 형식의 자막으로 포맷된 동일한 transcript가 포함되어 있습니다. 이러한 출력들을 살펴보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc0b01-a3e4-46a3-8385-d29d0672f1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Transcript JSON 출력\n",
    "\n",
    "transcript의 `results` 속성에는 몇 가지 흥미롭고 유용한 출력이 포함되어 있습니다:\n",
    "* **transcripts** - 비디오에 대한 대체 텍스트 전용 transcripts 목록입니다. 우리의 결과는 1개의 대안만 생성하도록 구성되어 있지만, 필요한 경우 Amazon Transcribe를 구성하여 더 많이 생성할 수 있습니다. 대안은 비디오의 speech에 대한 서로 다른 의미론적 해석일 뿐입니다.\n",
    "* **items** - items는 Amazon Transcribe가 비디오의 speech에서 추론한 `pronunciations`(단어)와 `punctuation`의 시계열입니다. 이는 AI 추론이기 때문에 각 item에 대한 _confidence score_가 있습니다. Amazon Transcribe에서 confidence scores는 서비스가 각 전사된 단어의 정확성에 대해 얼마나 확신하는지를 나타냅니다. 마지막으로, 각 item에 대한 시작 및 종료 시간이 있어 items의 타이밍을 비디오의 다른 타임스탬프가 있는 요소와 정렬하는 데 사용할 수 있습니다.\n",
    "* **audio_segments** - audio segments에는 Amazon Transcribe가 감지한 distinct speech segments 목록이 포함되어 있습니다. 세그먼트 경계는 다음에 의해 결정됩니다:\n",
    "    * 자연스러운 speech 일시 중지\n",
    "    * 화자 변경\n",
    "    * 최대 세그먼트 지속 시간 제한\n",
    "    * 문장부호\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    ❓ <b>Amazon Q Developer에게 질문하기</b>: What are the attributes of items returned from Amazon Transcribe? \n",
    "<br></br>\n",
    "    ❓ <b>Amazon Q Developer에게 질문하기</b>: What are confidence scores in Amazon Transcribe? \n",
    "</div>\n",
    "\n",
    "아래 샘플 비디오의 `results`에서 이러한 각 속성을 잠시 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a331b9e-cc86-4ed8-a8de-bf03829b73dc",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(filename=video['transcript'].transcript_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd334b64-011e-4405-ae18-86b505650ffc",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### WebVTT 자막\n",
    "\n",
    "Amazon Transcribe에서 생성한 WebVTT 형식 자막의 처음 몇 줄을 살펴보겠습니다. WebVTT 자막은 비디오 플레이어에서 비디오의 speech를 화면에 텍스트로 표시하는 데 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36dafd9-7520-46da-93aa-0f0f71d2cb73",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo \"$(<{video['transcript'].vtt_file})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88471866-4743-4eef-8d20-2f22f0b42be3",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 비디오와 함께 자막 재생하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb94b5d-906f-4028-9907-df6a38377fc1",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "마지막으로, 생성된 자막 트랙과 함께 비디오를 보겠습니다. 참고: 비디오에서 첫 번째 speech가 발생하는 shot에서 비디오를 시작하기 위해 shot 정보를 사용했습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5071a3-b3ab-463b-bbc6-27489497242a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Play the video with subtitles\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "start = video['shots'].shots[8]['start_ms']/1000\n",
    "end = video['shots'].shots[8]['end_ms']/1000\n",
    "speech_shot_url = f'{video[\"url\"]}#t={start}'\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "    <source src=\"{speech_shot_url}\" type=\"video/mp4\">\n",
    "    <track src=\"{video['transcript'].vtt_file}\" kind=\"captions\" srclang=\"en\" label=\"English\" default>\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf388e0-f71d-4bff-bc4b-f6b44659bcbe",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Amazon Bedrock을 사용하여 대화 주제 생성\n",
    "\n",
    "다음 섹션에서는 generative AI를 사용하여 비디오 transcript에서 시간에 따라 발생하는 대화 주제를 이해할 것입니다. 이는 여러 Foundation Models로 수행할 수 있는 텍스트 요약 작업입니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "💡 이 워크숍 전반에 걸쳐 이 작업과 다른 모든 generative AI 작업에 Amazon Bedrock의 Anthropic Claude Sonnet 3.5를 사용할 것입니다. 멀티모달(이미지와 텍스트) 입력에 대해 다양한 작업을 수행할 수 있는 유연성 때문에 Anthropic Claude Sonnet 3를 선택했습니다. 실제로는 요구 사항에 따라 다른 작업에 다른 FM을 대체할 수 있습니다. 예를 들어, 특정 사용 사례에서 Anthropic Claude Haiku가 더 낮은 가격으로 적절한 결과를 제공할 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e602f7-c9b8-4c9d-97c2-444efe83e243",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "transcript를 Amazon Bedrock의 Anthropic Claude 3 Sonnet 모델에 전달할 것입니다. 모델은 transcript를 분석하고 특정 JSON 형식으로 대화 주제 포인트를 제안합니다. 프롬프트에서 각 주제에 주제를 설명하는 이유와 함께 시작 및 종료 타임스탬프가 포함되어야 함을 지정합니다. Sonnet 모델의 프롬프트는 아래와 같습니다. 이 프롬프트는 [Anthropic Claude Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html)를 사용합니다:\n",
    "\n",
    "\n",
    "**System prompt**\n",
    "\n",
    "```\n",
    "당신은 WebVTT 형식의 영화 대본을 분석하고 대화의 주제 변화를 기반으로 주제 포인트를 제안하는 미디어 운영 어시스턴트입니다. 전체 대본을 읽는 것이 중요합니다.\n",
    "```\n",
    "\n",
    "\n",
    "**Messages**\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'content': '여기 <transcript> 태그 안에 대본이 있습니다:\\n'\n",
    "                '<transcript>{transcript}\\n</transcript>\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': '네. 대본을 받았습니다. 어떤 출력 형식을 원하시나요?',\n",
    "        'role': 'assistant'\n",
    "    },\n",
    "    {\n",
    "        'content': 'JSON 형식입니다. 출력 예시:\\n'\n",
    "                '{\"topics\": [{\"start\": \"00:00:10.000\", \"end\": \"00:00:32.000\", '\n",
    "                '\"reason\": \"주제가 ...에 대해 이야기하는 것으로 보입니다\"}]}\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': '{', 'role': 'assistant'\n",
    "    }\n",
    " ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039f33b-2e09-46dc-8cba-182d0deacbc8",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    ❓ <b>Amazon Q Developer에게 질문하기</b>: What are the inputs for the Anthropic Claude Messages API?\n",
    "    <br></br>* Jupyter 노트북의 사이드바 메뉴에서 Amazon Q Developer 메뉴 아이콘을 보셨을 것입니다. 노트북의 코드나 AWS API에 대해 질문이 있을 때마다 Amazon Q에게 물어보세요.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b91c4f-6b57-44ca-886f-1811771c0da4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 프롬프트 구성하기\n",
    "\n",
    "아래 코드는 Amazon Bedrock용 프롬프트를 구성한 다음 프롬프트를 실행하기 위해 Amazon Bedrock API를 호출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ead40-3039-4c02-8cd5-018ef31de2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import bedrock_helper as brh\n",
    "from lib import util\n",
    "\n",
    "# MODEL_ID = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "# MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "MODEL_VER = 'bedrock-2023-05-31'\n",
    "CLAUDE_PRICING = (0.00025, 0.00125)\n",
    "\n",
    "def analyze_conversations(vtt_file):\n",
    "\n",
    "    response = {}\n",
    "    messages = []\n",
    "\n",
    "    # transcript\n",
    "    transcript_message = make_transcript(vtt_file)\n",
    "    messages.append(transcript_message)\n",
    "\n",
    "    # output format?\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the transcript. What output format?'\n",
    "    })\n",
    "\n",
    "    # example output\n",
    "    example_message = make_conversation_example()\n",
    "    messages.append(example_message)\n",
    "\n",
    "    # prefill output\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "\n",
    "    ## system prompt to role play\n",
    "    system = '''\n",
    "    You are a media operation assistant who analyses movie transcripts in WebVTT format\n",
    "    and suggest topic points based on the topic changes in the conversations. \n",
    "    It is important to read the entire transcript.\n",
    "    '''\n",
    "\n",
    "    ## setting up the model params\n",
    "    model_params = {\n",
    "        'anthropic_version': MODEL_VER,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system,\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "    response['model_params'] = model_params\n",
    "    try:\n",
    "        response['response'] = inference(model_params)\n",
    "    except Exception as e:\n",
    "        print(colored(f\"ERR: inference: {str(e)}\\n RETRY...\", 'red'))\n",
    "        response['response'] = inference(model_params)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def make_conversation_example():\n",
    "    example = {\n",
    "        'topics': [\n",
    "            {\n",
    "                'start': '00:00:10.000',\n",
    "                'end': '00:00:32.000',\n",
    "                'reason': 'It appears the topic talks about...'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'JSON format. An example of the output:\\n{0}\\n'.format(json.dumps(example))\n",
    "    }\n",
    "\n",
    "def make_transcript(vtt_file):\n",
    "    with open(vtt_file, encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "    \n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'Here is the transcripts in <transcript> tag:\\n<transcript>{0}\\n</transcript>\\n Answer in detail in Korean.'.format(transcript)\n",
    "    }\n",
    "\n",
    "def make_conversation_message(text):\n",
    "    message = {\n",
    "        'role': 'user',\n",
    "        'content': 'No conversation.'\n",
    "    }\n",
    "\n",
    "    if text:\n",
    "        message['content'] = 'Here is the conversation of the scene in <conversation> tag.\\n<conversation>\\n{0}\\n</conversation>\\n'.format(text)\n",
    "\n",
    "    return message\n",
    "\n",
    "def chapters_to_vtt(chapters, output_file):\n",
    "    \"\"\"\n",
    "      Constructs a webvtt caption file based on the timestamps from the given chapters.\n",
    "      Args:\n",
    "         chapters - the topic points\n",
    "         output_file - output file where the caption webvtt content is stored.\n",
    "      Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    vtt_lines = 'WEBVTT\\n\\n'\n",
    "    for idx, chapter in enumerate(chapters):\n",
    "        line = f\"{idx}\\n{chapter['start']} --> {chapter['end']}\\n{chapter['reason']}\\n\"\n",
    "        vtt_lines = vtt_lines+line\n",
    "\n",
    "    util.save_to_file(output_file, vtt_lines)\n",
    "    return\n",
    "\n",
    "def inference(model_params):\n",
    "    model_id = MODEL_ID\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    bedrock_runtime_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps(model_params),\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # patch the json string output with '{' and parse it\n",
    "    response_content = response_body['content'][0]['text']\n",
    "    if response_content[0] != '{':\n",
    "        response_content = '{' + response_content\n",
    "\n",
    "    try:\n",
    "        response_content = json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        print(colored(\"Malformed JSON response. Try to repair it...\", 'red'))\n",
    "        try:\n",
    "            response_content = json_repair.loads(response_content, strict=False)\n",
    "        except Exception as e:\n",
    "            print(colored(\"Failed to repair the JSON response...\", 'red'))\n",
    "            print(colored(response_content, 'red'))\n",
    "            raise e\n",
    "\n",
    "    response_body['content'][0]['json'] = response_content\n",
    "    response_body['model_params'] = model_params\n",
    "\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287629d3-233e-4ecb-b0b7-1a5a920bff67",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### 프롬프트 실행 및 출력 검토\n",
    "\n",
    "모델은 프롬프트 `messages`에서 샘플 출력을 사용하여 지정된 방식으로 포맷된 결과를 반환할 것입니다. 이 경우 비디오의 주제 목록입니다. 각 주제에는 다음이 포함됩니다:\n",
    "\n",
    "* **start** - HH:MM:SS.MS 형식의 비디오 시작을 기준으로 한 주제의 시작 시간\n",
    "* **end** - HH:MM:SS.MS 형식의 비디오 시작을 기준으로 한 주제의 종료 시간\n",
    "* **reason** - `start`와 `end` 사이의 시간 범위에서 대화의 요약\n",
    "\n",
    "다음 셀을 실행하여 프롬프트를 실행하고 결과로 나온 주제들을 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7864-1a65-45ca-bf6e-d126a41a1f80",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversations_response = analyze_conversations(video['transcript'].vtt_file)\n",
    "video['topics'] = conversations_response['response']['content'][0]['json']['topics']\n",
    "\n",
    "# show the conversation cost\n",
    "conversation_cost = brh.display_conversation_cost(conversations_response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a2c34-9ae7-4e49-b86c-4d3866bfe2da",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(JSON(video['topics'], root='topics', expanded=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2bbd9e-68d8-4890-9858-936242f30e29",
   "metadata": {},
   "source": [
    "#### 마지막으로, 모든 매개변수가 채워진 Anthropic Claude에 전달된 실제 프롬프트를 마지막으로 살펴보겠습니다.\n",
    "\n",
    "`system` 프롬프트는 Anthropic Claude의 작업과 제약 조건을 설명하고, 프롬프트의 `messages` 부분은 FM과의 대화를 모델링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f8d81-26ad-447f-a536-106e4472fbf3",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(conversations_response['model_params'], root='prompt', expanded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec2e96-7be9-44f8-8ddb-1e5e704bd696",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## \"topic points\" 생성\n",
    "\n",
    "모델의 출력이 원본 transcript를 정확하게 반영하도록 하기 위해, 출력 JSON은 중복되는 chapter 타임스탬프를 병합하고 chapter 경계를 WebVTT 파일의 실제 캡션 타임스탬프와 정렬하도록 후처리됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b0dfe-0604-4841-a1fb-719a4bfea83b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lib import topics\n",
    "\n",
    "# merge overlapped conversation timestamps\n",
    "video['topics'] = topics.merge_topics(video['topics'])\n",
    "\n",
    "# validate the conversation timestamps against the caption timestamps\n",
    "captions = topics.parse_webvtt(video['transcript'].vtt_file)\n",
    "video['topics'] = topics.validate_timestamps(video['topics'], captions)\n",
    "\n",
    "# save the conversations\n",
    "util.save_to_file(os.path.join(video[\"output_dir\"], 'topics.json'), video['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e575ce-4108-4102-a809-0b7870d2f784",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(video['topics'], root='topics', expanded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8ab79-9d0f-4cf9-8893-dadecb07f23a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 주제 시각화\n",
    "마지막으로, 비디오와 함께 주제를 시각화합니다. chapter 요약을 WebVTT 파일로 출력하고 비디오와 함께 재생할 것입니다.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "참고: 주제들은 12분 전체 비디오에 걸쳐 분포되어 있습니다. 전체 비디오를 재생하는 대신 플레이어 타임라인의 다른 지점을 클릭하여 비디오를 건너뛸 수 있습니다. 5~6개의 주제를 모두 찾을 수 있는지 확인해보세요.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80c20b-ba02-4e7f-b0b2-042514dbd4bc",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['topics_vtt'] = os.path.join(video['frames'].video_asset_dir(), \"topics.vtt\")\n",
    "chapters_to_vtt(video['topics'], video['topics_vtt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500f720-7c1f-4801-ae1f-fa0e439da80a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Play the video with topic summaries\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "start = video['shots'].shots[8]['start_ms']/1000\n",
    "end = video['shots'].shots[8]['end_ms']/1000\n",
    "speech_shot_url = f'{video[\"url\"]}#t={start}'\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "    <source src=\"{speech_shot_url}\" type=\"video/mp4\">\n",
    "    <track src=\"{video['topics_vtt']}\" kind=\"captions\" srclang=\"en\" label=\"English\" default>\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba889078-83ae-4bce-a1d9-af198caef450",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "방금 generative AI를 사용하여 비디오의 주제 요약을 만들었습니다. 멋진 기술이죠!\n",
    "\n",
    "🤔 오디오나 비디오 콘텐츠의 다른 시간 세그먼트를 요약하는 다른 사용 사례를 생각할 수 있나요?\n",
    "<br></br>\n",
    "🤔 Foundation Model이 찾아내는 주제의 수를 어떻게 늘릴 수 있을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98de755-2cd6-451a-9570-7b26474447e7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "# 워크숍의 나머지 부분에서 사용할 수 있도록 비디오 메타데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6a12e-9e70-4120-b1de-1b23f827c2a7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa2234-0461-4488-a50b-dbfd8c8ea48a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# 다음은 무엇인가요?\n",
    "\n",
    "이제 기본적인 세그먼테이션을 만들고 비디오 클립에 대한 프롬프트를 작성했으므로, 이러한 기술을 적용하여 몇 가지 사용 사례를 해결할 준비가 되었습니다. 여기서 더 탐구할 사용 사례를 선택할 수 있습니다.\n",
    "\n",
    "* [Ad break detection and contextual Ad targeting](02-ad-breaks-and-contextual-ad-targeting.ipynb) - 광고 삽입 기회를 식별합니다. 표준 분류법을 사용하여 비디오 콘텐츠를 광고 콘텐츠와 매칭합니다.\n",
    "* [Video summarization](03-video-summarization.ipynb) - 긴 비디오에서 짧은 형식의 비디오를 생성합니다\n",
    "* [Semantic video search](04-semantic-video-search.ipynb) - 이미지와 자연어를 사용하여 관련 클립을 찾는 비디오 검색\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eede4a-597e-40cd-ab80-b5e31a03f669",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
